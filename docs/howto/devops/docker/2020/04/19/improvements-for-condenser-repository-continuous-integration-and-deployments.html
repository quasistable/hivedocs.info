<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Improvements for Condenser repository, Continuous Integration and Deployments | Hive Chain Documentation</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Improvements for Condenser repository, Continuous Integration and Deployments" />
<meta name="author" content="engrave" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I want to elaborate not only on introduced features but also on my thought and reasoning so everyone can benefit and introduce changes in their projects. This post will be long but I believe it might be useful for the entire HiveDevs community. The workflow is built on top of Gitlab CI/CD but can be easily modified for any other CI/CD tool (i.e. CircleCI or Jenkins). Docker I’ve started my work by reviewing Dockerfile. I’ve built the condenser image and was a bit shocked: $ docker image ls condenser latest 6d57c0c8a904 19 seconds ago 1.54GB 1.54 GB for a simple frontend application? Boy, it’s just too much. Let’s do something with it. Leaner docker image Let’s start by recognizing the reason why is this so big. You can also look into your Dockerfile and package.json and search for some common mistakes: using full-sized images rather than alpine versions installing dev dependencies for production images putting dev dependencies into production dependencies list copying useless files into the final image Yep, you can check every point here. Let’s make some work on a new Dockerfile: FROM node:12.16.2 as development WORKDIR /var/app COPY package.json yarn.lock ./ RUN yarn install --non-interactive --frozen-lockfile --ignore-optional COPY . . RUN mkdir tmp &amp;&amp; yarn build CMD [ &quot;yarn&quot;, &quot;run&quot;, &quot;start&quot; ] ### REMOVE DEV DEPENDENCIES ## FROM development as dependencies RUN yarn install --non-interactive --frozen-lockfile --ignore-optional --production ### BUILD MINIFIED PRODUCTION ## FROM node:12.16.2-alpine as production WORKDIR /var/app ARG SOURCE_COMMIT ENV SOURCE_COMMIT ${SOURCE_COMMIT} ARG DOCKER_TAG ENV DOCKER_TAG ${DOCKER_TAG} COPY --from=dependencies /var/app/package.json /var/app/package.json COPY --from=dependencies /var/app/config /var/app/config COPY --from=dependencies /var/app/dist /var/app/dist COPY --from=dependencies /var/app/lib /var/app/lib COPY --from=dependencies /var/app/src /var/app/src COPY --from=dependencies /var/app/tmp /var/app/tmp COPY --from=dependencies /var/app/webpack /var/app/webpack COPY --from=dependencies /var/app/node_modules /var/app/node_modules COPY --from=dependencies /var/app/healthcheck.js /var/app/healthcheck.js HEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=5 CMD node /var/app/healthcheck.js CMD [ &quot;yarn&quot;, &quot;run&quot;, &quot;production&quot; ] What has been improved: Node.js version was upgraded from 8.7 to 12.16.2 which is the latest LTS at the moment. Always try using the latest framework versions, which may include security fixes and performance upgrades. It’s also a good habit to use a specific version up to the patch number. Multistage build was used to build an optimal image for production deployment. First, we build a development stage with every dependency to be able to compile React application. Next, we’re removing development dependencies with --production switch during the dependencies stage. After all, we’re creating a minimal image from the alpine node version which is the smallest base available, by copying only necessary files and directories. Healthcheck has been introduced, so the docker daemon can easily manage containers with automatic restarts if necessary and zero-downtime deployments which will be explained later in this post. It will be also useful for dynamic scaling capabilities with docker swarm. Also, package.json file was modified, but it’s not worth to post its content here: Dev dependencies like webpack, babel or eslint was moved into a proper list called devDependencies so yarn could install only production deps for the final image. Strict versions were introduced for every dependency to make sure every build produces exactly the same image. Semantic versioning is popular, but there is no way to check if the package author does not introduce breaking changes with only patch version increased. If you need to update a package, do it manually. If you have enough test coverage, you can update the package and run CI/CD to check if everything works fine. After all that work done, the condenser image size was massively reduced: $ docker image ls condenser latest 58406d338e67 8 seconds ago 226MB The compressed image in the docker registry is even smaller. Much better, right? Shame on you, Steemit! Healthcheck Simply speaking, docker is running containers and trying to keep it alive as long as possible. But the system needs to have a tool to determine if the container is actually alive. It may seem alive, but is your app responding to requests for example? Fortunately, docker has also integrated health check mechanism which can be integrated into Dockerfile or docker-compose.yml. Usually, you need to create an endpoint for liveness checks. Fortunately, condenser already has one so we can utilize it easily. There is a lot of examples with curl used as a docker health check, but it’s not a good way to go. Healthcheck should work cross-platform and curl implementation differs on Windows and Unix. You should write health check in the same technology or framework as your projects are written, for condenser it’s Node.js. const http = require(&quot;http&quot;); const options = { host: &quot;localhost&quot;, port: &quot;8080&quot;, path: &#39;/.well-known/healthcheck.json&#39;, timeout: 5000 }; const request = http.request(options, (res) =&gt; { console.log(`STATUS: ${res.statusCode}`); if (res.statusCode == 200) { process.exit(0); } else { console.error(`ERROR: ${res.statusCode}`); process.exit(1); } }); request.on(&#39;error&#39;, (err) =&gt; { console.error(&#39;ERROR&#39;, err); process.exit(1); }); request.end(); When ready, instruct Docker to use your health check mechanism. Add following line into your Dockefile: HEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=5 CMD node /var/app/healthcheck.js Make sure this file /var/app/healthcheck.js exists inside your image. If you want to be sure your health check is working, inspect your container after running it: $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 060166cf52ee hiveio/condenser:development &quot;docker-entrypoint.s…&quot; 5 minutes ago Up 5 minutes (healthy) 0.0.0.0:8080-&gt;8080/tcp mystifying_dhawan There should be a (healthy) indicator in STATUS column. And there is. Please also note that during the container startup process, it will indicate slightly different status (starting), as the docker daemon will wait before making the first check. It’s because we’re giving some time for our app to startup. It’s the start-period parameter. Depending on your app and traffic/load, those parameters should vary. Pushing images to Docker Hub We already have an improved (smaller) docker image, so it’s time to push it to the repository. Doing it manually is a waste of time and may cause human mistakes. The best way is to utilize Gitlab Runner to do it for us in an automatic and bullet-proof manner. Here is a job definition from .gitlab-ci.yml file with some additional code which we will breakdown: variables: DOCKER_IMAGE: hiveio/condenser .docker-job: &amp;docker-job image: docker:stable services: - docker:dind before_script: - echo $HUB_TOKEN | docker login -u $HUB_USERNAME --password-stdin build-development: &lt;&lt;: *docker-job stage: build variables: DOCKER_TAG: $DOCKER_IMAGE:$CI_COMMIT_SHORT_SHA DOCKER_TAG_MAIN: $DOCKER_IMAGE:development SOURCE_COMMIT: $CI_COMMIT_SHA only: - develop script: - docker build -t $DOCKER_TAG -t $DOCKER_TAG_MAIN --build-arg SOURCE_COMMIT --build-arg DOCKER_TAG . - docker push $DOCKER_TAG - docker push $DOCKER_TAG_MAIN First, we’re creating global variable DOCKER_IMAGE so we can reuse it later in many places. And in case we would like to change the image name, we do it only in a single place. Hence we have multiple jobs defined in .gitlab-ci.yml file, it’s good to utilize advanced YAML syntax, which includes hidden keys and anchors. It will decrease the duplicated code and make the file easier to read and maintain. Every job name starting with the dot will be considered as a hidden key and won’t be directly executed. Btw, this is a quick way to temporarily disable any job in your GitLab CI/CD without commenting or removing it. By using .docker-job: &amp;docker-job we created an anchor which can be later used to extend any job. If you add &lt;&lt;: *docker-job, it will populate image, services and before_script properties automatically. It’s a good move if you have multiple jobs that do similar things. Later on, we’re creating some additional local (job scoped) variables: DOCKER_TAG_MAIN which will be evaluated to hiveio/condenser:development DOCKER_TAG which will be evaluated to hiveio/condenser:344e55ef or similar SOURCE_COMMIT which will be evaluated to 344e55efefd56e00b15eea6ccf8560a1107b9ff6 (or similar commit SHA) It’s a good idea to double tag an image. Latest tag is useless if you want to track your development process. Later on, I will describe the way how we’re using this specific, sha-tagged image to track deployments and rollback them anytime with a single click. Finally, we’re building an image with additional build arguments by using --build-arg: docker build -t $DOCKER_TAG -t $DOCKER_TAG_MAIN --build-arg SOURCE_COMMIT --build-arg DOCKER_TAG . If you scroll up to the Dockerfile section, you will notice ARG SOURCE_COMMIT and ENV SOURCE_COMMIT ${SOURCE_COMMIT} which means these build arguments will be injected as an environment variables into your containers. It’s a quick and easy way to pass additional, build-level variables into your images. Those specific variables are later returned by the condenser health endpoint. It may be useful to check a specific instance source. $ curl &lt;a href=&quot;https://staging.condenser.engrave.dev/.well-known/healthcheck.json&quot;&gt;https://staging.condenser.engrave.dev/.well-known/healthcheck.json&lt;/a&gt; {&quot;status&quot;:&quot;ok&quot;,&quot;docker_tag&quot;:&quot;hiveio/condenser:344e55ef&quot;,&quot;source_commit&quot;:&quot;344e55efefd56e00b15eea6ccf8560a1107b9ff6&quot;} An important thing to mention is that HUB_TOKEN and HUB_USERNAME are environment variables injected into the GitLab runner job and configured in Projects Settings. To prevent unauthorized users from pushing malicious images into the official registry, those variables are configured as protected and masked, which means they can be only used on specific branches and are moderated from job logs, so there is no way it can leak without internal bad actor with elevated permissions. Merge requests To improve the process of adding new features and fixes to the codebase, Merge Requests have got a brand new CI/CD workflow which includes the following jobs: The entire pipeline is fired on every Merge Request and it’s required to pass before changes could be merged. If the pipeline fails for some reason (i.e. failing unit tests), there is no way to merge changes into the main branch. This will enforce code quality and prevent regression. Eslint Code quality and standardization are important, especially if it’s an open-source project that could be maintained by totally different developers from all around the world. Eslint is a tool that statically analyzes the code to quickly find potential problems and keep code organized with specified rules. Code analysis is especially useful when developing Javascript applications. It’s really easy to make some stupid mistakes. Eslint job will be fired on every Merge Request and on every branch pushed to the repository: run-eslint: stage: test image: node:12.16.2 only: - branches - merge_requests before_script: - yarn install --frozen-lockfile --ignore-optional script: - yarn ci:eslint allow_failure: true # will be changed to false when all linter errors removed Because the codebase is a bit neglected, run-eslint job is allowed to fail for the moment (allow_failure: true), which is indicated by an orange exclamation mark on a MR view or pipelines list. There are “some” errors and warnings right now but it should be cleaned up soon, so we can require eslint job to pass before merging proposed changes: ✖ 1208 problems (1187 errors, 21 warnings) 831 errors and 0 warnings potentially fixable with the `--fix` option. error Command failed with exit code 1. Unit tests Extensive testing is the only way to produce bullet-proof code and stable applications. Similar to run-eslint, run-unit-tests job is fired on every branch and every merge request. run-unit-tests: stage: test image: node:12.16.2 only: - branches - merge_requests before_script: - yarn install --frozen-lockfile --ignore-optional script: - yarn run ci:test coverage: /All files[^|]*\|[^|]*\s+([\d\.]+)/ Testing suit (jest) was configured to produce coverage report: This report is later parsed by a Gitlab, using coverage: /All files[^|]*\|[^|]*\s+([\d\.]+)/ configuration. It will display coverage status and percentage change on the Merge Request view, allowing reviewers to quickly inspect if the code quality is increasing or not. I would advise declining merging new features not covered with tests. This may be omitted for important fixes, but we all should try to make the code better, not worse. Also, repository settings were changed and Pipelines must succeed setting is checked by default. It means no more broken code on develop/production branches. Review apps Review Apps are a huge and very important feature. From now on, every feature can be inspected visually by the reviewer with a single click. Gitlab Runner will create a special instance built from proposed code and expose it for the reviewers: Review app requires three jobs to run on a merge request: build-review-app: &lt;&lt;: *docker-job stage: build variables: DOCKER_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA SOURCE_COMMIT: $CI_COMMIT_SHA only: - merge_requests before_script: - echo $CI_JOB_TOKEN | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin script: - docker build -t $DOCKER_TAG --build-arg SOURCE_COMMIT --build-arg DOCKER_TAG . - docker push $DOCKER_TAG deploy-review-app: &lt;&lt;: *docker-job &lt;&lt;: *docker-remote-host-review stage: deploy variables: DOCKER_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA SERVICE_NAME: review_$CI_ENVIRONMENT_SLUG only: - merge_requests script: - DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify pull $DOCKER_TAG - DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify service rm $SERVICE_NAME || true # try to remove previous service but do not fail if it not exist - DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify service create --with-registry-auth --network infrastructure --name $SERVICE_NAME $DOCKER_TAG - echo &quot;Review app deployed&quot; environment: name: review/$CI_COMMIT_REF_NAME url: &lt;a href=&quot;https://&quot;&gt;https://&lt;/a&gt; $CI_ENVIRONMENT_SLUG$APP_REVIEW_SUBDOMAIN on_stop: stop-review-app auto_stop_in: 1 week stop-review-app: &lt;&lt;: *docker-job &lt;&lt;: *docker-remote-host-review stage: deploy variables: SERVICE_NAME: review_$CI_ENVIRONMENT_SLUG only: - merge_requests when: manual script: - DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify service rm $SERVICE_NAME || true # try to remove previous service but do not fail if it not exist - echo &quot;Review app stopped&quot; environment: name: review/$CI_COMMIT_REF_NAME action: stop The first job should look familiar if you read previous parts of the post. The only difference is that we’re overwriting before_script. Note that if you’re using anchors, you can always overwrite the template freely which is what we did here. Because merge requests could be opened by developers which may not be well known in the community (vel. bad actors), it could be a security issue if the CI/CD on Merge Requests could push images to the official repository. To prevent this, we’re using an internal registry provided by the Gitlab itself. It’s private, will work well for Review Apps but won’t be accessible by anyone else. echo $CI_JOB_TOKEN | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin We are using CI_JOB_TOKEN, CI_REGISTRY_USER and CI_REGISTRY which are environment variables injected automatically by Gitlab, no need to configure them. Also, please note using --password-stdin which is a more secure way to log in as it will prevent the password from being exposed in job logs. By default, docker will connect to the local daemon on unsecured, but not exposed port. It is yet possible to configure Docker daemon to validate TLS certificates so it could be exposed to the external world in a secure way, which is how we deploy services from our runners to our machine. You need to pass additional parameters: -H which is remote docker address --tlsverify makes sure your daemon is trying to identify itself with certificates --with-registry-auth will send registry authentication to swarm agents --network will connect service to the specified network so the reverse proxy could expose the instance DOCKER_CERT_PATH will instruct demon where to search for TLS certificates DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify service create --with-registry-auth --network infrastructure --name $SERVICE_NAME $DOCKER_TAG This will create a new docker service and expose it automatically under the URL created with CI_ENVIRONMENT_SLUG, which is a variable provided by Gitlab. It’s guaranteed to be a valid URL or docker/kubernetes service name. Review App instances are automatically removed when MR is closed or 1 week after it’s opened. This is achieved by running stop-review-app job which is configured to be manually triggered (when: manual). Tracking environment deployments By using sha-tagged images, it’s possible to quickly redeploy the environment at any moment, with a single click. In case of emergency, project maintainers can rollback the environment to the specified point in time (docker image to be specific) Staging Deploying a staging environment is quite similar to deploying a Review App. It also uses remote docker daemon but the service is not created on-demand, it’s updated with a new image. With docker swarm mode, you can ensure your application to be highly available. The swarm agent will take care of your containers. It will restart them or spin a new one if necessary (this is why health check is so important). It is a built-in, native docker mode everyone should start using. deploy-staging: &lt;&lt;: *docker-job &lt;&lt;: *docker-remote-host-staging stage: deploy variables: DOCKER_TAG: $DOCKER_IMAGE:$CI_COMMIT_SHORT_SHA SERVICE_NAME: staging_condenser only: - develop script: - DOCKER_CERT_PATH=$CERTS docker -H $STAGING_HOST --tlsverify pull $DOCKER_TAG - DOCKER_CERT_PATH=$CERTS docker -H $STAGING_HOST --tlsverify service update --image $DOCKER_TAG --update-failure-action rollback --update-order start-first $SERVICE_NAME environment: name: staging url: &lt;a href=&quot;https://&quot;&gt;https://&lt;/a&gt; $STAGING_DOMAIN Job uses protected variables to prevent “bad” developers from pushing/deploying malicious code. It is only possible to push staging from the protected develop branch. Pushing directly to develop is disabled. Zero downtime deployments Updating single service is easy with docker swarm: DOCKER_CERT_PATH=$CERTS docker -H $STAGING_HOST --tlsverify service update --image $DOCKER_TAG --update-failure-action rollback --update-order start-first $SERVICE_NAME There are additional but important parameters provided: --image $DOCKER_TAG - update existing service by running new containers with the specified image. For this case, it’s sha-tagged image build from develop branch --update-failure-action rollback - by default, docker daemon will try to update the service and do nothing if it fails. By passing this parameter, we’re instructing docker to roll back the service to a previous state, which means containers using the previous image. --update-order start-first - by default, docker will kill current containers and spin new ones after it. It may cause some downtime which we don’t want. By setting start-first, we instruct docker to spin new containers first. Swarm agent will switch containers without downtime if the healtcheck result becomes positive. And in case something gone wrong (healtcheck failed for any reason), we end up with a working staging environment because old containers are not touched at all. Resources under control With docker swarm you have full control over your services and containers. This is an example configuration which is used for staging environment. With some tweaks, it could be used for production also: version: &quot;3.7&quot; services: condenser: image: hiveio/condenser:latest deploy: mode: replicated replicas: 2 resources: limits: cpus: &quot;0.85&quot; memory: 2024M restart_policy: condition: any delay: 5s update_config: parallelism: 1 delay: 10s failure_action: rollback order: start-first rollback_config: parallelism: 1 delay: 5s networks: - reverse-proxy networks: reverse-proxy: external: name: reverse-proxy Production It is not finished yet, but my advice is to start using docker swarm mode for production deployments (CI/CD is ready). It’s designed to serve services like a condenser. Some key features of Docker Swarm (based on official documentation): Cluster management integrated with Docker Engine: if you know how to build the docker image, start a container, read some logs, you’re ready to use Docker Swarm. You don’t need to install additional software as it is a native Docker feature. Decentralized design: adding a worker or a manager to your swarm is as easy as running a single command. Those could be machines from all around the world. Scaling: For each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state. Multi-host networking: You can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application. Load balancing: You can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes. Secure by default: Each node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA. Rolling updates: At rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll back to a previous version of the service. And after all, it is called Swarm, can’t be a coincidence! ;) I’m using Swarm mode for most of my projects. Within Engrave (dblog.org) swarm is automatically managing a set of 23 microservices with almost 30 containers at the moment. My requests and tips for condenser Developers (and not only) Please do write unit tests covering your code. Improving coverage will only cause fewer problems and bugs. Coverage is now visible on every merge request so it’s damn easy to see a regression. Take care of the code quality. Use exact package versions in package.json file. When using ^, it’s not guaranteed to install the same version on two different builds. And some developers like to introduce breaking changes without changing the major version number. Use alpine images to create minified production images. Use the latest images from official Docker HUB when possible. Use multi-stage builds to create leaner docker image Write and configure health checks for your applications Run eslint to clean up your code before you push it to the repository. You can use husky to ensure it happens automatically. Vote for @engrave witness if you find my work valuable See: Improvements for hive.blog, Continuous Integration and Deployments by @engrave" />
<meta property="og:description" content="I want to elaborate not only on introduced features but also on my thought and reasoning so everyone can benefit and introduce changes in their projects. This post will be long but I believe it might be useful for the entire HiveDevs community. The workflow is built on top of Gitlab CI/CD but can be easily modified for any other CI/CD tool (i.e. CircleCI or Jenkins). Docker I’ve started my work by reviewing Dockerfile. I’ve built the condenser image and was a bit shocked: $ docker image ls condenser latest 6d57c0c8a904 19 seconds ago 1.54GB 1.54 GB for a simple frontend application? Boy, it’s just too much. Let’s do something with it. Leaner docker image Let’s start by recognizing the reason why is this so big. You can also look into your Dockerfile and package.json and search for some common mistakes: using full-sized images rather than alpine versions installing dev dependencies for production images putting dev dependencies into production dependencies list copying useless files into the final image Yep, you can check every point here. Let’s make some work on a new Dockerfile: FROM node:12.16.2 as development WORKDIR /var/app COPY package.json yarn.lock ./ RUN yarn install --non-interactive --frozen-lockfile --ignore-optional COPY . . RUN mkdir tmp &amp;&amp; yarn build CMD [ &quot;yarn&quot;, &quot;run&quot;, &quot;start&quot; ] ### REMOVE DEV DEPENDENCIES ## FROM development as dependencies RUN yarn install --non-interactive --frozen-lockfile --ignore-optional --production ### BUILD MINIFIED PRODUCTION ## FROM node:12.16.2-alpine as production WORKDIR /var/app ARG SOURCE_COMMIT ENV SOURCE_COMMIT ${SOURCE_COMMIT} ARG DOCKER_TAG ENV DOCKER_TAG ${DOCKER_TAG} COPY --from=dependencies /var/app/package.json /var/app/package.json COPY --from=dependencies /var/app/config /var/app/config COPY --from=dependencies /var/app/dist /var/app/dist COPY --from=dependencies /var/app/lib /var/app/lib COPY --from=dependencies /var/app/src /var/app/src COPY --from=dependencies /var/app/tmp /var/app/tmp COPY --from=dependencies /var/app/webpack /var/app/webpack COPY --from=dependencies /var/app/node_modules /var/app/node_modules COPY --from=dependencies /var/app/healthcheck.js /var/app/healthcheck.js HEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=5 CMD node /var/app/healthcheck.js CMD [ &quot;yarn&quot;, &quot;run&quot;, &quot;production&quot; ] What has been improved: Node.js version was upgraded from 8.7 to 12.16.2 which is the latest LTS at the moment. Always try using the latest framework versions, which may include security fixes and performance upgrades. It’s also a good habit to use a specific version up to the patch number. Multistage build was used to build an optimal image for production deployment. First, we build a development stage with every dependency to be able to compile React application. Next, we’re removing development dependencies with --production switch during the dependencies stage. After all, we’re creating a minimal image from the alpine node version which is the smallest base available, by copying only necessary files and directories. Healthcheck has been introduced, so the docker daemon can easily manage containers with automatic restarts if necessary and zero-downtime deployments which will be explained later in this post. It will be also useful for dynamic scaling capabilities with docker swarm. Also, package.json file was modified, but it’s not worth to post its content here: Dev dependencies like webpack, babel or eslint was moved into a proper list called devDependencies so yarn could install only production deps for the final image. Strict versions were introduced for every dependency to make sure every build produces exactly the same image. Semantic versioning is popular, but there is no way to check if the package author does not introduce breaking changes with only patch version increased. If you need to update a package, do it manually. If you have enough test coverage, you can update the package and run CI/CD to check if everything works fine. After all that work done, the condenser image size was massively reduced: $ docker image ls condenser latest 58406d338e67 8 seconds ago 226MB The compressed image in the docker registry is even smaller. Much better, right? Shame on you, Steemit! Healthcheck Simply speaking, docker is running containers and trying to keep it alive as long as possible. But the system needs to have a tool to determine if the container is actually alive. It may seem alive, but is your app responding to requests for example? Fortunately, docker has also integrated health check mechanism which can be integrated into Dockerfile or docker-compose.yml. Usually, you need to create an endpoint for liveness checks. Fortunately, condenser already has one so we can utilize it easily. There is a lot of examples with curl used as a docker health check, but it’s not a good way to go. Healthcheck should work cross-platform and curl implementation differs on Windows and Unix. You should write health check in the same technology or framework as your projects are written, for condenser it’s Node.js. const http = require(&quot;http&quot;); const options = { host: &quot;localhost&quot;, port: &quot;8080&quot;, path: &#39;/.well-known/healthcheck.json&#39;, timeout: 5000 }; const request = http.request(options, (res) =&gt; { console.log(`STATUS: ${res.statusCode}`); if (res.statusCode == 200) { process.exit(0); } else { console.error(`ERROR: ${res.statusCode}`); process.exit(1); } }); request.on(&#39;error&#39;, (err) =&gt; { console.error(&#39;ERROR&#39;, err); process.exit(1); }); request.end(); When ready, instruct Docker to use your health check mechanism. Add following line into your Dockefile: HEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=5 CMD node /var/app/healthcheck.js Make sure this file /var/app/healthcheck.js exists inside your image. If you want to be sure your health check is working, inspect your container after running it: $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 060166cf52ee hiveio/condenser:development &quot;docker-entrypoint.s…&quot; 5 minutes ago Up 5 minutes (healthy) 0.0.0.0:8080-&gt;8080/tcp mystifying_dhawan There should be a (healthy) indicator in STATUS column. And there is. Please also note that during the container startup process, it will indicate slightly different status (starting), as the docker daemon will wait before making the first check. It’s because we’re giving some time for our app to startup. It’s the start-period parameter. Depending on your app and traffic/load, those parameters should vary. Pushing images to Docker Hub We already have an improved (smaller) docker image, so it’s time to push it to the repository. Doing it manually is a waste of time and may cause human mistakes. The best way is to utilize Gitlab Runner to do it for us in an automatic and bullet-proof manner. Here is a job definition from .gitlab-ci.yml file with some additional code which we will breakdown: variables: DOCKER_IMAGE: hiveio/condenser .docker-job: &amp;docker-job image: docker:stable services: - docker:dind before_script: - echo $HUB_TOKEN | docker login -u $HUB_USERNAME --password-stdin build-development: &lt;&lt;: *docker-job stage: build variables: DOCKER_TAG: $DOCKER_IMAGE:$CI_COMMIT_SHORT_SHA DOCKER_TAG_MAIN: $DOCKER_IMAGE:development SOURCE_COMMIT: $CI_COMMIT_SHA only: - develop script: - docker build -t $DOCKER_TAG -t $DOCKER_TAG_MAIN --build-arg SOURCE_COMMIT --build-arg DOCKER_TAG . - docker push $DOCKER_TAG - docker push $DOCKER_TAG_MAIN First, we’re creating global variable DOCKER_IMAGE so we can reuse it later in many places. And in case we would like to change the image name, we do it only in a single place. Hence we have multiple jobs defined in .gitlab-ci.yml file, it’s good to utilize advanced YAML syntax, which includes hidden keys and anchors. It will decrease the duplicated code and make the file easier to read and maintain. Every job name starting with the dot will be considered as a hidden key and won’t be directly executed. Btw, this is a quick way to temporarily disable any job in your GitLab CI/CD without commenting or removing it. By using .docker-job: &amp;docker-job we created an anchor which can be later used to extend any job. If you add &lt;&lt;: *docker-job, it will populate image, services and before_script properties automatically. It’s a good move if you have multiple jobs that do similar things. Later on, we’re creating some additional local (job scoped) variables: DOCKER_TAG_MAIN which will be evaluated to hiveio/condenser:development DOCKER_TAG which will be evaluated to hiveio/condenser:344e55ef or similar SOURCE_COMMIT which will be evaluated to 344e55efefd56e00b15eea6ccf8560a1107b9ff6 (or similar commit SHA) It’s a good idea to double tag an image. Latest tag is useless if you want to track your development process. Later on, I will describe the way how we’re using this specific, sha-tagged image to track deployments and rollback them anytime with a single click. Finally, we’re building an image with additional build arguments by using --build-arg: docker build -t $DOCKER_TAG -t $DOCKER_TAG_MAIN --build-arg SOURCE_COMMIT --build-arg DOCKER_TAG . If you scroll up to the Dockerfile section, you will notice ARG SOURCE_COMMIT and ENV SOURCE_COMMIT ${SOURCE_COMMIT} which means these build arguments will be injected as an environment variables into your containers. It’s a quick and easy way to pass additional, build-level variables into your images. Those specific variables are later returned by the condenser health endpoint. It may be useful to check a specific instance source. $ curl &lt;a href=&quot;https://staging.condenser.engrave.dev/.well-known/healthcheck.json&quot;&gt;https://staging.condenser.engrave.dev/.well-known/healthcheck.json&lt;/a&gt; {&quot;status&quot;:&quot;ok&quot;,&quot;docker_tag&quot;:&quot;hiveio/condenser:344e55ef&quot;,&quot;source_commit&quot;:&quot;344e55efefd56e00b15eea6ccf8560a1107b9ff6&quot;} An important thing to mention is that HUB_TOKEN and HUB_USERNAME are environment variables injected into the GitLab runner job and configured in Projects Settings. To prevent unauthorized users from pushing malicious images into the official registry, those variables are configured as protected and masked, which means they can be only used on specific branches and are moderated from job logs, so there is no way it can leak without internal bad actor with elevated permissions. Merge requests To improve the process of adding new features and fixes to the codebase, Merge Requests have got a brand new CI/CD workflow which includes the following jobs: The entire pipeline is fired on every Merge Request and it’s required to pass before changes could be merged. If the pipeline fails for some reason (i.e. failing unit tests), there is no way to merge changes into the main branch. This will enforce code quality and prevent regression. Eslint Code quality and standardization are important, especially if it’s an open-source project that could be maintained by totally different developers from all around the world. Eslint is a tool that statically analyzes the code to quickly find potential problems and keep code organized with specified rules. Code analysis is especially useful when developing Javascript applications. It’s really easy to make some stupid mistakes. Eslint job will be fired on every Merge Request and on every branch pushed to the repository: run-eslint: stage: test image: node:12.16.2 only: - branches - merge_requests before_script: - yarn install --frozen-lockfile --ignore-optional script: - yarn ci:eslint allow_failure: true # will be changed to false when all linter errors removed Because the codebase is a bit neglected, run-eslint job is allowed to fail for the moment (allow_failure: true), which is indicated by an orange exclamation mark on a MR view or pipelines list. There are “some” errors and warnings right now but it should be cleaned up soon, so we can require eslint job to pass before merging proposed changes: ✖ 1208 problems (1187 errors, 21 warnings) 831 errors and 0 warnings potentially fixable with the `--fix` option. error Command failed with exit code 1. Unit tests Extensive testing is the only way to produce bullet-proof code and stable applications. Similar to run-eslint, run-unit-tests job is fired on every branch and every merge request. run-unit-tests: stage: test image: node:12.16.2 only: - branches - merge_requests before_script: - yarn install --frozen-lockfile --ignore-optional script: - yarn run ci:test coverage: /All files[^|]*\|[^|]*\s+([\d\.]+)/ Testing suit (jest) was configured to produce coverage report: This report is later parsed by a Gitlab, using coverage: /All files[^|]*\|[^|]*\s+([\d\.]+)/ configuration. It will display coverage status and percentage change on the Merge Request view, allowing reviewers to quickly inspect if the code quality is increasing or not. I would advise declining merging new features not covered with tests. This may be omitted for important fixes, but we all should try to make the code better, not worse. Also, repository settings were changed and Pipelines must succeed setting is checked by default. It means no more broken code on develop/production branches. Review apps Review Apps are a huge and very important feature. From now on, every feature can be inspected visually by the reviewer with a single click. Gitlab Runner will create a special instance built from proposed code and expose it for the reviewers: Review app requires three jobs to run on a merge request: build-review-app: &lt;&lt;: *docker-job stage: build variables: DOCKER_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA SOURCE_COMMIT: $CI_COMMIT_SHA only: - merge_requests before_script: - echo $CI_JOB_TOKEN | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin script: - docker build -t $DOCKER_TAG --build-arg SOURCE_COMMIT --build-arg DOCKER_TAG . - docker push $DOCKER_TAG deploy-review-app: &lt;&lt;: *docker-job &lt;&lt;: *docker-remote-host-review stage: deploy variables: DOCKER_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA SERVICE_NAME: review_$CI_ENVIRONMENT_SLUG only: - merge_requests script: - DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify pull $DOCKER_TAG - DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify service rm $SERVICE_NAME || true # try to remove previous service but do not fail if it not exist - DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify service create --with-registry-auth --network infrastructure --name $SERVICE_NAME $DOCKER_TAG - echo &quot;Review app deployed&quot; environment: name: review/$CI_COMMIT_REF_NAME url: &lt;a href=&quot;https://&quot;&gt;https://&lt;/a&gt; $CI_ENVIRONMENT_SLUG$APP_REVIEW_SUBDOMAIN on_stop: stop-review-app auto_stop_in: 1 week stop-review-app: &lt;&lt;: *docker-job &lt;&lt;: *docker-remote-host-review stage: deploy variables: SERVICE_NAME: review_$CI_ENVIRONMENT_SLUG only: - merge_requests when: manual script: - DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify service rm $SERVICE_NAME || true # try to remove previous service but do not fail if it not exist - echo &quot;Review app stopped&quot; environment: name: review/$CI_COMMIT_REF_NAME action: stop The first job should look familiar if you read previous parts of the post. The only difference is that we’re overwriting before_script. Note that if you’re using anchors, you can always overwrite the template freely which is what we did here. Because merge requests could be opened by developers which may not be well known in the community (vel. bad actors), it could be a security issue if the CI/CD on Merge Requests could push images to the official repository. To prevent this, we’re using an internal registry provided by the Gitlab itself. It’s private, will work well for Review Apps but won’t be accessible by anyone else. echo $CI_JOB_TOKEN | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin We are using CI_JOB_TOKEN, CI_REGISTRY_USER and CI_REGISTRY which are environment variables injected automatically by Gitlab, no need to configure them. Also, please note using --password-stdin which is a more secure way to log in as it will prevent the password from being exposed in job logs. By default, docker will connect to the local daemon on unsecured, but not exposed port. It is yet possible to configure Docker daemon to validate TLS certificates so it could be exposed to the external world in a secure way, which is how we deploy services from our runners to our machine. You need to pass additional parameters: -H which is remote docker address --tlsverify makes sure your daemon is trying to identify itself with certificates --with-registry-auth will send registry authentication to swarm agents --network will connect service to the specified network so the reverse proxy could expose the instance DOCKER_CERT_PATH will instruct demon where to search for TLS certificates DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify service create --with-registry-auth --network infrastructure --name $SERVICE_NAME $DOCKER_TAG This will create a new docker service and expose it automatically under the URL created with CI_ENVIRONMENT_SLUG, which is a variable provided by Gitlab. It’s guaranteed to be a valid URL or docker/kubernetes service name. Review App instances are automatically removed when MR is closed or 1 week after it’s opened. This is achieved by running stop-review-app job which is configured to be manually triggered (when: manual). Tracking environment deployments By using sha-tagged images, it’s possible to quickly redeploy the environment at any moment, with a single click. In case of emergency, project maintainers can rollback the environment to the specified point in time (docker image to be specific) Staging Deploying a staging environment is quite similar to deploying a Review App. It also uses remote docker daemon but the service is not created on-demand, it’s updated with a new image. With docker swarm mode, you can ensure your application to be highly available. The swarm agent will take care of your containers. It will restart them or spin a new one if necessary (this is why health check is so important). It is a built-in, native docker mode everyone should start using. deploy-staging: &lt;&lt;: *docker-job &lt;&lt;: *docker-remote-host-staging stage: deploy variables: DOCKER_TAG: $DOCKER_IMAGE:$CI_COMMIT_SHORT_SHA SERVICE_NAME: staging_condenser only: - develop script: - DOCKER_CERT_PATH=$CERTS docker -H $STAGING_HOST --tlsverify pull $DOCKER_TAG - DOCKER_CERT_PATH=$CERTS docker -H $STAGING_HOST --tlsverify service update --image $DOCKER_TAG --update-failure-action rollback --update-order start-first $SERVICE_NAME environment: name: staging url: &lt;a href=&quot;https://&quot;&gt;https://&lt;/a&gt; $STAGING_DOMAIN Job uses protected variables to prevent “bad” developers from pushing/deploying malicious code. It is only possible to push staging from the protected develop branch. Pushing directly to develop is disabled. Zero downtime deployments Updating single service is easy with docker swarm: DOCKER_CERT_PATH=$CERTS docker -H $STAGING_HOST --tlsverify service update --image $DOCKER_TAG --update-failure-action rollback --update-order start-first $SERVICE_NAME There are additional but important parameters provided: --image $DOCKER_TAG - update existing service by running new containers with the specified image. For this case, it’s sha-tagged image build from develop branch --update-failure-action rollback - by default, docker daemon will try to update the service and do nothing if it fails. By passing this parameter, we’re instructing docker to roll back the service to a previous state, which means containers using the previous image. --update-order start-first - by default, docker will kill current containers and spin new ones after it. It may cause some downtime which we don’t want. By setting start-first, we instruct docker to spin new containers first. Swarm agent will switch containers without downtime if the healtcheck result becomes positive. And in case something gone wrong (healtcheck failed for any reason), we end up with a working staging environment because old containers are not touched at all. Resources under control With docker swarm you have full control over your services and containers. This is an example configuration which is used for staging environment. With some tweaks, it could be used for production also: version: &quot;3.7&quot; services: condenser: image: hiveio/condenser:latest deploy: mode: replicated replicas: 2 resources: limits: cpus: &quot;0.85&quot; memory: 2024M restart_policy: condition: any delay: 5s update_config: parallelism: 1 delay: 10s failure_action: rollback order: start-first rollback_config: parallelism: 1 delay: 5s networks: - reverse-proxy networks: reverse-proxy: external: name: reverse-proxy Production It is not finished yet, but my advice is to start using docker swarm mode for production deployments (CI/CD is ready). It’s designed to serve services like a condenser. Some key features of Docker Swarm (based on official documentation): Cluster management integrated with Docker Engine: if you know how to build the docker image, start a container, read some logs, you’re ready to use Docker Swarm. You don’t need to install additional software as it is a native Docker feature. Decentralized design: adding a worker or a manager to your swarm is as easy as running a single command. Those could be machines from all around the world. Scaling: For each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state. Multi-host networking: You can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application. Load balancing: You can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes. Secure by default: Each node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA. Rolling updates: At rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll back to a previous version of the service. And after all, it is called Swarm, can’t be a coincidence! ;) I’m using Swarm mode for most of my projects. Within Engrave (dblog.org) swarm is automatically managing a set of 23 microservices with almost 30 containers at the moment. My requests and tips for condenser Developers (and not only) Please do write unit tests covering your code. Improving coverage will only cause fewer problems and bugs. Coverage is now visible on every merge request so it’s damn easy to see a regression. Take care of the code quality. Use exact package versions in package.json file. When using ^, it’s not guaranteed to install the same version on two different builds. And some developers like to introduce breaking changes without changing the major version number. Use alpine images to create minified production images. Use the latest images from official Docker HUB when possible. Use multi-stage builds to create leaner docker image Write and configure health checks for your applications Run eslint to clean up your code before you push it to the repository. You can use husky to ensure it happens automatically. Vote for @engrave witness if you find my work valuable See: Improvements for hive.blog, Continuous Integration and Deployments by @engrave" />
<link rel="canonical" href="https://peakd.com/hive-139531/@engrave/improvements-for-condenser-repository-continuous-integration-and-deployments" />
<meta property="og:url" content="https://peakd.com/hive-139531/@engrave/improvements-for-condenser-repository-continuous-integration-and-deployments" />
<meta property="og:site_name" content="Hive Chain Documentation" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-19T13:38:27-07:00" />
<script type="application/ld+json">
{"headline":"Improvements for Condenser repository, Continuous Integration and Deployments","dateModified":"2020-04-19T13:38:27-07:00","datePublished":"2020-04-19T13:38:27-07:00","url":"https://peakd.com/hive-139531/@engrave/improvements-for-condenser-repository-continuous-integration-and-deployments","mainEntityOfPage":{"@type":"WebPage","@id":"https://peakd.com/hive-139531/@engrave/improvements-for-condenser-repository-continuous-integration-and-deployments"},"author":{"@type":"Person","name":"engrave"},"description":"I want to elaborate not only on introduced features but also on my thought and reasoning so everyone can benefit and introduce changes in their projects. This post will be long but I believe it might be useful for the entire HiveDevs community. The workflow is built on top of Gitlab CI/CD but can be easily modified for any other CI/CD tool (i.e. CircleCI or Jenkins). Docker I’ve started my work by reviewing Dockerfile. I’ve built the condenser image and was a bit shocked: $ docker image ls condenser latest 6d57c0c8a904 19 seconds ago 1.54GB 1.54 GB for a simple frontend application? Boy, it’s just too much. Let’s do something with it. Leaner docker image Let’s start by recognizing the reason why is this so big. You can also look into your Dockerfile and package.json and search for some common mistakes: using full-sized images rather than alpine versions installing dev dependencies for production images putting dev dependencies into production dependencies list copying useless files into the final image Yep, you can check every point here. Let’s make some work on a new Dockerfile: FROM node:12.16.2 as development WORKDIR /var/app COPY package.json yarn.lock ./ RUN yarn install --non-interactive --frozen-lockfile --ignore-optional COPY . . RUN mkdir tmp &amp;&amp; yarn build CMD [ &quot;yarn&quot;, &quot;run&quot;, &quot;start&quot; ] ### REMOVE DEV DEPENDENCIES ## FROM development as dependencies RUN yarn install --non-interactive --frozen-lockfile --ignore-optional --production ### BUILD MINIFIED PRODUCTION ## FROM node:12.16.2-alpine as production WORKDIR /var/app ARG SOURCE_COMMIT ENV SOURCE_COMMIT ${SOURCE_COMMIT} ARG DOCKER_TAG ENV DOCKER_TAG ${DOCKER_TAG} COPY --from=dependencies /var/app/package.json /var/app/package.json COPY --from=dependencies /var/app/config /var/app/config COPY --from=dependencies /var/app/dist /var/app/dist COPY --from=dependencies /var/app/lib /var/app/lib COPY --from=dependencies /var/app/src /var/app/src COPY --from=dependencies /var/app/tmp /var/app/tmp COPY --from=dependencies /var/app/webpack /var/app/webpack COPY --from=dependencies /var/app/node_modules /var/app/node_modules COPY --from=dependencies /var/app/healthcheck.js /var/app/healthcheck.js HEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=5 CMD node /var/app/healthcheck.js CMD [ &quot;yarn&quot;, &quot;run&quot;, &quot;production&quot; ] What has been improved: Node.js version was upgraded from 8.7 to 12.16.2 which is the latest LTS at the moment. Always try using the latest framework versions, which may include security fixes and performance upgrades. It’s also a good habit to use a specific version up to the patch number. Multistage build was used to build an optimal image for production deployment. First, we build a development stage with every dependency to be able to compile React application. Next, we’re removing development dependencies with --production switch during the dependencies stage. After all, we’re creating a minimal image from the alpine node version which is the smallest base available, by copying only necessary files and directories. Healthcheck has been introduced, so the docker daemon can easily manage containers with automatic restarts if necessary and zero-downtime deployments which will be explained later in this post. It will be also useful for dynamic scaling capabilities with docker swarm. Also, package.json file was modified, but it’s not worth to post its content here: Dev dependencies like webpack, babel or eslint was moved into a proper list called devDependencies so yarn could install only production deps for the final image. Strict versions were introduced for every dependency to make sure every build produces exactly the same image. Semantic versioning is popular, but there is no way to check if the package author does not introduce breaking changes with only patch version increased. If you need to update a package, do it manually. If you have enough test coverage, you can update the package and run CI/CD to check if everything works fine. After all that work done, the condenser image size was massively reduced: $ docker image ls condenser latest 58406d338e67 8 seconds ago 226MB The compressed image in the docker registry is even smaller. Much better, right? Shame on you, Steemit! Healthcheck Simply speaking, docker is running containers and trying to keep it alive as long as possible. But the system needs to have a tool to determine if the container is actually alive. It may seem alive, but is your app responding to requests for example? Fortunately, docker has also integrated health check mechanism which can be integrated into Dockerfile or docker-compose.yml. Usually, you need to create an endpoint for liveness checks. Fortunately, condenser already has one so we can utilize it easily. There is a lot of examples with curl used as a docker health check, but it’s not a good way to go. Healthcheck should work cross-platform and curl implementation differs on Windows and Unix. You should write health check in the same technology or framework as your projects are written, for condenser it’s Node.js. const http = require(&quot;http&quot;); const options = { host: &quot;localhost&quot;, port: &quot;8080&quot;, path: &#39;/.well-known/healthcheck.json&#39;, timeout: 5000 }; const request = http.request(options, (res) =&gt; { console.log(`STATUS: ${res.statusCode}`); if (res.statusCode == 200) { process.exit(0); } else { console.error(`ERROR: ${res.statusCode}`); process.exit(1); } }); request.on(&#39;error&#39;, (err) =&gt; { console.error(&#39;ERROR&#39;, err); process.exit(1); }); request.end(); When ready, instruct Docker to use your health check mechanism. Add following line into your Dockefile: HEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=5 CMD node /var/app/healthcheck.js Make sure this file /var/app/healthcheck.js exists inside your image. If you want to be sure your health check is working, inspect your container after running it: $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 060166cf52ee hiveio/condenser:development &quot;docker-entrypoint.s…&quot; 5 minutes ago Up 5 minutes (healthy) 0.0.0.0:8080-&gt;8080/tcp mystifying_dhawan There should be a (healthy) indicator in STATUS column. And there is. Please also note that during the container startup process, it will indicate slightly different status (starting), as the docker daemon will wait before making the first check. It’s because we’re giving some time for our app to startup. It’s the start-period parameter. Depending on your app and traffic/load, those parameters should vary. Pushing images to Docker Hub We already have an improved (smaller) docker image, so it’s time to push it to the repository. Doing it manually is a waste of time and may cause human mistakes. The best way is to utilize Gitlab Runner to do it for us in an automatic and bullet-proof manner. Here is a job definition from .gitlab-ci.yml file with some additional code which we will breakdown: variables: DOCKER_IMAGE: hiveio/condenser .docker-job: &amp;docker-job image: docker:stable services: - docker:dind before_script: - echo $HUB_TOKEN | docker login -u $HUB_USERNAME --password-stdin build-development: &lt;&lt;: *docker-job stage: build variables: DOCKER_TAG: $DOCKER_IMAGE:$CI_COMMIT_SHORT_SHA DOCKER_TAG_MAIN: $DOCKER_IMAGE:development SOURCE_COMMIT: $CI_COMMIT_SHA only: - develop script: - docker build -t $DOCKER_TAG -t $DOCKER_TAG_MAIN --build-arg SOURCE_COMMIT --build-arg DOCKER_TAG . - docker push $DOCKER_TAG - docker push $DOCKER_TAG_MAIN First, we’re creating global variable DOCKER_IMAGE so we can reuse it later in many places. And in case we would like to change the image name, we do it only in a single place. Hence we have multiple jobs defined in .gitlab-ci.yml file, it’s good to utilize advanced YAML syntax, which includes hidden keys and anchors. It will decrease the duplicated code and make the file easier to read and maintain. Every job name starting with the dot will be considered as a hidden key and won’t be directly executed. Btw, this is a quick way to temporarily disable any job in your GitLab CI/CD without commenting or removing it. By using .docker-job: &amp;docker-job we created an anchor which can be later used to extend any job. If you add &lt;&lt;: *docker-job, it will populate image, services and before_script properties automatically. It’s a good move if you have multiple jobs that do similar things. Later on, we’re creating some additional local (job scoped) variables: DOCKER_TAG_MAIN which will be evaluated to hiveio/condenser:development DOCKER_TAG which will be evaluated to hiveio/condenser:344e55ef or similar SOURCE_COMMIT which will be evaluated to 344e55efefd56e00b15eea6ccf8560a1107b9ff6 (or similar commit SHA) It’s a good idea to double tag an image. Latest tag is useless if you want to track your development process. Later on, I will describe the way how we’re using this specific, sha-tagged image to track deployments and rollback them anytime with a single click. Finally, we’re building an image with additional build arguments by using --build-arg: docker build -t $DOCKER_TAG -t $DOCKER_TAG_MAIN --build-arg SOURCE_COMMIT --build-arg DOCKER_TAG . If you scroll up to the Dockerfile section, you will notice ARG SOURCE_COMMIT and ENV SOURCE_COMMIT ${SOURCE_COMMIT} which means these build arguments will be injected as an environment variables into your containers. It’s a quick and easy way to pass additional, build-level variables into your images. Those specific variables are later returned by the condenser health endpoint. It may be useful to check a specific instance source. $ curl &lt;a href=&quot;https://staging.condenser.engrave.dev/.well-known/healthcheck.json&quot;&gt;https://staging.condenser.engrave.dev/.well-known/healthcheck.json&lt;/a&gt; {&quot;status&quot;:&quot;ok&quot;,&quot;docker_tag&quot;:&quot;hiveio/condenser:344e55ef&quot;,&quot;source_commit&quot;:&quot;344e55efefd56e00b15eea6ccf8560a1107b9ff6&quot;} An important thing to mention is that HUB_TOKEN and HUB_USERNAME are environment variables injected into the GitLab runner job and configured in Projects Settings. To prevent unauthorized users from pushing malicious images into the official registry, those variables are configured as protected and masked, which means they can be only used on specific branches and are moderated from job logs, so there is no way it can leak without internal bad actor with elevated permissions. Merge requests To improve the process of adding new features and fixes to the codebase, Merge Requests have got a brand new CI/CD workflow which includes the following jobs: The entire pipeline is fired on every Merge Request and it’s required to pass before changes could be merged. If the pipeline fails for some reason (i.e. failing unit tests), there is no way to merge changes into the main branch. This will enforce code quality and prevent regression. Eslint Code quality and standardization are important, especially if it’s an open-source project that could be maintained by totally different developers from all around the world. Eslint is a tool that statically analyzes the code to quickly find potential problems and keep code organized with specified rules. Code analysis is especially useful when developing Javascript applications. It’s really easy to make some stupid mistakes. Eslint job will be fired on every Merge Request and on every branch pushed to the repository: run-eslint: stage: test image: node:12.16.2 only: - branches - merge_requests before_script: - yarn install --frozen-lockfile --ignore-optional script: - yarn ci:eslint allow_failure: true # will be changed to false when all linter errors removed Because the codebase is a bit neglected, run-eslint job is allowed to fail for the moment (allow_failure: true), which is indicated by an orange exclamation mark on a MR view or pipelines list. There are “some” errors and warnings right now but it should be cleaned up soon, so we can require eslint job to pass before merging proposed changes: ✖ 1208 problems (1187 errors, 21 warnings) 831 errors and 0 warnings potentially fixable with the `--fix` option. error Command failed with exit code 1. Unit tests Extensive testing is the only way to produce bullet-proof code and stable applications. Similar to run-eslint, run-unit-tests job is fired on every branch and every merge request. run-unit-tests: stage: test image: node:12.16.2 only: - branches - merge_requests before_script: - yarn install --frozen-lockfile --ignore-optional script: - yarn run ci:test coverage: /All files[^|]*\\|[^|]*\\s+([\\d\\.]+)/ Testing suit (jest) was configured to produce coverage report: This report is later parsed by a Gitlab, using coverage: /All files[^|]*\\|[^|]*\\s+([\\d\\.]+)/ configuration. It will display coverage status and percentage change on the Merge Request view, allowing reviewers to quickly inspect if the code quality is increasing or not. I would advise declining merging new features not covered with tests. This may be omitted for important fixes, but we all should try to make the code better, not worse. Also, repository settings were changed and Pipelines must succeed setting is checked by default. It means no more broken code on develop/production branches. Review apps Review Apps are a huge and very important feature. From now on, every feature can be inspected visually by the reviewer with a single click. Gitlab Runner will create a special instance built from proposed code and expose it for the reviewers: Review app requires three jobs to run on a merge request: build-review-app: &lt;&lt;: *docker-job stage: build variables: DOCKER_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA SOURCE_COMMIT: $CI_COMMIT_SHA only: - merge_requests before_script: - echo $CI_JOB_TOKEN | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin script: - docker build -t $DOCKER_TAG --build-arg SOURCE_COMMIT --build-arg DOCKER_TAG . - docker push $DOCKER_TAG deploy-review-app: &lt;&lt;: *docker-job &lt;&lt;: *docker-remote-host-review stage: deploy variables: DOCKER_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA SERVICE_NAME: review_$CI_ENVIRONMENT_SLUG only: - merge_requests script: - DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify pull $DOCKER_TAG - DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify service rm $SERVICE_NAME || true # try to remove previous service but do not fail if it not exist - DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify service create --with-registry-auth --network infrastructure --name $SERVICE_NAME $DOCKER_TAG - echo &quot;Review app deployed&quot; environment: name: review/$CI_COMMIT_REF_NAME url: &lt;a href=&quot;https://&quot;&gt;https://&lt;/a&gt; $CI_ENVIRONMENT_SLUG$APP_REVIEW_SUBDOMAIN on_stop: stop-review-app auto_stop_in: 1 week stop-review-app: &lt;&lt;: *docker-job &lt;&lt;: *docker-remote-host-review stage: deploy variables: SERVICE_NAME: review_$CI_ENVIRONMENT_SLUG only: - merge_requests when: manual script: - DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify service rm $SERVICE_NAME || true # try to remove previous service but do not fail if it not exist - echo &quot;Review app stopped&quot; environment: name: review/$CI_COMMIT_REF_NAME action: stop The first job should look familiar if you read previous parts of the post. The only difference is that we’re overwriting before_script. Note that if you’re using anchors, you can always overwrite the template freely which is what we did here. Because merge requests could be opened by developers which may not be well known in the community (vel. bad actors), it could be a security issue if the CI/CD on Merge Requests could push images to the official repository. To prevent this, we’re using an internal registry provided by the Gitlab itself. It’s private, will work well for Review Apps but won’t be accessible by anyone else. echo $CI_JOB_TOKEN | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin We are using CI_JOB_TOKEN, CI_REGISTRY_USER and CI_REGISTRY which are environment variables injected automatically by Gitlab, no need to configure them. Also, please note using --password-stdin which is a more secure way to log in as it will prevent the password from being exposed in job logs. By default, docker will connect to the local daemon on unsecured, but not exposed port. It is yet possible to configure Docker daemon to validate TLS certificates so it could be exposed to the external world in a secure way, which is how we deploy services from our runners to our machine. You need to pass additional parameters: -H which is remote docker address --tlsverify makes sure your daemon is trying to identify itself with certificates --with-registry-auth will send registry authentication to swarm agents --network will connect service to the specified network so the reverse proxy could expose the instance DOCKER_CERT_PATH will instruct demon where to search for TLS certificates DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify service create --with-registry-auth --network infrastructure --name $SERVICE_NAME $DOCKER_TAG This will create a new docker service and expose it automatically under the URL created with CI_ENVIRONMENT_SLUG, which is a variable provided by Gitlab. It’s guaranteed to be a valid URL or docker/kubernetes service name. Review App instances are automatically removed when MR is closed or 1 week after it’s opened. This is achieved by running stop-review-app job which is configured to be manually triggered (when: manual). Tracking environment deployments By using sha-tagged images, it’s possible to quickly redeploy the environment at any moment, with a single click. In case of emergency, project maintainers can rollback the environment to the specified point in time (docker image to be specific) Staging Deploying a staging environment is quite similar to deploying a Review App. It also uses remote docker daemon but the service is not created on-demand, it’s updated with a new image. With docker swarm mode, you can ensure your application to be highly available. The swarm agent will take care of your containers. It will restart them or spin a new one if necessary (this is why health check is so important). It is a built-in, native docker mode everyone should start using. deploy-staging: &lt;&lt;: *docker-job &lt;&lt;: *docker-remote-host-staging stage: deploy variables: DOCKER_TAG: $DOCKER_IMAGE:$CI_COMMIT_SHORT_SHA SERVICE_NAME: staging_condenser only: - develop script: - DOCKER_CERT_PATH=$CERTS docker -H $STAGING_HOST --tlsverify pull $DOCKER_TAG - DOCKER_CERT_PATH=$CERTS docker -H $STAGING_HOST --tlsverify service update --image $DOCKER_TAG --update-failure-action rollback --update-order start-first $SERVICE_NAME environment: name: staging url: &lt;a href=&quot;https://&quot;&gt;https://&lt;/a&gt; $STAGING_DOMAIN Job uses protected variables to prevent “bad” developers from pushing/deploying malicious code. It is only possible to push staging from the protected develop branch. Pushing directly to develop is disabled. Zero downtime deployments Updating single service is easy with docker swarm: DOCKER_CERT_PATH=$CERTS docker -H $STAGING_HOST --tlsverify service update --image $DOCKER_TAG --update-failure-action rollback --update-order start-first $SERVICE_NAME There are additional but important parameters provided: --image $DOCKER_TAG - update existing service by running new containers with the specified image. For this case, it’s sha-tagged image build from develop branch --update-failure-action rollback - by default, docker daemon will try to update the service and do nothing if it fails. By passing this parameter, we’re instructing docker to roll back the service to a previous state, which means containers using the previous image. --update-order start-first - by default, docker will kill current containers and spin new ones after it. It may cause some downtime which we don’t want. By setting start-first, we instruct docker to spin new containers first. Swarm agent will switch containers without downtime if the healtcheck result becomes positive. And in case something gone wrong (healtcheck failed for any reason), we end up with a working staging environment because old containers are not touched at all. Resources under control With docker swarm you have full control over your services and containers. This is an example configuration which is used for staging environment. With some tweaks, it could be used for production also: version: &quot;3.7&quot; services: condenser: image: hiveio/condenser:latest deploy: mode: replicated replicas: 2 resources: limits: cpus: &quot;0.85&quot; memory: 2024M restart_policy: condition: any delay: 5s update_config: parallelism: 1 delay: 10s failure_action: rollback order: start-first rollback_config: parallelism: 1 delay: 5s networks: - reverse-proxy networks: reverse-proxy: external: name: reverse-proxy Production It is not finished yet, but my advice is to start using docker swarm mode for production deployments (CI/CD is ready). It’s designed to serve services like a condenser. Some key features of Docker Swarm (based on official documentation): Cluster management integrated with Docker Engine: if you know how to build the docker image, start a container, read some logs, you’re ready to use Docker Swarm. You don’t need to install additional software as it is a native Docker feature. Decentralized design: adding a worker or a manager to your swarm is as easy as running a single command. Those could be machines from all around the world. Scaling: For each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state. Multi-host networking: You can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application. Load balancing: You can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes. Secure by default: Each node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA. Rolling updates: At rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll back to a previous version of the service. And after all, it is called Swarm, can’t be a coincidence! ;) I’m using Swarm mode for most of my projects. Within Engrave (dblog.org) swarm is automatically managing a set of 23 microservices with almost 30 containers at the moment. My requests and tips for condenser Developers (and not only) Please do write unit tests covering your code. Improving coverage will only cause fewer problems and bugs. Coverage is now visible on every merge request so it’s damn easy to see a regression. Take care of the code quality. Use exact package versions in package.json file. When using ^, it’s not guaranteed to install the same version on two different builds. And some developers like to introduce breaking changes without changing the major version number. Use alpine images to create minified production images. Use the latest images from official Docker HUB when possible. Use multi-stage builds to create leaner docker image Write and configure health checks for your applications Run eslint to clean up your code before you push it to the repository. You can use husky to ensure it happens automatically. Vote for @engrave witness if you find my work valuable See: Improvements for hive.blog, Continuous Integration and Deployments by @engrave","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://hivedocs.info/feed.xml" title="Hive Chain Documentation" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-161692811-1', 'auto');
  ga('send', 'pageview');
}
</script>
  
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/turbolinks/5.2.0/turbolinks.js" integrity="sha256-iM4Yzi/zLj/IshPWMC1IluRxTtRjMqjPGd97TZ9yYpU=" crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Cousine|Inconsolata" rel="stylesheet">
  <link rel="shortcut icon" type="image/png" href="/assets/images/favicon.png">
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Hive Chain Documentation</a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/news/">News</a><a class="page-link" href="/howto/">How To</a><a class="page-link" href="/links/">Useful Links</a><!-- HTML elements for search -->
          <input type="text" id="search-input" placeholder="Search" />
          
          <ul id="results-container"></ul>

          <script src="/assets/javascript/search.js"></script>
          <script>
            var sjs = SimpleJekyllSearch({
              searchInput: document.getElementById('search-input'),
              resultsContainer: document.getElementById('results-container'),
              json: '/search.json'
            })
          </script>
          <!-- <form style="float: right;" class="form-inline" method="get" action="http://www.google.com/search">
            <input type="hidden" name="sitesearch" value="hivedocs.info" />
            <input type="text" name="q" maxlength="255" placeholder="Search" />
          </form> -->
        </div>
      </nav>
  </div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Improvements for Condenser repository, Continuous Integration and Deployments</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-04-19T13:38:27-07:00" itemprop="datePublished">Apr 19, 2020
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">
          <a href="https://hive.blog/@engrave">engrave</a>
        </span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><img src="https://i.imgur.com/yCIuIaF.png" alt="" /></p>

<p><strong>I want to elaborate not only on introduced features but also on my thought and reasoning so everyone can benefit and introduce changes in their projects. This post will be long but I believe it might be useful for the entire HiveDevs community.</strong></p>

<p>The workflow is built on top of Gitlab CI/CD but can be easily modified for any other CI/CD tool (i.e. CircleCI or Jenkins).</p>

<h1 id="docker">Docker</h1>

<p>I’ve started my work by reviewing <code class="highlighter-rouge">Dockerfile</code>. I’ve built the <code class="highlighter-rouge">condenser</code> image and was a bit shocked:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ docker image ls
condenser   latest   6d57c0c8a904   19 seconds ago   1.54GB
</code></pre></div></div>

<p>1.54 GB for a simple frontend application? Boy, it’s just too much. Let’s do something with it.</p>

<h2 id="leaner-docker-image">Leaner docker image</h2>

<p>Let’s start by recognizing the reason why is this so big. You can also look into your <code class="highlighter-rouge">Dockerfile</code> and <code class="highlighter-rouge">package.json</code> and search for some common mistakes:</p>

<ul>
  <li>using full-sized images rather than <code class="highlighter-rouge">alpine</code> versions</li>
  <li>installing dev dependencies for production images</li>
  <li>putting dev dependencies into production dependencies list</li>
  <li>copying useless files into the final image</li>
</ul>

<p>Yep, you can check every point here. Let’s make some work on a new Dockerfile:</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> node:12.16.2 as development</span>

<span class="k">WORKDIR</span><span class="s"> /var/app</span>

<span class="k">COPY</span><span class="s"> package.json yarn.lock ./</span>

<span class="k">RUN </span>yarn <span class="nb">install</span> <span class="nt">--non-interactive</span> <span class="nt">--frozen-lockfile</span> <span class="nt">--ignore-optional</span>

<span class="k">COPY</span><span class="s"> . .</span>

<span class="k">RUN </span><span class="nb">mkdir </span>tmp <span class="o">&amp;&amp;</span> yarn build

<span class="k">CMD</span><span class="s"> [ "yarn", "run", "start" ]</span>

<span class="c">### REMOVE DEV DEPENDENCIES ##</span>
<span class="k">FROM</span><span class="s"> development as dependencies</span>

<span class="k">RUN </span>yarn <span class="nb">install</span> <span class="nt">--non-interactive</span> <span class="nt">--frozen-lockfile</span> <span class="nt">--ignore-optional</span> <span class="nt">--production</span>

<span class="c">### BUILD MINIFIED PRODUCTION ##</span>
<span class="k">FROM</span><span class="s"> node:12.16.2-alpine as production</span>

<span class="k">WORKDIR</span><span class="s"> /var/app</span>

<span class="k">ARG</span><span class="s"> SOURCE_COMMIT</span>
<span class="k">ENV</span><span class="s"> SOURCE_COMMIT ${SOURCE_COMMIT}</span>
<span class="k">ARG</span><span class="s"> DOCKER_TAG</span>
<span class="k">ENV</span><span class="s"> DOCKER_TAG ${DOCKER_TAG}</span>

<span class="k">COPY</span><span class="s"> --from=dependencies /var/app/package.json /var/app/package.json</span>
<span class="k">COPY</span><span class="s"> --from=dependencies /var/app/config /var/app/config</span>
<span class="k">COPY</span><span class="s"> --from=dependencies /var/app/dist /var/app/dist</span>
<span class="k">COPY</span><span class="s"> --from=dependencies /var/app/lib /var/app/lib</span>
<span class="k">COPY</span><span class="s"> --from=dependencies /var/app/src /var/app/src</span>
<span class="k">COPY</span><span class="s"> --from=dependencies /var/app/tmp /var/app/tmp</span>
<span class="k">COPY</span><span class="s"> --from=dependencies /var/app/webpack /var/app/webpack</span>
<span class="k">COPY</span><span class="s"> --from=dependencies /var/app/node_modules /var/app/node_modules</span>

<span class="k">COPY</span><span class="s"> --from=dependencies /var/app/healthcheck.js /var/app/healthcheck.js</span>

<span class="k">HEALTHCHECK</span><span class="s"> --interval=30s --timeout=5s --start-period=30s --retries=5 CMD node /var/app/healthcheck.js</span>

<span class="k">CMD</span><span class="s"> [ "yarn", "run", "production" ]</span>
</code></pre></div></div>

<p>What has been improved:</p>
<ul>
  <li>Node.js version was upgraded from <code class="highlighter-rouge">8.7</code> to <code class="highlighter-rouge">12.16.2</code> which is the latest LTS at the moment. Always try using the latest framework versions, which may include security fixes and performance upgrades. It’s also a good habit to use a specific version up to the <code class="highlighter-rouge">patch</code> number.</li>
  <li>Multistage build was used to build an optimal image for production deployment. First, we build a <code class="highlighter-rouge">development</code> stage with every dependency to be able to compile React application. Next, we’re removing development dependencies with <code class="highlighter-rouge">--production</code> switch during the <code class="highlighter-rouge">dependencies</code> stage. After all, we’re creating a minimal image from the <code class="highlighter-rouge">alpine</code> node version which is the smallest base available, by copying only necessary files and directories.</li>
  <li>Healthcheck has been introduced, so the docker daemon can easily manage containers with automatic restarts if necessary and zero-downtime deployments which will be explained later in this post. It will be also useful for dynamic scaling capabilities with <code class="highlighter-rouge">docker swarm</code>.</li>
</ul>

<p>Also, <code class="highlighter-rouge">package.json</code> file was modified, but it’s not worth to post its content here:</p>
<ul>
  <li>Dev dependencies like <code class="highlighter-rouge">webpack</code>, <code class="highlighter-rouge">babel</code> or <code class="highlighter-rouge">eslint</code> was moved into a proper list called <code class="highlighter-rouge">devDependencies</code> so <code class="highlighter-rouge">yarn</code> could install only production deps for the final image.</li>
  <li>Strict versions were introduced for every dependency to make sure every build produces exactly the same image. Semantic versioning is popular, but there is no way to check if the package author does not introduce breaking changes with only <code class="highlighter-rouge">patch</code> version increased. If you need to update a package, do it manually. If you have enough test coverage, you can update the package and run CI/CD to check if everything works fine.</li>
</ul>

<p>After all that work done, the condenser image size was massively reduced:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ docker image ls
condenser   latest   58406d338e67   8 seconds ago   226MB
</code></pre></div></div>

<p><img src="https://i.imgur.com/xGtpXCQ.png" alt="" /></p>

<p><img src="https://i.imgur.com/aCCoCud.png" alt="" /></p>

<p>The compressed image in the docker registry is even smaller. Much better, right? Shame on you, Steemit!</p>

<h2 id="healthcheck">Healthcheck</h2>

<p>Simply speaking, docker is running containers and trying to keep it alive as long as possible. But the system needs to have a tool to determine if the container is <code class="highlighter-rouge">actually alive</code>. It may seem alive, but is your app responding to requests for example? Fortunately, docker has also integrated health check mechanism which can be integrated into <code class="highlighter-rouge">Dockerfile</code> or <code class="highlighter-rouge">docker-compose.yml</code>. Usually, you need to create an endpoint for liveness checks. Fortunately, condenser already has one so we can utilize it easily.</p>

<p>There is a lot of examples with <code class="highlighter-rouge">curl</code> used as a docker health check, but it’s not a good way to go. Healthcheck should work cross-platform and <code class="highlighter-rouge">curl</code> implementation differs on Windows and Unix. You should write health check in the same technology or framework as your projects are written, for condenser it’s Node.js.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">const</span> <span class="nx">http</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="dl">"</span><span class="s2">http</span><span class="dl">"</span><span class="p">);</span>

<span class="kd">const</span> <span class="nx">options</span> <span class="o">=</span> <span class="p">{</span>
    <span class="na">host</span><span class="p">:</span> <span class="dl">"</span><span class="s2">localhost</span><span class="dl">"</span><span class="p">,</span>
    <span class="na">port</span><span class="p">:</span> <span class="dl">"</span><span class="s2">8080</span><span class="dl">"</span><span class="p">,</span>
    <span class="na">path</span><span class="p">:</span> <span class="dl">'</span><span class="s1">/.well-known/healthcheck.json</span><span class="dl">'</span><span class="p">,</span>
    <span class="na">timeout</span><span class="p">:</span> <span class="mi">5000</span>
<span class="p">};</span>

<span class="kd">const</span> <span class="nx">request</span> <span class="o">=</span> <span class="nx">http</span><span class="p">.</span><span class="nx">request</span><span class="p">(</span><span class="nx">options</span><span class="p">,</span> <span class="p">(</span><span class="nx">res</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="s2">`STATUS: </span><span class="p">${</span><span class="nx">res</span><span class="p">.</span><span class="nx">statusCode</span><span class="p">}</span><span class="s2">`</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="nx">res</span><span class="p">.</span><span class="nx">statusCode</span> <span class="o">==</span> <span class="mi">200</span><span class="p">)</span> <span class="p">{</span>
        <span class="nx">process</span><span class="p">.</span><span class="nx">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">else</span> <span class="p">{</span>
        <span class="nx">console</span><span class="p">.</span><span class="nx">error</span><span class="p">(</span><span class="s2">`ERROR: </span><span class="p">${</span><span class="nx">res</span><span class="p">.</span><span class="nx">statusCode</span><span class="p">}</span><span class="s2">`</span><span class="p">);</span>
        <span class="nx">process</span><span class="p">.</span><span class="nx">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">});</span>

<span class="nx">request</span><span class="p">.</span><span class="nx">on</span><span class="p">(</span><span class="dl">'</span><span class="s1">error</span><span class="dl">'</span><span class="p">,</span> <span class="p">(</span><span class="nx">err</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nx">error</span><span class="p">(</span><span class="dl">'</span><span class="s1">ERROR</span><span class="dl">'</span><span class="p">,</span> <span class="nx">err</span><span class="p">);</span>
    <span class="nx">process</span><span class="p">.</span><span class="nx">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="p">});</span>

<span class="nx">request</span><span class="p">.</span><span class="nx">end</span><span class="p">();</span>
</code></pre></div></div>

<p>When ready, instruct Docker to use your health check mechanism. Add following line into your <code class="highlighter-rouge">Dockefile</code>:</p>

<p><code class="highlighter-rouge">HEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=5 CMD node /var/app/healthcheck.js</code></p>

<p>Make sure this file <code class="highlighter-rouge">/var/app/healthcheck.js</code> exists inside your image. If you want to be sure your health check is working, inspect your container after running it:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ docker container ls
CONTAINER ID        IMAGE                          COMMAND                  CREATED             STATUS                   PORTS                    NAMES
060166cf52ee        hiveio/condenser:development   "docker-entrypoint.s…"   5 minutes ago       Up 5 minutes (healthy)   0.0.0.0:8080-&gt;8080/tcp   mystifying_dhawan
</code></pre></div></div>

<p>There should be a <code class="highlighter-rouge">(healthy)</code> indicator in <code class="highlighter-rouge">STATUS</code> column. And there is. Please also note that during the container startup process, it will indicate slightly different status (<code class="highlighter-rouge">starting</code>), as the docker daemon will wait before making the first check. It’s because we’re giving some time for our app to startup. It’s the <code class="highlighter-rouge">start-period</code> parameter.</p>

<p>Depending on your app and traffic/load, those parameters should vary.</p>

<h2 id="pushing-images-to-docker-hub">Pushing images to Docker Hub</h2>

<p>We already have an improved (smaller) docker image, so it’s time to push it to the repository. Doing it manually is a waste of time and may cause human mistakes. The best way is to utilize Gitlab Runner to do it for us in an automatic and bullet-proof manner.</p>

<p>Here is a <code class="highlighter-rouge">job</code> definition from <code class="highlighter-rouge">.gitlab-ci.yml</code> file with some additional code which we will breakdown:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">variables</span><span class="pi">:</span>
    <span class="na">DOCKER_IMAGE</span><span class="pi">:</span> <span class="s">hiveio/condenser</span>

<span class="s">.docker-job</span><span class="pi">:</span> <span class="nl">&amp;docker-job</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">docker:stable</span>
    <span class="na">services</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">docker:dind</span>
    <span class="na">before_script</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">echo $HUB_TOKEN | docker login -u $HUB_USERNAME --password-stdin</span>

<span class="na">build-development</span><span class="pi">:</span>
    <span class="s">&lt;&lt;</span><span class="pi">:</span> <span class="nv">*docker-job</span>
    <span class="na">stage</span><span class="pi">:</span> <span class="s">build</span>
    <span class="na">variables</span><span class="pi">:</span>
        <span class="na">DOCKER_TAG</span><span class="pi">:</span> <span class="s">$DOCKER_IMAGE:$CI_COMMIT_SHORT_SHA</span>
        <span class="na">DOCKER_TAG_MAIN</span><span class="pi">:</span> <span class="s">$DOCKER_IMAGE:development</span>
        <span class="na">SOURCE_COMMIT</span><span class="pi">:</span> <span class="s">$CI_COMMIT_SHA</span>
    <span class="na">only</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">develop</span>
    <span class="na">script</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">docker build -t $DOCKER_TAG -t $DOCKER_TAG_MAIN --build-arg SOURCE_COMMIT --build-arg DOCKER_TAG .</span>
        <span class="pi">-</span> <span class="s">docker push $DOCKER_TAG</span>
        <span class="pi">-</span> <span class="s">docker push $DOCKER_TAG_MAIN</span>
</code></pre></div></div>

<p>First, we’re creating global variable <code class="highlighter-rouge">DOCKER_IMAGE</code> so we can reuse it later in many places. And in case we would like to change the image name, we do it only in a single place.</p>

<p>Hence we have multiple <code class="highlighter-rouge">jobs</code> defined in <code class="highlighter-rouge">.gitlab-ci.yml</code> file, it’s good to utilize advanced YAML syntax, which includes <strong>hidden keys</strong> and <strong>anchors</strong>. It will decrease the duplicated code and make the file easier to read and maintain. Every job name starting with the dot will be considered as a hidden key and <strong>won’t be directly executed</strong>. Btw, this is a quick way to temporarily disable any job in your GitLab CI/CD without commenting or removing it.</p>

<p>By using <code class="highlighter-rouge">.docker-job: &amp;docker-job</code> we created an anchor which can be later used to extend any job. If you add <code class="highlighter-rouge">&lt;&lt;: *docker-job</code>, it will populate <code class="highlighter-rouge">image</code>, <code class="highlighter-rouge">services</code> and <code class="highlighter-rouge">before_script</code> properties automatically. It’s a good move if you have multiple jobs that do similar things.</p>

<p>Later on, we’re creating some additional local (job scoped) variables:</p>
<ul>
  <li><code class="highlighter-rouge">DOCKER_TAG_MAIN</code>  which will be evaluated to <code class="highlighter-rouge">hiveio/condenser:development</code></li>
  <li><code class="highlighter-rouge">DOCKER_TAG</code> which will be evaluated to <code class="highlighter-rouge">hiveio/condenser:344e55ef</code> or similar</li>
  <li><code class="highlighter-rouge">SOURCE_COMMIT</code> which will be evaluated to <code class="highlighter-rouge">344e55efefd56e00b15eea6ccf8560a1107b9ff6</code> (or similar commit SHA)</li>
</ul>

<p>It’s a good idea to double tag an image. <code class="highlighter-rouge">Latest</code> tag is useless if you want to track your development process. Later on, I will describe the way how we’re using this specific, <code class="highlighter-rouge">sha</code>-tagged image to track deployments and rollback them anytime with a single click.</p>

<p>Finally, we’re building an image with additional build arguments by using <code class="highlighter-rouge">--build-arg</code>:</p>

<p><code class="highlighter-rouge">docker build -t $DOCKER_TAG -t $DOCKER_TAG_MAIN --build-arg SOURCE_COMMIT --build-arg DOCKER_TAG .</code></p>

<p>If you scroll up to the Dockerfile section, you will notice <code class="highlighter-rouge">ARG SOURCE_COMMIT</code> and <code class="highlighter-rouge">ENV SOURCE_COMMIT ${SOURCE_COMMIT}</code> which means these build arguments will be injected as an environment variables into your containers. It’s a quick and easy way to pass additional, build-level variables into your images. Those specific variables are later returned by the condenser health endpoint. It may be useful to check a specific instance source.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>curl 
&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">"https://staging.condenser.engrave.dev/.well-known/healthcheck.json"</span><span class="o">&gt;</span>https://staging.condenser.engrave.dev/.well-known/healthcheck.json&lt;/a&gt;

<span class="o">{</span><span class="s2">"status"</span>:<span class="s2">"ok"</span>,<span class="s2">"docker_tag"</span>:<span class="s2">"hiveio/condenser:344e55ef"</span>,<span class="s2">"source_commit"</span>:<span class="s2">"344e55efefd56e00b15eea6ccf8560a1107b9ff6"</span><span class="o">}</span>
</code></pre></div></div>

<hr />

<p>An important thing to mention is that <code class="highlighter-rouge">HUB_TOKEN</code> and <code class="highlighter-rouge">HUB_USERNAME</code> are environment variables injected into the GitLab runner job and configured in Projects Settings. To prevent unauthorized users from pushing malicious images into the official registry, those variables are configured as <code class="highlighter-rouge">protected</code> and <code class="highlighter-rouge">masked</code>, which means they can be only used on specific branches and are moderated from job logs, so there is no way it can leak without internal bad actor with elevated permissions.</p>

<p><img src="https://i.imgur.com/o3FAaQb.png" alt="" /></p>

<h1 id="merge-requests">Merge requests</h1>

<p>To improve the process of adding new features and fixes to the codebase, Merge Requests have got a brand new CI/CD workflow which includes the following jobs:</p>

<p><img src="https://i.imgur.com/3kV2zPi.png" alt="" /></p>

<p>The entire pipeline is fired on every Merge Request and it’s required to pass before changes could be merged. If the pipeline fails for some reason (i.e. failing unit tests), there is no way to merge changes into the main branch. This will enforce code quality and prevent regression.</p>

<h2 id="eslint">Eslint</h2>

<p>Code quality and standardization are important, especially if it’s an open-source project that could be maintained by totally different developers from all around the world. <code class="highlighter-rouge">Eslint</code> is a tool that statically analyzes the code to quickly find potential problems and keep code organized with specified rules. Code analysis is especially useful when developing Javascript applications. It’s really easy to make some stupid mistakes.</p>

<p>Eslint job will be fired on every Merge Request and on every branch pushed to the repository:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">run-eslint</span><span class="pi">:</span>
    <span class="na">stage</span><span class="pi">:</span> <span class="s">test</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">node:12.16.2</span>
    <span class="na">only</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">branches</span>
        <span class="pi">-</span> <span class="s">merge_requests</span>
    <span class="na">before_script</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">yarn install --frozen-lockfile --ignore-optional</span>
    <span class="na">script</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">yarn ci:eslint</span>
    <span class="na">allow_failure</span><span class="pi">:</span> <span class="no">true</span> <span class="c1"># will be changed to false when all linter errors removed</span>
</code></pre></div></div>

<p>Because the codebase is a bit neglected, <code class="highlighter-rouge">run-eslint</code> job is allowed to fail for the moment (<code class="highlighter-rouge">allow_failure: true</code>), which is indicated by an orange exclamation mark on a MR view or pipelines list. There are “some” errors and warnings right now but it should be cleaned up soon, so we can require eslint job to pass before merging proposed changes:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ✖ 1208 problems (1187 errors, 21 warnings)
   831 errors and 0 warnings potentially fixable with the `--fix` option.
 error Command failed with exit code 1.
</code></pre></div></div>

<h2 id="unit-tests">Unit tests</h2>

<p>Extensive testing is the only way to produce bullet-proof code and stable applications. Similar to <code class="highlighter-rouge">run-eslint</code>, <code class="highlighter-rouge">run-unit-tests</code> job is fired on every branch and every merge request.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">run-unit-tests</span><span class="pi">:</span>
    <span class="na">stage</span><span class="pi">:</span> <span class="s">test</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">node:12.16.2</span>
    <span class="na">only</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">branches</span>
        <span class="pi">-</span> <span class="s">merge_requests</span>
    <span class="na">before_script</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">yarn install --frozen-lockfile --ignore-optional</span>
    <span class="na">script</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">yarn run ci:test</span>
    <span class="na">coverage</span><span class="pi">:</span> <span class="s">/All files[^|]*\|[^|]*\s+([\d\.]+)/</span>
</code></pre></div></div>

<p>Testing suit (<code class="highlighter-rouge">jest</code>) was configured to produce coverage report:</p>

<p><img src="https://i.imgur.com/43JrdbE.png" alt="" /></p>

<p>This report is later parsed by a Gitlab, using <code class="highlighter-rouge">coverage: /All files[^|]*\|[^|]*\s+([\d\.]+)/</code> configuration. It will display coverage status and percentage change on the Merge Request view, allowing reviewers to quickly inspect if the code quality is increasing or not.</p>

<p><img src="https://i.imgur.com/Yn6lbe8.png" alt="" /></p>

<p>I would advise declining merging new features not covered with tests. This may be omitted for important fixes, but we all should try to make the code better, not worse.</p>

<p>Also, repository settings were changed and <code class="highlighter-rouge">Pipelines must succeed</code> setting is checked by default. It means no more broken code on develop/production branches.</p>

<h2 id="review-apps">Review apps</h2>

<p><code class="highlighter-rouge">Review Apps</code> are a huge and very important feature. From now on, every feature can be inspected visually by the reviewer with a single click. Gitlab Runner will create a special instance built from proposed code and expose it for the reviewers:</p>

<p><img src="https://i.imgur.com/FOEhZ6d.png" alt="" /></p>

<p>Review app requires three jobs to run on a merge request:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">build-review-app</span><span class="pi">:</span>
    <span class="s">&lt;&lt;</span><span class="pi">:</span> <span class="nv">*docker-job</span>
    <span class="na">stage</span><span class="pi">:</span> <span class="s">build</span>
    <span class="na">variables</span><span class="pi">:</span>
        <span class="na">DOCKER_TAG</span><span class="pi">:</span> <span class="s">$CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA</span>
        <span class="na">SOURCE_COMMIT</span><span class="pi">:</span> <span class="s">$CI_COMMIT_SHA</span>
    <span class="na">only</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">merge_requests</span>
    <span class="na">before_script</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">echo $CI_JOB_TOKEN | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin</span>
    <span class="na">script</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">docker build -t $DOCKER_TAG --build-arg SOURCE_COMMIT --build-arg DOCKER_TAG .</span>
        <span class="pi">-</span> <span class="s">docker push $DOCKER_TAG</span>

<span class="na">deploy-review-app</span><span class="pi">:</span>
    <span class="s">&lt;&lt;</span><span class="pi">:</span> <span class="nv">*docker-job</span>
    <span class="s">&lt;&lt;</span><span class="pi">:</span> <span class="nv">*docker-remote-host-review</span>
    <span class="na">stage</span><span class="pi">:</span> <span class="s">deploy</span>
    <span class="na">variables</span><span class="pi">:</span>
        <span class="na">DOCKER_TAG</span><span class="pi">:</span> <span class="s">$CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA</span>
        <span class="na">SERVICE_NAME</span><span class="pi">:</span> <span class="s">review_$CI_ENVIRONMENT_SLUG</span>
    <span class="na">only</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">merge_requests</span>
    <span class="na">script</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify pull $DOCKER_TAG</span>
        <span class="pi">-</span> <span class="s">DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify service rm $SERVICE_NAME || </span><span class="no">true</span> <span class="c1"># try to remove previous service but do not fail if it not exist</span>
        <span class="pi">-</span> <span class="s">DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify service create --with-registry-auth --network infrastructure --name $SERVICE_NAME $DOCKER_TAG</span>
        <span class="pi">-</span> <span class="s">echo "Review app deployed"</span>
    <span class="na">environment</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">review/$CI_COMMIT_REF_NAME</span>
        <span class="na">url</span><span class="pi">:</span> 
<span class="s">&lt;a href="https://"&gt;https://&lt;/a&gt;</span>
<span class="s">$CI_ENVIRONMENT_SLUG$APP_REVIEW_SUBDOMAIN</span>
        <span class="s">on_stop</span><span class="pi">:</span> <span class="s">stop-review-app</span>
        <span class="s">auto_stop_in</span><span class="pi">:</span> <span class="s">1 week</span>
        
<span class="na">stop-review-app</span><span class="pi">:</span>
    <span class="s">&lt;&lt;</span><span class="pi">:</span> <span class="nv">*docker-job</span>
    <span class="s">&lt;&lt;</span><span class="pi">:</span> <span class="nv">*docker-remote-host-review</span>
    <span class="na">stage</span><span class="pi">:</span> <span class="s">deploy</span>
    <span class="na">variables</span><span class="pi">:</span>
        <span class="na">SERVICE_NAME</span><span class="pi">:</span> <span class="s">review_$CI_ENVIRONMENT_SLUG</span>
    <span class="na">only</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">merge_requests</span>
    <span class="na">when</span><span class="pi">:</span> <span class="s">manual</span>
    <span class="na">script</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify service rm $SERVICE_NAME || </span><span class="no">true</span> <span class="c1"># try to remove previous service but do not fail if it not exist</span>
        <span class="pi">-</span> <span class="s">echo "Review app stopped"</span>
    <span class="na">environment</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">review/$CI_COMMIT_REF_NAME</span>
        <span class="na">action</span><span class="pi">:</span> <span class="s">stop</span>
</code></pre></div></div>

<p>The first job should look familiar if you read previous parts of the post. The only difference is that we’re overwriting <code class="highlighter-rouge">before_script</code>. Note that if you’re using <strong>anchors</strong>, you can always overwrite the template freely which is what we did here.</p>

<p>Because merge requests could be opened by developers which may not be well known in the community (vel. bad actors), it could be a security issue if the CI/CD on Merge Requests could push images to the official repository. To prevent this, we’re using an internal registry provided by the Gitlab itself. It’s private, will work well for Review Apps but won’t be accessible by anyone else.</p>

<p><code class="highlighter-rouge">echo $CI_JOB_TOKEN | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin</code></p>

<p>We are using <code class="highlighter-rouge">CI_JOB_TOKEN</code>, <code class="highlighter-rouge">CI_REGISTRY_USER</code> and <code class="highlighter-rouge">CI_REGISTRY</code> which are <a href="https://docs.gitlab.com/ee/ci/variables/predefined_variables.html">environment variables injected automatically by Gitlab</a>, no need to configure them. Also, please note using <code class="highlighter-rouge">--password-stdin</code> which is a more secure way to log in as it will prevent the password from being exposed in job logs.</p>

<p>By default, docker will connect to the local daemon on unsecured, but not exposed port. It is yet possible to configure Docker daemon to validate TLS certificates so it could be exposed to the external world in a secure way, which is how we deploy services from our runners to our machine. You need to pass additional parameters:</p>
<ul>
  <li><code class="highlighter-rouge">-H</code> which is remote docker address</li>
  <li><code class="highlighter-rouge">--tlsverify</code> makes sure your daemon is trying to identify itself with certificates</li>
  <li><code class="highlighter-rouge">--with-registry-auth</code> will send registry authentication to swarm agents</li>
  <li><code class="highlighter-rouge">--network</code> will connect service to the specified network so the reverse proxy could expose the instance</li>
  <li><code class="highlighter-rouge">DOCKER_CERT_PATH</code> will instruct demon where to search for TLS certificates</li>
</ul>

<p><code class="highlighter-rouge">DOCKER_CERT_PATH=$CERTS docker -H $REVIEW_HOST --tlsverify service create --with-registry-auth --network infrastructure --name $SERVICE_NAME $DOCKER_TAG</code></p>

<p>This will create a new <code class="highlighter-rouge">docker service</code> and expose it automatically under the URL created with <code class="highlighter-rouge">CI_ENVIRONMENT_SLUG</code>, which is a variable provided by Gitlab. It’s guaranteed to be a valid URL or docker/kubernetes service name.</p>

<p>Review App instances are automatically removed when MR is closed or 1 week after it’s opened. This is achieved by running <code class="highlighter-rouge">stop-review-app</code> job which is configured to be manually triggered (<code class="highlighter-rouge">when: manual</code>).</p>

<h1 id="tracking-environment-deployments">Tracking environment deployments</h1>

<p><img src="https://i.imgur.com/8ncGrE9.png" alt="" /></p>

<p>By using <code class="highlighter-rouge">sha</code>-tagged images, it’s possible to quickly redeploy the environment at any moment, with a single click. In case of emergency, project maintainers can rollback the environment to the specified point in time (docker image to be specific)</p>

<h1 id="staging">Staging</h1>

<p>Deploying a staging environment is quite similar to deploying a Review App. It also uses remote docker daemon but the service is not created on-demand, it’s updated with a new image.</p>

<p>With <code class="highlighter-rouge">docker swarm</code> mode, you can ensure your application to be highly available. The swarm agent will take care of your containers. It will restart them or spin a new one if necessary (this is why health check is so important). It is a built-in, native docker mode everyone should start using.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="na">deploy-staging</span><span class="pi">:</span>
    <span class="s">&lt;&lt;</span><span class="pi">:</span> <span class="nv">*docker-job</span>
    <span class="s">&lt;&lt;</span><span class="pi">:</span> <span class="nv">*docker-remote-host-staging</span>
    <span class="na">stage</span><span class="pi">:</span> <span class="s">deploy</span>
    <span class="na">variables</span><span class="pi">:</span>
        <span class="na">DOCKER_TAG</span><span class="pi">:</span> <span class="s">$DOCKER_IMAGE:$CI_COMMIT_SHORT_SHA</span>
        <span class="na">SERVICE_NAME</span><span class="pi">:</span> <span class="s">staging_condenser</span>
    <span class="na">only</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">develop</span>
    <span class="na">script</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">DOCKER_CERT_PATH=$CERTS docker -H $STAGING_HOST --tlsverify pull $DOCKER_TAG</span>
        <span class="pi">-</span> <span class="s">DOCKER_CERT_PATH=$CERTS docker -H $STAGING_HOST --tlsverify service update --image $DOCKER_TAG --update-failure-action rollback --update-order start-first $SERVICE_NAME</span>
    <span class="na">environment</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">staging</span>
        <span class="na">url</span><span class="pi">:</span> 
<span class="s">&lt;a href="https://"&gt;https://&lt;/a&gt;</span>
<span class="s">$STAGING_DOMAIN</span>
</code></pre></div></div>

<p>Job uses protected variables to prevent  “bad” developers from pushing/deploying malicious code. It is only possible to push staging from the protected <code class="highlighter-rouge">develop</code> branch. Pushing directly to <code class="highlighter-rouge">develop</code> is disabled.</p>

<h2 id="zero-downtime-deployments">Zero downtime deployments</h2>

<p>Updating single service is easy with <code class="highlighter-rouge">docker swarm</code>:</p>

<p><code class="highlighter-rouge">DOCKER_CERT_PATH=$CERTS docker -H $STAGING_HOST --tlsverify service update --image $DOCKER_TAG --update-failure-action rollback --update-order start-first $SERVICE_NAME</code></p>

<p>There are additional but important parameters provided:</p>
<ul>
  <li><code class="highlighter-rouge">--image $DOCKER_TAG</code> - update existing service by running new containers with the specified image. For this case, it’s sha-tagged image build from develop branch</li>
  <li><code class="highlighter-rouge">--update-failure-action rollback</code> - by default, docker daemon will try to update the service and do nothing if it fails. By passing this parameter, we’re instructing docker to roll back the service to a previous state, which means containers using the previous image.</li>
  <li><code class="highlighter-rouge">--update-order start-first</code> - by default, docker will kill current containers and spin new ones after it. It may cause some downtime which we don’t want. By setting <code class="highlighter-rouge">start-first</code>, we instruct docker to spin new containers first. Swarm agent will switch containers without downtime if the healtcheck result becomes positive. And in case something gone wrong (healtcheck failed for any reason), we end up with a working staging environment because old containers are not touched at all.</li>
</ul>

<h2 id="resources-under-control">Resources under control</h2>

<p>With <code class="highlighter-rouge">docker swarm</code> you have full control over your services and containers. This is an example configuration which is used for staging environment. With some tweaks, it could be used for production also:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">version</span><span class="pi">:</span> <span class="s2">"</span><span class="s">3.7"</span>
<span class="na">services</span><span class="pi">:</span>
    <span class="na">condenser</span><span class="pi">:</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">hiveio/condenser:latest</span>
        <span class="na">deploy</span><span class="pi">:</span>
            <span class="na">mode</span><span class="pi">:</span> <span class="s">replicated</span>
            <span class="na">replicas</span><span class="pi">:</span> <span class="m">2</span>
            <span class="na">resources</span><span class="pi">:</span>
                <span class="na">limits</span><span class="pi">:</span>
                    <span class="na">cpus</span><span class="pi">:</span> <span class="s2">"</span><span class="s">0.85"</span>
                    <span class="na">memory</span><span class="pi">:</span> <span class="s">2024M</span>
            <span class="na">restart_policy</span><span class="pi">:</span>
                <span class="na">condition</span><span class="pi">:</span> <span class="s">any</span>
                <span class="na">delay</span><span class="pi">:</span> <span class="s">5s</span>
            <span class="na">update_config</span><span class="pi">:</span>
                <span class="na">parallelism</span><span class="pi">:</span> <span class="m">1</span>
                <span class="na">delay</span><span class="pi">:</span> <span class="s">10s</span>
                <span class="na">failure_action</span><span class="pi">:</span> <span class="s">rollback</span>
                <span class="na">order</span><span class="pi">:</span> <span class="s">start-first</span>
            <span class="na">rollback_config</span><span class="pi">:</span>
                <span class="na">parallelism</span><span class="pi">:</span> <span class="m">1</span>
                <span class="na">delay</span><span class="pi">:</span> <span class="s">5s</span>
        <span class="na">networks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s">reverse-proxy</span>

<span class="na">networks</span><span class="pi">:</span>
    <span class="na">reverse-proxy</span><span class="pi">:</span>
        <span class="na">external</span><span class="pi">:</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">reverse-proxy</span>
</code></pre></div></div>

<h1 id="production">Production</h1>

<p>It is not finished yet, but my advice is to start using <code class="highlighter-rouge">docker swarm</code> mode for production deployments (CI/CD is ready). It’s designed to serve services like a condenser.</p>

<p>Some key features of Docker Swarm (based on official documentation):</p>

<ul>
  <li>
    <p><strong>Cluster management integrated with Docker Engine</strong>: if you know how to build the docker image, start a container, read some logs, you’re ready to use Docker Swarm. You don’t need to install additional software as it is a native Docker feature.</p>
  </li>
  <li>
    <p><strong>Decentralized design</strong>: adding a worker or a manager to your swarm is as easy as running a single command. Those could be machines from all around the world.</p>
  </li>
  <li>
    <p><strong>Scaling</strong>: For each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.</p>
  </li>
  <li>
    <p><strong>Multi-host networking</strong>: You can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.</p>
  </li>
  <li>
    <p><strong>Load balancing</strong>: You can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.</p>
  </li>
  <li>
    <p><strong>Secure by default</strong>: Each node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.</p>
  </li>
  <li>
    <p><strong>Rolling updates</strong>: At rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll back to a previous version of the service.</p>
  </li>
</ul>

<p>And after all, it is called <code class="highlighter-rouge">Swarm</code>, can’t be a coincidence! ;)</p>

<p>I’m using Swarm mode for most of my projects. Within Engrave (dblog.org) swarm is automatically managing a set of 23 microservices with almost 30 containers at the moment.</p>

<p><img src="https://i.imgur.com/JMs3d48.png" alt="" /></p>

<h1 id="my-requests-and-tips-for-condenser-developers-and-not-only">My requests and tips for condenser Developers (and not only)</h1>

<ul>
  <li>Please do write unit tests covering your code. Improving coverage will only cause fewer problems and bugs. Coverage is now visible on every merge request so it’s damn easy to see a regression. Take care of the code quality.</li>
  <li>Use exact package versions in <code class="highlighter-rouge">package.json</code> file. When using <code class="highlighter-rouge">^</code>, it’s not guaranteed to install the same version on two different builds. And some developers like to introduce breaking changes without changing the <code class="highlighter-rouge">major</code> version number.</li>
  <li>Use <code class="highlighter-rouge">alpine</code> images to create minified production images.</li>
  <li>Use the latest images from official Docker HUB when possible.</li>
  <li>Use <a href="https://docs.docker.com/develop/develop-images/multistage-build/">multi-stage builds</a> to create leaner docker image</li>
  <li>Write and configure health checks for your applications</li>
  <li>Run <code class="highlighter-rouge">eslint</code> to clean up your code before you push it to the repository. You can use <code class="highlighter-rouge">husky</code> to ensure it happens automatically.</li>
</ul>

<h1 id="vote-for-engrave-witness-if-you-find-my-work-valuable">Vote for @engrave witness if you find my work valuable</h1>

<p><a href="https://hivesigner.com/sign/account-witness-vote?witness=engrave&amp;approve=1"><img src="https://i.imgur.com/lawFvZF.png" alt="Vote for @engrave witness" /></a></p>
<hr />

<p>
  See: <a href="https://peakd.com/hive-139531/@engrave/improvements-for-condenser-repository-continuous-integration-and-deployments">Improvements for hive.blog, Continuous Integration and Deployments</a>
  by
  <a href="https://hive.blog/@engrave">@engrave</a>
</p>


  </div><a class="u-url" href="/howto/devops/docker/2020/04/19/improvements-for-condenser-repository-continuous-integration-and-deployments.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading"><a rel="author" href="/">Hive Chain Documentation</a></h2>
    
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">site curated by: @inertia</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Your resource for various levels of Hive Documentation.</p>
      </div>
    </div>
  </div>

</footer>
</body>

</html>
