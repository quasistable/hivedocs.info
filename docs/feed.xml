<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://hivedocs.info/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hivedocs.info/" rel="alternate" type="text/html" /><updated>2022-05-07T12:47:06-07:00</updated><id>https://hivedocs.info/feed.xml</id><title type="html">Hive Chain Documentation</title><subtitle>Your resource for various levels of Hive Documentation.</subtitle><author><name>site curated by: @inertia</name></author><entry><title type="html">7th update of 2022 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2022/05/05/7th-update-of-2022-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="7th update of 2022 on BlockTrades work on Hive software" /><published>2022-05-05T14:11:51-07:00</published><updated>2022-05-05T14:11:51-07:00</updated><id>https://hivedocs.info/news/core/development/2022/05/05/7th-update-of-2022-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2022/05/05/7th-update-of-2022-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-7th-update-of-2022-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)

Below are highlights of some of the Hive-related programming issues worked on by the BlockTrades team since my last post.

# Hived (blockchain node software) work


### Mirrornet (testnet that mirrors traffic from mainnet) to test p2p code

We started testing the develop branch of hived with the mirrornet code this week. In this process, we found and fixed a couple of problems with the mirrornet itself (it wasn’t compatible with default BOOST library on Ubuntu20 and transactions ported from blocks in the mainnet were using the block time as their expiration time instead of their original expiration time). 

More importantly, we found a longstanding performance issue with the peer-to-peer code that limits the rate at which transactions can be shared over the network: each node requests at most one transaction at a time from each of its peers. 

So, for example, with an average latency of 100ms to all its peers, a node could get at most 30 transactions per peer during a block interval (3s block interval / 0.1s = 30). 

This problem showed up very clearly when we were testing a mirror net configured with only 3 nodes, with one node receiving the transactions from the mirrornet and sharing the transactions to a close-by node (1ms latency) and a far-away node (200ms latency). The close-by node was able to receive the transactions in a timely manner and fill it’s blocks with transactions, but the far-away node was receiving transactions at such a low transaction rate that it got the transactions after they were either expired or put into the other producer’s block, so it was only able to create 0-transaction blocks. 

Empirically, we’ve observed this problem occasionally on the mainnet as well, for nodes that are far away from the other nodes during high-traffic times, but the problems have been transient and therefore difficult to analyze. In the more constrained environment of the mirrornet the problem was much easier to reproduce and analyze because we had full control of the network topology and access to all the nodes in the network. It was also helpful to be able to run all the nodes with the new p2p code, since that allowed us to discount potential performance problems that were already fixed related to the locking protocol between the p2p code and the blockchain processing code during our search for the performance bottleneck.

### Planned p2p performance enhancement

To address the issue above, we’re updating the p2p code to allow a node to request more than one transaction at a time from a peer. This brings up issues of how a node can best load balance its transaction requests to its peers, since multiple peers can have the same set of transactions the node wants to fetch. 

Previously, load balancing was achieved automatically: since we only allowed one active request to any peer, the node would just round-robin its requests to peers, for the most part. Since we are now allowing multiple active requests to a peer, we need a new load-balancing algorithm. We’re using two simple heuristics for this: 1) the code won’t request more than 100 transactions from any given peer at a time and 2) we will favor peers that have low latency over high latency. This should result in transactions getting shared to peers as quickly as possible.

As a side note, I suspect that the above performance problem has probably been limiting average block size as well and preventing many “full” blocks, so once we’ve implemented the above enhancement we will also take a look at how the updated code functions in scenarios where we get a series of “full blocks” since this has never been tested much (if at all) in the past.

### Completed implementation of OBI (one-block irreversibility) protocol
As mentioned last week, the new block finality protocol has been coded and is waiting for testing resources to free up. I’ll publish a separate post about the implementation and effects of the new protocol on Monday.

# Hive Application Framework (HAF)

### Reduced size of hashes stored in HAF databases
As reported last week, we’ve changed the storage mechanism now to store them as a pure binary hash, which reduces the size of the transactions table (and some indexes) and also should improve performance somewhat. Benchmarking showed that this reduced overall HAF database size by about 10%.

### Benchmarking of various HAF server configurations (using ZFS in particular)

We’ve continued to benchmark HAF running in various hardware and software configurations to figure out optimal configurations for HAF servers in terms of performance and cost effectiveness. The majority of the experiments we’ve done so far are recorded here (it doesn’t include some of the more exotic tests we’ve done such as enabling huge_pages and use of postgres CLUSTER command, nor does it include a set of various pg_bench tests we’ve performed yet): https://gitlab.syncad.com/hive/haf/-/wikis/benchmarks

### Dockerized version of HAF server

We’ve begun experimenting with various dockerized variations of a HAF server, to determine which configuration will offer the most performance while offering the easiest options for server setup and administration.

# HAF account history app (aka hafah)

We’ve created a dockerized version of hafah (with postgrest server) that will be used by CI tests, and we’ll be adding performance results for it to the benchmark table above soon (hopefully tonight). 

Assuming this version performs as well as expected based on previous standalone benchmarks, we’ll replace the current python-based hafah with the dockerized postgrest-based one on our production API node (api.hive.blog) in the next few days.

# Hivemind (social media middleware server used by web sites)

This week we identified and fixed the remaining test fails for the new HAF-based hivemind that replaces the old hivemind, so the next steps will be to do some cleanup work and then begin performance testing.

# Some upcoming tasks

* Implement p2p protocol changes to increase transaction-sharing bandwidth and test changes on mirrornet.
* Modify hived to process transactions containing either NAI based assets or legacy assets.
* Complete work on resource credit rationalization.
* More dockerization and CI improvements for HAF and HAF apps.
* Collect benchmarks for a hafah app operating in “irreversible block mode” and compare to a hafah app operating in “normal” mode.
* Test postgrest-based hafah on production server (api.hive.blog).
* Clean up and benchmarking of HAF-based hivemind. If this goes well, we will also deploy and test on our API node.
* Finished testing of new one-block irreversibility (OBI) code.

# When hardfork 26?

Two weeks ago we were mostly in a “testing-only” mode, so I was hopeful we could still complete the hardfork by the end of this month, since we only had one outstanding big change to hived (RC credit rationalization). But yesterday we discovered that we needed to code up two new features: a fix for the p2p bottleneck and support for transactions with NAI and legacy assets in hived (originally it appeared that this work was complete, but further investigation revealed that there were cases that weren’t covered).

With three significant code changes still to be made to hived, I think it is best to push back the hardfork date till the end of next month, in order to allow proper testing time for all these new changes once they are implemented (especially as the last two tasks are being done by the same developer).&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/7th-update-of-2022-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-7th-update-of-2022-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-7th-update-of-2022-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-7th-update-of-2022-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-7th-update-of-2022-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-7th-update-of-2022-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-7th-update-of-2022-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-7th-update-of-2022-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-7th-update-of-2022-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-7th-update-of-2022-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-7th-update-of-2022-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/7th-update-of-2022-on-blocktrades-work-on-hive-software&quot;&gt;7th update of 2022 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below are highlights of some of the Hive-related programming issues worked on by the BlockTrades team since my last post. # Hived (blockchain node software) work ### Mirrornet (testnet that mirrors traffic from mainnet) to test p2p code We started testing the develop branch of hived with the mirrornet code this week. In this process, we found and fixed a couple of problems with the mirrornet itself (it wasn’t compatible with default BOOST library on Ubuntu20 and transactions ported from blocks in the mainnet were using the block time as their expiration time instead of their original expiration time). More importantly, we found a longstanding performance issue with the peer-to-peer code that limits the rate at which transactions can be shared over the network: each node requests at most one transaction at a time from each of its peers. So, for example, with an average latency of 100ms to all its peers, a node could get at most 30 transactions per peer during a block interval (3s block interval / 0.1s = 30). This problem showed up very clearly when we were testing a mirror net configured with only 3 nodes, with one node receiving the transactions from the mirrornet and sharing the transactions to a close-by node (1ms latency) and a far-away node (200ms latency). The close-by node was able to receive the transactions in a timely manner and fill it’s blocks with transactions, but the far-away node was receiving transactions at such a low transaction rate that it got the transactions after they were either expired or put into the other producer’s block, so it was only able to create 0-transaction blocks. Empirically, we’ve observed this problem occasionally on the mainnet as well, for nodes that are far away from the other nodes during high-traffic times, but the problems have been transient and therefore difficult to analyze. In the more constrained environment of the mirrornet the problem was much easier to reproduce and analyze because we had full control of the network topology and access to all the nodes in the network. It was also helpful to be able to run all the nodes with the new p2p code, since that allowed us to discount potential performance problems that were already fixed related to the locking protocol between the p2p code and the blockchain processing code during our search for the performance bottleneck. ### Planned p2p performance enhancement To address the issue above, we’re updating the p2p code to allow a node to request more than one transaction at a time from a peer. This brings up issues of how a node can best load balance its transaction requests to its peers, since multiple peers can have the same set of transactions the node wants to fetch. Previously, load balancing was achieved automatically: since we only allowed one active request to any peer, the node would just round-robin its requests to peers, for the most part. Since we are now allowing multiple active requests to a peer, we need a new load-balancing algorithm. We’re using two simple heuristics for this: 1) the code won’t request more than 100 transactions from any given peer at a time and 2) we will favor peers that have low latency over high latency. This should result in transactions getting shared to peers as quickly as possible. As a side note, I suspect that the above performance problem has probably been limiting average block size as well and preventing many “full” blocks, so once we’ve implemented the above enhancement we will also take a look at how the updated code functions in scenarios where we get a series of “full blocks” since this has never been tested much (if at all) in the past. ### Completed implementation of OBI (one-block irreversibility) protocol As mentioned last week, the new block finality protocol has been coded and is waiting for testing resources to free up. I’ll publish a separate post about the implementation and effects of the new protocol on Monday. # Hive Application Framework (HAF) ### Reduced size of hashes stored in HAF databases As reported last week, we’ve changed the storage mechanism now to store them as a pure binary hash, which reduces the size of the transactions table (and some indexes) and also should improve performance somewhat. Benchmarking showed that this reduced overall HAF database size by about 10%. ### Benchmarking of various HAF server configurations (using ZFS in particular) We’ve continued to benchmark HAF running in various hardware and software configurations to figure out optimal configurations for HAF servers in terms of performance and cost effectiveness. The majority of the experiments we’ve done so far are recorded here (it doesn’t include some of the more exotic tests we’ve done such as enabling huge_pages and use of postgres CLUSTER command, nor does it include a set of various pg_bench tests we’ve performed yet): https://gitlab.syncad.com/hive/haf/-/wikis/benchmarks ### Dockerized version of HAF server We’ve begun experimenting with various dockerized variations of a HAF server, to determine which configuration will offer the most performance while offering the easiest options for server setup and administration. # HAF account history app (aka hafah) We’ve created a dockerized version of hafah (with postgrest server) that will be used by CI tests, and we’ll be adding performance results for it to the benchmark table above soon (hopefully tonight). Assuming this version performs as well as expected based on previous standalone benchmarks, we’ll replace the current python-based hafah with the dockerized postgrest-based one on our production API node (api.hive.blog) in the next few days. # Hivemind (social media middleware server used by web sites) This week we identified and fixed the remaining test fails for the new HAF-based hivemind that replaces the old hivemind, so the next steps will be to do some cleanup work and then begin performance testing. # Some upcoming tasks * Implement p2p protocol changes to increase transaction-sharing bandwidth and test changes on mirrornet. * Modify hived to process transactions containing either NAI based assets or legacy assets. * Complete work on resource credit rationalization. * More dockerization and CI improvements for HAF and HAF apps. * Collect benchmarks for a hafah app operating in “irreversible block mode” and compare to a hafah app operating in “normal” mode. * Test postgrest-based hafah on production server (api.hive.blog). * Clean up and benchmarking of HAF-based hivemind. If this goes well, we will also deploy and test on our API node. * Finished testing of new one-block irreversibility (OBI) code. # When hardfork 26? Two weeks ago we were mostly in a “testing-only” mode, so I was hopeful we could still complete the hardfork by the end of this month, since we only had one outstanding big change to hived (RC credit rationalization). But yesterday we discovered that we needed to code up two new features: a fix for the p2p bottleneck and support for transactions with NAI and legacy assets in hived (originally it appeared that this work was complete, but further investigation revealed that there were cases that weren’t covered). With three significant code changes still to be made to hived, I think it is best to push back the hardfork date till the end of next month, in order to allow proper testing time for all these new changes once they are implemented (especially as the last two tasks are being done by the same developer). See: 7th update of 2022 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Hive Mirrornet (a.k.a. Fakenet) is up and running!</title><link href="https://hivedocs.info/testnet/2022/04/30/hive-mirrornet-a-k-a-fakenet-is-up-and-running.html" rel="alternate" type="text/html" title="Hive Mirrornet (a.k.a. Fakenet) is up and running!" /><published>2022-04-30T13:47:18-07:00</published><updated>2022-04-30T13:47:18-07:00</updated><id>https://hivedocs.info/testnet/2022/04/30/hive-mirrornet-a-k-a-fakenet-is-up-and-running</id><content type="html" xml:base="https://hivedocs.info/testnet/2022/04/30/hive-mirrornet-a-k-a-fakenet-is-up-and-running.html">&lt;div id=&quot;content-gtg-hive-mirrornet-a-k-a-fakenet-is-up-and-running&quot;&gt;Long story short: 
Public Hive Mirrornet seed instance is up and running.

`p2p-seed-node = seed.fake.openhive.network:2001`
`--chain-id 42`

https://gtg.openhive.network/get/testnet/mirror/

That’s pretty much all you need to start playing with the mirror. Have fun.

If you don’t understand the above, don’t worry, now I’ll try to explain what is all about.

&lt;center&gt;https://www.youtube.com/watch?v=vOV2srXzNW4 &lt;sup&gt;- Of course, such post is a good excuse to publish yet another Hive logo reveal&lt;/sup&gt;&lt;/center&gt;

# Nothing is true on the mirror net
It might look the same, it might feel the same, but it’s fake.
The only purpose of its existence is research and development of the Hive ecosystem.
We will break it, shatter it, and re-create it over and over again.
Of course you are very welcome to join and play in this large scale sandbox, but …

# YOU HAVE TO BE CAREFUL WITH THE MIRROR
The most important thing is to remember that everything you see in the mirror is fake. Assets are not real and they have no value. Accounts are fake and you shouldn’t assume that @gandalf in the mirror is the same @gandalf on the Hive. Your actions on the Hive can affect the mirror, but things you do in the mirror can’t affect the Hive.
You can do crazy things with your mirror account without a fear of breaking something in the real world, but that’s true as long as you can distinguish the mirror from reality. Also make sure that tools that you are using for chain interaction can tell such a difference.

It’s not that easy to make such a mistake because while the private keys on both networks might be the same and many transactions are the same, there are different chain IDs and resulting hashes, so the TaPoS keeps us from replay attacks, etc.
Given that, it requires some extra effort to shoot ourselves in the foot, but…

&amp;gt; _”Two things are infinite, the universe and human stupidity, and I am not yet completely sure about the universe.”_

Well, just be careful please. :-)

# Testnet vs Mirrornet

#### Testnet
- Easy and quick to set up
- Low resource requirements:
   - Seed node or witness node takes 2GB for block log and 5GB for shared memory file.
  - API node takes extra 3GB for account history storage
  - Hivemind’s database takes 2GB
- Bootstrap process creates accounts based on mainnet
- Tool called `tinman-gatling` can port live transactions from mainnet to testnet to mimic activity
- A lot of mimicked transactions are going to fail because of huge difference with the mainnet
- TESTS and TBD as native assets
#### Mirrornet
- Way more time consuming to set up, less tools currently
- High resource requirements (similar to the mainnet):
  - Seed node or witness node takes 500GB for block log and 16GB for shared memory file.
  - API node takes extra 750GB for account history storage
  - Hivemind’s database takes 650GB
- Initial “conversion” turns mainnet blocks to mirror (i.e. accounts, balances, transactions)
- Node based converter ports live transactions from mainnet to mirrornet to mimic activity
- Some of mimicked transactions are going to fail because of small difference with the mainnet
- HIVE and HBD as native assets (but of course fake)

# Recipe for Copy&amp;amp;Paste Experts
```
# Go to a directory where you want to set up your node
# Create subdirs
mkdir -pv mirror-consensus/blockchain mirror-consensus/snapshot
# Get the block_log
wget https://gtg.openhive.network/get/testnet/mirror/block_log -O mirror-consensus/blockchain/block_log
# Get the snapshot (unpack on the fly)
wget https://gtg.openhive.network/get/testnet/mirror/mirror-consensus-bootstrap.tar.bz2 -O - | tar -xjvC mirror-consensus/snapshot
# Optionally get the block_log index
wget https://gtg.openhive.network/get/testnet/mirror/block_log.index -O mirror-consensus/blockchain/block_log.index
# Get binaries
wget https://gtg.openhive.network/get/testnet/mirror/hived-mirror-develop
wget https://gtg.openhive.network/get/testnet/mirror/cli_wallet-mirror-develop
# Make them executable
chmod -c 755 {hived,cli_wallet}-mirror-develop
# Get configuration
wget https://gtg.openhive.network/get/testnet/mirror/config.ini -O mirror-consensus/config.ini
# Run it for the first time:
./hived-mirror-develop –chain-id 42 -d mirror-consensus --load-snapshot mirror-consensus-bootstrap
# Next time you just need to run
./hived-mirror-develop –chain-id 42 -d mirror-consensus
```

# What's next?
That's just a seed node and a pack of block producers (I'm pretending to be multiple witnesses until they take over), hopefully soon other peers will join (real fake producers), and once we have a fully featured API node running (in the next week or so) - dApps could start to test their stuff.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@gtg&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/hive-mirrornet-a-k-a-fakenet-is-up-and-running&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-gtg-hive-mirrornet-a-k-a-fakenet-is-up-and-running').html();
      const outputElem = $('#content-gtg-hive-mirrornet-a-k-a-fakenet-is-up-and-running');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-gtg-hive-mirrornet-a-k-a-fakenet-is-up-and-running {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-gtg-hive-mirrornet-a-k-a-fakenet-is-up-and-running code {
    background: white;
  }
  #content-gtg-hive-mirrornet-a-k-a-fakenet-is-up-and-running a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-gtg-hive-mirrornet-a-k-a-fakenet-is-up-and-running a:hover {
    border-bottom: 0;
  }
  #content-gtg-hive-mirrornet-a-k-a-fakenet-is-up-and-running h1 {
    font-size: 2.2em;
  }
  #content-gtg-hive-mirrornet-a-k-a-fakenet-is-up-and-running h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-gtg-hive-mirrornet-a-k-a-fakenet-is-up-and-running header small {
    color: #999;
    font-size: 50%;
  }
  #content-gtg-hive-mirrornet-a-k-a-fakenet-is-up-and-running img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-160391/@gtg/hive-mirrornet-a-k-a-fakenet-is-up-and-running&quot;&gt;Hive Mirrornet (a.k.a. Fakenet) is up and running!&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@gtg&quot;&gt;@gtg&lt;/a&gt;
&lt;/p&gt;</content><author><name>gtg</name></author><category term="testnet" /><summary type="html">Long story short: Public Hive Mirrornet seed instance is up and running. `p2p-seed-node = seed.fake.openhive.network:2001` `--chain-id 42` https://gtg.openhive.network/get/testnet/mirror/ That’s pretty much all you need to start playing with the mirror. Have fun. If you don’t understand the above, don’t worry, now I’ll try to explain what is all about. https://www.youtube.com/watch?v=vOV2srXzNW4 - Of course, such post is a good excuse to publish yet another Hive logo reveal # Nothing is true on the mirror net It might look the same, it might feel the same, but it’s fake. The only purpose of its existence is research and development of the Hive ecosystem. We will break it, shatter it, and re-create it over and over again. Of course you are very welcome to join and play in this large scale sandbox, but … # YOU HAVE TO BE CAREFUL WITH THE MIRROR The most important thing is to remember that everything you see in the mirror is fake. Assets are not real and they have no value. Accounts are fake and you shouldn’t assume that @gandalf in the mirror is the same @gandalf on the Hive. Your actions on the Hive can affect the mirror, but things you do in the mirror can’t affect the Hive. You can do crazy things with your mirror account without a fear of breaking something in the real world, but that’s true as long as you can distinguish the mirror from reality. Also make sure that tools that you are using for chain interaction can tell such a difference. It’s not that easy to make such a mistake because while the private keys on both networks might be the same and many transactions are the same, there are different chain IDs and resulting hashes, so the TaPoS keeps us from replay attacks, etc. Given that, it requires some extra effort to shoot ourselves in the foot, but… &amp;gt; _”Two things are infinite, the universe and human stupidity, and I am not yet completely sure about the universe.”_ Well, just be careful please. :-) # Testnet vs Mirrornet #### Testnet - Easy and quick to set up - Low resource requirements: - Seed node or witness node takes 2GB for block log and 5GB for shared memory file. - API node takes extra 3GB for account history storage - Hivemind’s database takes 2GB - Bootstrap process creates accounts based on mainnet - Tool called `tinman-gatling` can port live transactions from mainnet to testnet to mimic activity - A lot of mimicked transactions are going to fail because of huge difference with the mainnet - TESTS and TBD as native assets #### Mirrornet - Way more time consuming to set up, less tools currently - High resource requirements (similar to the mainnet): - Seed node or witness node takes 500GB for block log and 16GB for shared memory file. - API node takes extra 750GB for account history storage - Hivemind’s database takes 650GB - Initial “conversion” turns mainnet blocks to mirror (i.e. accounts, balances, transactions) - Node based converter ports live transactions from mainnet to mirrornet to mimic activity - Some of mimicked transactions are going to fail because of small difference with the mainnet - HIVE and HBD as native assets (but of course fake) # Recipe for Copy&amp;amp;Paste Experts ``` # Go to a directory where you want to set up your node # Create subdirs mkdir -pv mirror-consensus/blockchain mirror-consensus/snapshot # Get the block_log wget https://gtg.openhive.network/get/testnet/mirror/block_log -O mirror-consensus/blockchain/block_log # Get the snapshot (unpack on the fly) wget https://gtg.openhive.network/get/testnet/mirror/mirror-consensus-bootstrap.tar.bz2 -O - | tar -xjvC mirror-consensus/snapshot # Optionally get the block_log index wget https://gtg.openhive.network/get/testnet/mirror/block_log.index -O mirror-consensus/blockchain/block_log.index # Get binaries wget https://gtg.openhive.network/get/testnet/mirror/hived-mirror-develop wget https://gtg.openhive.network/get/testnet/mirror/cli_wallet-mirror-develop # Make them executable chmod -c 755 {hived,cli_wallet}-mirror-develop # Get configuration wget https://gtg.openhive.network/get/testnet/mirror/config.ini -O mirror-consensus/config.ini # Run it for the first time: ./hived-mirror-develop –chain-id 42 -d mirror-consensus --load-snapshot mirror-consensus-bootstrap # Next time you just need to run ./hived-mirror-develop –chain-id 42 -d mirror-consensus ``` # What's next? That's just a seed node and a pack of block producers (I'm pretending to be multiple witnesses until they take over), hopefully soon other peers will join (real fake producers), and once we have a fully featured API node running (in the next week or so) - dApps could start to test their stuff. See: Hive Mirrornet (a.k.a. Fakenet) is up and running! by @gtg</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://img.youtube.com/vi/vOV2srXzNW4/0.jpg" /><media:content medium="image" url="https://img.youtube.com/vi/vOV2srXzNW4/0.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Hive Projects weekly update: 3 projects added, 163 listed in total!</title><link href="https://hivedocs.info/news/projects/2022/04/30/hiveprojects-weekly-update-2022-04-30-50237.html" rel="alternate" type="text/html" title="Hive Projects weekly update: 3 projects added, 163 listed in total!" /><published>2022-04-30T09:00:03-07:00</published><updated>2022-04-30T09:00:03-07:00</updated><id>https://hivedocs.info/news/projects/2022/04/30/hiveprojects-weekly-update-2022-04-30-50237</id><content type="html" xml:base="https://hivedocs.info/news/projects/2022/04/30/hiveprojects-weekly-update-2022-04-30-50237.html">&lt;div id=&quot;content-engrave-hiveprojects-weekly-update-2022-04-30-50237&quot;&gt;![](https://files.peakd.com/file/peakd-hive/engrave/IHxPwBD1-hiveprojects_update.png)

# What is [Hive Projects](https://hiveprojects.io)?

[Hive Projects](https://hiveprojects.io) is the biggest directory of apps, sites, tools, and scripts created for the Hive ecosystem. This website is an entirely volunteer-driven effort. That includes coding time and hosting costs. If you wish to help or show your gratitude, there are plenty of ways in which you can do that:
 * upvote this post
 * reblog or cross-post it into your favorite community
 * contribute to HiveProjects, by adding a new project - everyone can do it!
 * let us know about new project, by posting about it in [Hive Projects Community](https://peakd.com/c/hive-192847) or by cross-posting to it or simply paste us a link at [Engrave Discord server](https://discord.gg/8NktdFh)
 * vote for our @engrave witness
 * write a comment :)

***


Previous post: [Hive Projects weekly update: 4 projects added, 160 listed in total!](/@engrave/hiveprojects-weekly-update-2022-04-19-25719)

***



# Newly added projects 



## Crossworlds
**Team:** 

![](https://hiveprojects.io/media/imgs/173/1651158721711.jpg)

**Category:** [Games](https://hiveprojects.io/categories/games/)

**Description:** *Crossworlds is a blockchain-based MMO game that operates using smart contract
interaction between every game asset to provide self-sufficient gameplay mechanics and economics.
There are three key directions of roleplay that players can choose from for their own preference or even all of
them together at the same time such as mining, crafting, and fighting. Game tokenomics are built in a way to give
users freedom: diverging owned game assets control on the market and between players, which allows them to
create their own earning strategy inside the game which naturally decentralizes game ownership.*

[Crossworlds on HiveProjects.io](https://hiveprojects.io/projects/p/crossworlds/)



## TelePay
**Team:** @telepay, @lugodev, @calvarez95dev

![](https://hiveprojects.io/media/imgs/172/1651026406084.png)

**Category:** [Wallets](https://hiveprojects.io/categories/wallets/)

**Description:** *Payments gateway with Hive and HBD. Create merchants, get your API keys, and process payments easily.*

[TelePay on HiveProjects.io](https://hiveprojects.io/projects/p/telepay/)



## HIVE Monitor
**Team:** @primersion

![](https://hiveprojects.io/media/imgs/171/1650402566645.png)

**Category:** [Witness Tools](https://hiveprojects.io/categories/witness-tools/)

**Description:** *A web application for viewing and voting for HIVE and Hive-Engine witnesses. Additionally the page provides an overview of the earnings of witnesses and some useful resources for users that want to start a witness node.*

[HIVE Monitor on HiveProjects.io](https://hiveprojects.io/projects/p/hive-monitor/)



***

&lt;center&gt;

**That would be all for today. Stay tuned for the next update and consider contributing to Hive Projects. It is a community-driven website.**

**Click on the image to vote for @engrave witness:**

[![banner_engrave 100.png](https://images.hive.blog/DQmUghvic5TCPPvVkB4iB7eXmgQ3RQ4L8jkVizvuCBW8RMT/banner_engrave%20100.png)](https://hivesigner.com/sign/account-witness-vote?witness=engrave&amp;amp;approve=1)

**Dont forget to follow @engrave account!**

&lt;/center&gt;
&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@engrave&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/hiveprojects-weekly-update-2022-04-30-50237&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-engrave-hiveprojects-weekly-update-2022-04-30-50237').html();
      const outputElem = $('#content-engrave-hiveprojects-weekly-update-2022-04-30-50237');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-engrave-hiveprojects-weekly-update-2022-04-30-50237 {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-engrave-hiveprojects-weekly-update-2022-04-30-50237 code {
    background: white;
  }
  #content-engrave-hiveprojects-weekly-update-2022-04-30-50237 a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-engrave-hiveprojects-weekly-update-2022-04-30-50237 a:hover {
    border-bottom: 0;
  }
  #content-engrave-hiveprojects-weekly-update-2022-04-30-50237 h1 {
    font-size: 2.2em;
  }
  #content-engrave-hiveprojects-weekly-update-2022-04-30-50237 h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-engrave-hiveprojects-weekly-update-2022-04-30-50237 header small {
    color: #999;
    font-size: 50%;
  }
  #content-engrave-hiveprojects-weekly-update-2022-04-30-50237 img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-192847/@engrave/hiveprojects-weekly-update-2022-04-30-50237&quot;&gt;Hive Projects weekly update: 3 projects added, 163 listed in total!&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@engrave&quot;&gt;@engrave&lt;/a&gt;
&lt;/p&gt;</content><author><name>engrave</name></author><category term="news" /><category term="projects" /><summary type="html">![](https://files.peakd.com/file/peakd-hive/engrave/IHxPwBD1-hiveprojects_update.png) # What is [Hive Projects](https://hiveprojects.io)? [Hive Projects](https://hiveprojects.io) is the biggest directory of apps, sites, tools, and scripts created for the Hive ecosystem. This website is an entirely volunteer-driven effort. That includes coding time and hosting costs. If you wish to help or show your gratitude, there are plenty of ways in which you can do that: * upvote this post * reblog or cross-post it into your favorite community * contribute to HiveProjects, by adding a new project - everyone can do it! * let us know about new project, by posting about it in [Hive Projects Community](https://peakd.com/c/hive-192847) or by cross-posting to it or simply paste us a link at [Engrave Discord server](https://discord.gg/8NktdFh) * vote for our @engrave witness * write a comment :) *** Previous post: [Hive Projects weekly update: 4 projects added, 160 listed in total!](/@engrave/hiveprojects-weekly-update-2022-04-19-25719) *** # Newly added projects ## Crossworlds **Team:** ![](https://hiveprojects.io/media/imgs/173/1651158721711.jpg) **Category:** [Games](https://hiveprojects.io/categories/games/) **Description:** *Crossworlds is a blockchain-based MMO game that operates using smart contract interaction between every game asset to provide self-sufficient gameplay mechanics and economics. There are three key directions of roleplay that players can choose from for their own preference or even all of them together at the same time such as mining, crafting, and fighting. Game tokenomics are built in a way to give users freedom: diverging owned game assets control on the market and between players, which allows them to create their own earning strategy inside the game which naturally decentralizes game ownership.* [Crossworlds on HiveProjects.io](https://hiveprojects.io/projects/p/crossworlds/) ## TelePay **Team:** @telepay, @lugodev, @calvarez95dev ![](https://hiveprojects.io/media/imgs/172/1651026406084.png) **Category:** [Wallets](https://hiveprojects.io/categories/wallets/) **Description:** *Payments gateway with Hive and HBD. Create merchants, get your API keys, and process payments easily.* [TelePay on HiveProjects.io](https://hiveprojects.io/projects/p/telepay/) ## HIVE Monitor **Team:** @primersion ![](https://hiveprojects.io/media/imgs/171/1650402566645.png) **Category:** [Witness Tools](https://hiveprojects.io/categories/witness-tools/) **Description:** *A web application for viewing and voting for HIVE and Hive-Engine witnesses. Additionally the page provides an overview of the earnings of witnesses and some useful resources for users that want to start a witness node.* [HIVE Monitor on HiveProjects.io](https://hiveprojects.io/projects/p/hive-monitor/) *** **That would be all for today. Stay tuned for the next update and consider contributing to Hive Projects. It is a community-driven website.** **Click on the image to vote for @engrave witness:** [![banner_engrave 100.png](https://images.hive.blog/DQmUghvic5TCPPvVkB4iB7eXmgQ3RQ4L8jkVizvuCBW8RMT/banner_engrave%20100.png)](https://hivesigner.com/sign/account-witness-vote?witness=engrave&amp;amp;approve=1) **Dont forget to follow @engrave account!** See: Hive Projects weekly update: 3 projects added, 163 listed in total! by @engrave</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hivedocs.info/assets/images/favicon.png" /><media:content medium="image" url="https://hivedocs.info/assets/images/favicon.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">HiveSQL Infrastructure Upgrade Complete</title><link href="https://hivedocs.info/news/sql/database/2022/04/28/hivesql-update-complete-2204.html" rel="alternate" type="text/html" title="HiveSQL Infrastructure Upgrade Complete" /><published>2022-04-28T13:52:30-07:00</published><updated>2022-04-28T13:52:30-07:00</updated><id>https://hivedocs.info/news/sql/database/2022/04/28/hivesql-update-complete-2204</id><content type="html" xml:base="https://hivedocs.info/news/sql/database/2022/04/28/hivesql-update-complete-2204.html">&lt;div id=&quot;content-arcange-hivesql-update-complete-2204&quot;&gt;![](https://i.imgur.com/uvz44bF.png)

Yesterday I put HiveSQL's infrastructure under maintenance for a [major upgrade](/@arcange/hivesql-maintenance-202204).

I would have liked this one to last less long but there were some last-minute unforeseen events. Fortunately, I warned about its unavailability for a whole day, a well-advised caution.

The upgrade is now complete and HiveSQL is back alive and kicking!

### Brand new hardware

HiveSQL runs maintenance on brand new, latest-generation hardware, with better performances. More importantly, it now has double the storage space.

Since the database currently contains information for 6 years of activity, it should be ready for the next 6 years unless the blockchain activity skyrockets, something I can only wish for.

### HiveSQL is back!

HiveSQL is back and all apps and processes relying on it running are fully operational again.

For those who use it, enjoy your favorite tool!

---
&lt;center&gt;

### Check out my apps and services
&lt;a href=&quot;/hive/@hive.engage/stay-connected-with-your-hive-audience-and-catch-attention&quot;&gt;&lt;img src=&quot;https://i.imgur.com/GiNJqlm.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;https://hiveauth.com&quot;&gt;&lt;img src=&quot;https://i.imgur.com/rPlzjwP.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/@hivebuzz&quot;&gt;&lt;img src=&quot;https://i.imgur.com/B4UTun2.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/@hivesql&quot;&gt;&lt;img src=&quot;https://i.imgur.com/EPN8RW6.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/hive/@arcange/introducing-hive-account-recovery&quot;&gt;&lt;img src=&quot;https://i.imgur.com/6TWeW7V.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/@hive.autoclaim&quot;&gt;&lt;img src=&quot;https://i.imgur.com/ih2pEOw.png&quot; /&gt;&lt;/a&gt;
### [Vote for me as a witness ![](https://i.imgur.com/2bi4SnT.png)](https://vote.hive.uno/@arcange)
&lt;/center&gt;&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@arcange&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/hivesql-update-complete-2204&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-arcange-hivesql-update-complete-2204').html();
      const outputElem = $('#content-arcange-hivesql-update-complete-2204');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-arcange-hivesql-update-complete-2204 {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-arcange-hivesql-update-complete-2204 code {
    background: white;
  }
  #content-arcange-hivesql-update-complete-2204 a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-arcange-hivesql-update-complete-2204 a:hover {
    border-bottom: 0;
  }
  #content-arcange-hivesql-update-complete-2204 h1 {
    font-size: 2.2em;
  }
  #content-arcange-hivesql-update-complete-2204 h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-arcange-hivesql-update-complete-2204 header small {
    color: #999;
    font-size: 50%;
  }
  #content-arcange-hivesql-update-complete-2204 img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/hive-139531/@arcange/hivesql-update-complete-2204&quot;&gt;HiveSQL Infrastructure Upgrade Complete&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@arcange&quot;&gt;@arcange&lt;/a&gt;
&lt;/p&gt;</content><author><name>arcange</name></author><category term="news" /><category term="sql" /><category term="database" /><summary type="html">![](https://i.imgur.com/uvz44bF.png) Yesterday I put HiveSQL's infrastructure under maintenance for a [major upgrade](/@arcange/hivesql-maintenance-202204). I would have liked this one to last less long but there were some last-minute unforeseen events. Fortunately, I warned about its unavailability for a whole day, a well-advised caution. The upgrade is now complete and HiveSQL is back alive and kicking! ### Brand new hardware HiveSQL runs maintenance on brand new, latest-generation hardware, with better performances. More importantly, it now has double the storage space. Since the database currently contains information for 6 years of activity, it should be ready for the next 6 years unless the blockchain activity skyrockets, something I can only wish for. ### HiveSQL is back! HiveSQL is back and all apps and processes relying on it running are fully operational again. For those who use it, enjoy your favorite tool! --- ### Check out my apps and services ### [Vote for me as a witness ![](https://i.imgur.com/2bi4SnT.png)](https://vote.hive.uno/@arcange) See: HiveSQL Infrastructure Upgrade Complete by @arcange</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://i.imgur.com/uvz44bF.png" /><media:content medium="image" url="https://i.imgur.com/uvz44bF.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">HiveSQL Infrastructure Upgrade</title><link href="https://hivedocs.info/news/sql/database/2022/04/27/hivesql-maintenance-202204.html" rel="alternate" type="text/html" title="HiveSQL Infrastructure Upgrade" /><published>2022-04-27T01:23:12-07:00</published><updated>2022-04-27T01:23:12-07:00</updated><id>https://hivedocs.info/news/sql/database/2022/04/27/hivesql-maintenance-202204</id><content type="html" xml:base="https://hivedocs.info/news/sql/database/2022/04/27/hivesql-maintenance-202204.html">&lt;div id=&quot;content-arcange-hivesql-maintenance-202204&quot;&gt;![](https://i.imgur.com/ZTURdWq.png)

### TLDR;

HiveSQL will be **under maintenance and unavailable** as of **2022-04-27 22:00 UTC**. The maintenance is expected to last **1 day** (or less).

**A bit of history**

Last month, I had to put HiveSQL in maintenance mode to make a [major change to the database structure](/@arcange/22020302-hivesql-maintenance).

That unexpected maintenance lasted almost 3 days, mostly because I struggled with storage space as HiveSQL database size is growing every day and now exceeds 3.3TB (terabytes)!

To prevent such long unexpected maintenance from happening again, I ordered a new server with better performances and larger disks to **double the actual available storage size**, but given the current supply problems, it took forever to be available. 

The good news is that this new hardware arrived and the upgrade can finally begin.

## HiveSQL in maintenance mode

As of **2022-04-27 22:00 UTC**, HiveSQL will be put under maintenance.

The maintenance is **planned to last one day**. Although I think it should go faster, it's always better to take some safety margin.

To speed up the transition process and minimize downtime, **access to HiveSQL unavailable** for the whole maintenance duration.

I will notify you as soon as this maintenance is over. Any communication and support related to this maintenance will be done in the [HiveSQL Discord Channel](https://discord.gg/kyEFDfT)

---
&lt;center&gt;
### Check out my apps and services
&lt;a href=&quot;/hive/@hive.engage/stay-connected-with-your-hive-audience-and-catch-attention&quot;&gt;&lt;img src=&quot;https://i.imgur.com/GiNJqlm.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;https://hiveauth.com&quot;&gt;&lt;img src=&quot;https://i.imgur.com/rPlzjwP.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/@hivebuzz&quot;&gt;&lt;img src=&quot;https://i.imgur.com/B4UTun2.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/@hivesql&quot;&gt;&lt;img src=&quot;https://i.imgur.com/EPN8RW6.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/hive/@arcange/introducing-hive-account-recovery&quot;&gt;&lt;img src=&quot;https://i.imgur.com/6TWeW7V.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/@hive.autoclaim&quot;&gt;&lt;img src=&quot;https://i.imgur.com/ih2pEOw.png&quot; /&gt;&lt;/a&gt;
### [Vote for me as a witness ![](https://i.imgur.com/2bi4SnT.png)](https://vote.hive.uno/@arcange)
&lt;/center&gt;&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@arcange&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/hivesql-maintenance-202204&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-arcange-hivesql-maintenance-202204').html();
      const outputElem = $('#content-arcange-hivesql-maintenance-202204');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-arcange-hivesql-maintenance-202204 {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-arcange-hivesql-maintenance-202204 code {
    background: white;
  }
  #content-arcange-hivesql-maintenance-202204 a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-arcange-hivesql-maintenance-202204 a:hover {
    border-bottom: 0;
  }
  #content-arcange-hivesql-maintenance-202204 h1 {
    font-size: 2.2em;
  }
  #content-arcange-hivesql-maintenance-202204 h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-arcange-hivesql-maintenance-202204 header small {
    color: #999;
    font-size: 50%;
  }
  #content-arcange-hivesql-maintenance-202204 img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/hive-139531/@arcange/hivesql-maintenance-202204&quot;&gt;HiveSQL Infrastructure Upgrade&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@arcange&quot;&gt;@arcange&lt;/a&gt;
&lt;/p&gt;</content><author><name>arcange</name></author><category term="news" /><category term="sql" /><category term="database" /><summary type="html">![](https://i.imgur.com/ZTURdWq.png) ### TLDR; HiveSQL will be **under maintenance and unavailable** as of **2022-04-27 22:00 UTC**. The maintenance is expected to last **1 day** (or less). **A bit of history** Last month, I had to put HiveSQL in maintenance mode to make a [major change to the database structure](/@arcange/22020302-hivesql-maintenance). That unexpected maintenance lasted almost 3 days, mostly because I struggled with storage space as HiveSQL database size is growing every day and now exceeds 3.3TB (terabytes)! To prevent such long unexpected maintenance from happening again, I ordered a new server with better performances and larger disks to **double the actual available storage size**, but given the current supply problems, it took forever to be available. The good news is that this new hardware arrived and the upgrade can finally begin. ## HiveSQL in maintenance mode As of **2022-04-27 22:00 UTC**, HiveSQL will be put under maintenance. The maintenance is **planned to last one day**. Although I think it should go faster, it's always better to take some safety margin. To speed up the transition process and minimize downtime, **access to HiveSQL unavailable** for the whole maintenance duration. I will notify you as soon as this maintenance is over. Any communication and support related to this maintenance will be done in the [HiveSQL Discord Channel](https://discord.gg/kyEFDfT) --- ### Check out my apps and services ### [Vote for me as a witness ![](https://i.imgur.com/2bi4SnT.png)](https://vote.hive.uno/@arcange) See: HiveSQL Infrastructure Upgrade by @arcange</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://i.imgur.com/ZTURdWq.png" /><media:content medium="image" url="https://i.imgur.com/ZTURdWq.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">6th update of 2022 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2022/04/21/th-update-of-2022-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="6th update of 2022 on BlockTrades work on Hive software" /><published>2022-04-21T15:58:30-07:00</published><updated>2022-04-21T15:58:30-07:00</updated><id>https://hivedocs.info/news/core/development/2022/04/21/th-update-of-2022-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2022/04/21/th-update-of-2022-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-th-update-of-2022-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)

Below are highlights of some of the Hive-related programming issues worked on by the BlockTrades team since my last post.

# Hived (blockchain node software) work

### Optimization of Resource Credit (RC) calculations
We’re currently analyzing how proposed changes to RC calculations will affect different types of Hive users. As a first step to this, we’re adding code to insert RC data generated during the replay of the blockchain into a database to give us a flexible analysis environment.

### Testing and fixing bugs in rc delegation and new wallet_api code
We’ve continued to create new tests to cover bugs discovered (and fixed) in RC delegation and wallet_api code. I believe this work will be completed by tomorrow.

### Testing the new p2p code with testtools-based testnets
Our new tests that simulate network forks enabled us to identify a few more bugs in the updated p2p/locking code (actually a couple of the bugs were in the existing fork database code, but the buggy code was never called during a fork switch previously because the p2p thread was always frozen during a fork switch due to the bad locking in the old code). We fixed the bugs and also made some further improvements to logging to ease diagnosis of any further bugs we might find (and also help in performance optimization). 

Currently there’s no further known bugs associated with p2p/locking changes, but our testers are scheduled to create further test scenarios (e.g. testing transition of nodes to the new hardfork protocol).

### Mirrornet (testnet that mirrors traffic from mainnet) to test p2p code

After a couple more bug fixes, the mirrornet code is “mostly working” now, but there’s still one issue as a node is replayed and first enters live sync. This bug will probably be fixed tomorrow, so I expect we can launch a new public mirrornet early next week. At this point it would be good to get more people to launch nodes on the mirrornet so we can get a nicely distributed network with differently configured nodes. 

We plan to use the new mirrornet to look at potential optimizations of the new hived nodes with better locking when they are under heavy load conditions (and to perform further testing, of course).

### Completed implementation of block finality protocol
The new block finality protocol has been coded and very lightly tested. 

Next we’ll begin creating CI tests to exercise this code under various stressed network conditions (e.g. simulating breaks in the p2p network connections). Despite the lack of a lot of testing yet, based on the cleanness of the design, I don’t expect many bugs in this code.

# Hive Application Framework (HAF)

### Reduced size of hashes stored in HAF databases
One of our new HAF app developers noticed a discrepancy in the way HAF was storing transaction hashes (and block hashes as well), and we ultimately found that the sql_serializer plugin was storing off the hex values as ASCII strings instead of encoded binary (so we were using 40 bytes to store the hash when we could be using only 20 bytes to do it). 

We’ve changed the storage mechanism now to store them as a pure binary hash, which will reduce the size of the transactions table (and some indexes) and also should improve performance somewhat. We’re currently running a benchmark to determine how much smaller the affected tables get after a full replay.

### Fix performance problem when a HAF app “breaks”
We found that if a HAF app stops processing blocks (e.g. there is some bug in the app’s code), this could cause reversible data to begin accumulating in the database instead of being moved to an irreversible state, and this could result in a slowdown of other still-working HAF apps running on that server. This was undesirable, since we want HAF apps to be isolated from each other, by default. Fortunately, we found a quick fix that essentially alleviates this issue and the fix was committed today.

### Filtering of operations to create small HAF servers

I benchmarked HAF database size for both forms of filtering for a HAF servers (1. account-based filtering similar to what is used by the account-history plugin and 2. operations-regex filtering which enables filtering out specific types of operations with particular data) and the news is quite good, although not unexpected. 

For example, using the operations/regex filtering to filter out just splinterlands game operations (but not hive engine ones), the HAF database size on a compressed ZFS drive dropped from 1200GB down to 359GB. This also reduced the time to recreate the database table indexes after the replay on our “typically configured server” from 5.5 hours down to 1.1 hour.

Even more impressively, a HAF database configured just to store account history data for popular Hive exchanges only consumed 5GB on a ZFS compressed drive (as opposed to 1200GB for a full HAF database). And most of that 5GB was consumed by the transactions table, so we may cut that size in half with the new reduced-size hashes mentioned above. 

Note that for many “stand-alone” HAF servers that are dedicated to a particular app or set of apps that mostly rely on custom_json, 5GB size is a realistic “minimum database size” requirement, with any additional data just being whatever is required by the apps themselves (i.e. storage of the app-specific custom_json and state tables for the app).

### Benchmarking of various HAF server configurations

We’ve continued to benchmark HAF running in various hardware and software configurations to figure out optimal configurations for HAF servers in terms of performance and cost effectiveness. 

Based on that benchmarking, we’ve modified the default maintenance_worker_mem setting from the default 64MB to 6GB while we’re re-building the indexes after a HAF replay. We’ll also be checking to see if this change will be very beneficial when performing table vacuums of various types (and therefore whether it should be increased some at other times or even permanently for the server).

Currently we’re analyzing temporarily disabling filesystem sync acknowledgment during initial population of a HAF database. Surprisingly, we haven’t seen a benefit from this yet, so we need to investigate further to figure out why. However, we did see substantial benefits in configuring drives used by the database to “noatime”.

# HAF account history app (aka hafah)
We’re currently enabling automated testing of the version of Hafah that uses PostgREST instead of a python-based web server. Our benchmarking so far continues to show a substantial benefit from using PostgREST, with one of the latest tests showing a 2x improvement in response time (100ms for a 500kb response versus 200ms in the python-based server) for a `get_ops_in_block` call (which was previously the biggest bottleneck during our Hafah benchmark test). I believe this work will be completed by tomorrow.

In related work, we’ve making further changes to our benchmarking script so that this data can be analyzed easily from our automated testing system so that we can easily see improvements or regressions whenever the code is changed.

# Hivemind (social media middleware server used by web sites)
We continued to work on conversion of Hivemind to a HAF-based app. The HAF-based version has been tested using our standard hivemind CI tests and only a small number of tests failed, so I expect we’ll have a usable version available soon that we can begin benchmarking.


# HAF-based block explorer
We’re still in the early stages of this project, but this work is what led to the discovery that we were storing the transaction hashes inefficiently and the issue with retaining reversible data for too long when a HAF app breaks, so it’s already yielding some useful benefits as a means of testing and improving HAF itself.

# Some upcoming tasks
 * Modify the one-step script for installing HAF to optionally download a trusted block_log and block_log.index file (or maybe just allow an option for fast-syncing using a checkpoint to reduce block processing time now that peer syncing process is faster and may actually perform better than downloading a block_log and replaying it). This task is on hold until we have someone free to work on it.
* Collect benchmarks for hafah operating in “irreversible block mode” and compare to a hafah operation in “normal” mode. Task is on hold until we’ve finished optimization of HAfAH (mainly just PostgREST benchmarking now).
* Further testing of hafah on production servers (api.hive.blog).
* Finish conversion of hivemind to a HAF-based app.
* More testing of new P2P code under forking conditions and various live mode scenarios and in a mirrornet testnet using only hived servers with the new P2P code.
* Finished testing of new block finality code.
* Complete work on resource credit rationalization.

# When hardfork 26?

We’re mostly in a “test and benchmarking” mode now, so if all goes well with testing in the next week, we may be able to schedule the next hardfork in late May.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/th-update-of-2022-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-th-update-of-2022-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-th-update-of-2022-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-th-update-of-2022-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-th-update-of-2022-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-th-update-of-2022-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-th-update-of-2022-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-th-update-of-2022-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-th-update-of-2022-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-th-update-of-2022-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-th-update-of-2022-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/th-update-of-2022-on-blocktrades-work-on-hive-software&quot;&gt;6th update of 2022 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below are highlights of some of the Hive-related programming issues worked on by the BlockTrades team since my last post. # Hived (blockchain node software) work ### Optimization of Resource Credit (RC) calculations We’re currently analyzing how proposed changes to RC calculations will affect different types of Hive users. As a first step to this, we’re adding code to insert RC data generated during the replay of the blockchain into a database to give us a flexible analysis environment. ### Testing and fixing bugs in rc delegation and new wallet_api code We’ve continued to create new tests to cover bugs discovered (and fixed) in RC delegation and wallet_api code. I believe this work will be completed by tomorrow. ### Testing the new p2p code with testtools-based testnets Our new tests that simulate network forks enabled us to identify a few more bugs in the updated p2p/locking code (actually a couple of the bugs were in the existing fork database code, but the buggy code was never called during a fork switch previously because the p2p thread was always frozen during a fork switch due to the bad locking in the old code). We fixed the bugs and also made some further improvements to logging to ease diagnosis of any further bugs we might find (and also help in performance optimization). Currently there’s no further known bugs associated with p2p/locking changes, but our testers are scheduled to create further test scenarios (e.g. testing transition of nodes to the new hardfork protocol). ### Mirrornet (testnet that mirrors traffic from mainnet) to test p2p code After a couple more bug fixes, the mirrornet code is “mostly working” now, but there’s still one issue as a node is replayed and first enters live sync. This bug will probably be fixed tomorrow, so I expect we can launch a new public mirrornet early next week. At this point it would be good to get more people to launch nodes on the mirrornet so we can get a nicely distributed network with differently configured nodes. We plan to use the new mirrornet to look at potential optimizations of the new hived nodes with better locking when they are under heavy load conditions (and to perform further testing, of course). ### Completed implementation of block finality protocol The new block finality protocol has been coded and very lightly tested. Next we’ll begin creating CI tests to exercise this code under various stressed network conditions (e.g. simulating breaks in the p2p network connections). Despite the lack of a lot of testing yet, based on the cleanness of the design, I don’t expect many bugs in this code. # Hive Application Framework (HAF) ### Reduced size of hashes stored in HAF databases One of our new HAF app developers noticed a discrepancy in the way HAF was storing transaction hashes (and block hashes as well), and we ultimately found that the sql_serializer plugin was storing off the hex values as ASCII strings instead of encoded binary (so we were using 40 bytes to store the hash when we could be using only 20 bytes to do it). We’ve changed the storage mechanism now to store them as a pure binary hash, which will reduce the size of the transactions table (and some indexes) and also should improve performance somewhat. We’re currently running a benchmark to determine how much smaller the affected tables get after a full replay. ### Fix performance problem when a HAF app “breaks” We found that if a HAF app stops processing blocks (e.g. there is some bug in the app’s code), this could cause reversible data to begin accumulating in the database instead of being moved to an irreversible state, and this could result in a slowdown of other still-working HAF apps running on that server. This was undesirable, since we want HAF apps to be isolated from each other, by default. Fortunately, we found a quick fix that essentially alleviates this issue and the fix was committed today. ### Filtering of operations to create small HAF servers I benchmarked HAF database size for both forms of filtering for a HAF servers (1. account-based filtering similar to what is used by the account-history plugin and 2. operations-regex filtering which enables filtering out specific types of operations with particular data) and the news is quite good, although not unexpected. For example, using the operations/regex filtering to filter out just splinterlands game operations (but not hive engine ones), the HAF database size on a compressed ZFS drive dropped from 1200GB down to 359GB. This also reduced the time to recreate the database table indexes after the replay on our “typically configured server” from 5.5 hours down to 1.1 hour. Even more impressively, a HAF database configured just to store account history data for popular Hive exchanges only consumed 5GB on a ZFS compressed drive (as opposed to 1200GB for a full HAF database). And most of that 5GB was consumed by the transactions table, so we may cut that size in half with the new reduced-size hashes mentioned above. Note that for many “stand-alone” HAF servers that are dedicated to a particular app or set of apps that mostly rely on custom_json, 5GB size is a realistic “minimum database size” requirement, with any additional data just being whatever is required by the apps themselves (i.e. storage of the app-specific custom_json and state tables for the app). ### Benchmarking of various HAF server configurations We’ve continued to benchmark HAF running in various hardware and software configurations to figure out optimal configurations for HAF servers in terms of performance and cost effectiveness. Based on that benchmarking, we’ve modified the default maintenance_worker_mem setting from the default 64MB to 6GB while we’re re-building the indexes after a HAF replay. We’ll also be checking to see if this change will be very beneficial when performing table vacuums of various types (and therefore whether it should be increased some at other times or even permanently for the server). Currently we’re analyzing temporarily disabling filesystem sync acknowledgment during initial population of a HAF database. Surprisingly, we haven’t seen a benefit from this yet, so we need to investigate further to figure out why. However, we did see substantial benefits in configuring drives used by the database to “noatime”. # HAF account history app (aka hafah) We’re currently enabling automated testing of the version of Hafah that uses PostgREST instead of a python-based web server. Our benchmarking so far continues to show a substantial benefit from using PostgREST, with one of the latest tests showing a 2x improvement in response time (100ms for a 500kb response versus 200ms in the python-based server) for a `get_ops_in_block` call (which was previously the biggest bottleneck during our Hafah benchmark test). I believe this work will be completed by tomorrow. In related work, we’ve making further changes to our benchmarking script so that this data can be analyzed easily from our automated testing system so that we can easily see improvements or regressions whenever the code is changed. # Hivemind (social media middleware server used by web sites) We continued to work on conversion of Hivemind to a HAF-based app. The HAF-based version has been tested using our standard hivemind CI tests and only a small number of tests failed, so I expect we’ll have a usable version available soon that we can begin benchmarking. # HAF-based block explorer We’re still in the early stages of this project, but this work is what led to the discovery that we were storing the transaction hashes inefficiently and the issue with retaining reversible data for too long when a HAF app breaks, so it’s already yielding some useful benefits as a means of testing and improving HAF itself. # Some upcoming tasks * Modify the one-step script for installing HAF to optionally download a trusted block_log and block_log.index file (or maybe just allow an option for fast-syncing using a checkpoint to reduce block processing time now that peer syncing process is faster and may actually perform better than downloading a block_log and replaying it). This task is on hold until we have someone free to work on it. * Collect benchmarks for hafah operating in “irreversible block mode” and compare to a hafah operation in “normal” mode. Task is on hold until we’ve finished optimization of HAfAH (mainly just PostgREST benchmarking now). * Further testing of hafah on production servers (api.hive.blog). * Finish conversion of hivemind to a HAF-based app. * More testing of new P2P code under forking conditions and various live mode scenarios and in a mirrornet testnet using only hived servers with the new P2P code. * Finished testing of new block finality code. * Complete work on resource credit rationalization. # When hardfork 26? We’re mostly in a “test and benchmarking” mode now, so if all goes well with testing in the next week, we may be able to schedule the next hardfork in late May. See: 6th update of 2022 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Core dev meeting #36</title><link href="https://hivedocs.info/news/core/development/2022/04/14/core-dev-meeting-36.html" rel="alternate" type="text/html" title="Core dev meeting #36" /><published>2022-04-14T11:49:54-07:00</published><updated>2022-04-14T11:49:54-07:00</updated><id>https://hivedocs.info/news/core/development/2022/04/14/core-dev-meeting-36</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2022/04/14/core-dev-meeting-36.html">&lt;div id=&quot;content-howo-core-dev-meeting-36&quot;&gt;https://www.youtube.com/watch?v=uoskscKWT-A

A special edition where I use my camera ! 

# Dev sync

It's best to listen to this one.

## progressive lockup period for HBD (with variable rates, say 10% = 3 days, 20% = 30 days etc) ?

It's an interesting concept, which could allow us to offer better stabilities to people who stake, where you lock for x amount of time and you're guaranteed to have the same rate for the entire duration. The higher the lockup the better. @gtg pointed out that it's best to keep it as simple as possible, which I agree, should we create this, it would be either 3 days or something else (like a year) lockup.

### We could try to make this generalized to apply the same to have variable HP lockups

After some discussions, we agreed it was best to not mess with HP for now.

## Rosetta api (is anyone working on it or is it up for grabs ?)

I wanted to know if anyone was working on it right now or if it was still available, so no one is currently on it, I don't intend to build it yet, I believe communities improvements is more important for now. I'll work on it later on. 

## hivemind flow.txt inaccurate (see this: [comment](https://gitlab.syncad.com/hive/hivemind/-/merge_requests/539#note_80463) because it's made by hand, should we remove it altogether or expand the parser POC that I made ?

It's a bunch of technical discussions, I will work on some tooling to automate some of the documentation generation.

## There was some discussions about renaming HBD to something more explicit like USDH etc 

We could do it but it's a major undertaking for not a lot of benefits, we believe it's best to not do it unless there is a really really strong push for it.

And this is it for this meeting, have a good one.

@howo

![](https://i.imgur.com/oPJ63jA.png)
&lt;center&gt;&lt;sup&gt;You can vote for our witness directly using Hivesigner &lt;a href=&quot;https://hivesigner.com/sign/account-witness-vote?witness=steempress&amp;amp;approve=1&quot;&gt;here&lt;/a&gt;.&lt;/sup&gt;&lt;/center&gt;&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@howo&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/core-dev-meeting-36&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-howo-core-dev-meeting-36').html();
      const outputElem = $('#content-howo-core-dev-meeting-36');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-howo-core-dev-meeting-36 {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-howo-core-dev-meeting-36 code {
    background: white;
  }
  #content-howo-core-dev-meeting-36 a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-howo-core-dev-meeting-36 a:hover {
    border-bottom: 0;
  }
  #content-howo-core-dev-meeting-36 h1 {
    font-size: 2.2em;
  }
  #content-howo-core-dev-meeting-36 h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-howo-core-dev-meeting-36 header small {
    color: #999;
    font-size: 50%;
  }
  #content-howo-core-dev-meeting-36 img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/hive/@howo/core-dev-meeting-36&quot;&gt;Core dev meeting #36&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@howo&quot;&gt;@howo&lt;/a&gt;
&lt;/p&gt;</content><author><name>howo</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">https://www.youtube.com/watch?v=uoskscKWT-A A special edition where I use my camera ! # Dev sync It's best to listen to this one. ## progressive lockup period for HBD (with variable rates, say 10% = 3 days, 20% = 30 days etc) ? It's an interesting concept, which could allow us to offer better stabilities to people who stake, where you lock for x amount of time and you're guaranteed to have the same rate for the entire duration. The higher the lockup the better. @gtg pointed out that it's best to keep it as simple as possible, which I agree, should we create this, it would be either 3 days or something else (like a year) lockup. ### We could try to make this generalized to apply the same to have variable HP lockups After some discussions, we agreed it was best to not mess with HP for now. ## Rosetta api (is anyone working on it or is it up for grabs ?) I wanted to know if anyone was working on it right now or if it was still available, so no one is currently on it, I don't intend to build it yet, I believe communities improvements is more important for now. I'll work on it later on. ## hivemind flow.txt inaccurate (see this: [comment](https://gitlab.syncad.com/hive/hivemind/-/merge_requests/539#note_80463) because it's made by hand, should we remove it altogether or expand the parser POC that I made ? It's a bunch of technical discussions, I will work on some tooling to automate some of the documentation generation. ## There was some discussions about renaming HBD to something more explicit like USDH etc We could do it but it's a major undertaking for not a lot of benefits, we believe it's best to not do it unless there is a really really strong push for it. And this is it for this meeting, have a good one. @howo ![](https://i.imgur.com/oPJ63jA.png) You can vote for our witness directly using Hivesigner here. See: Core dev meeting #36 by @howo</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://i.imgur.com/oPJ63jA.png" /><media:content medium="image" url="https://i.imgur.com/oPJ63jA.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">5th update of 2022 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2022/04/07/5th-update-of-2022-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="5th update of 2022 on BlockTrades work on Hive software" /><published>2022-04-07T16:33:27-07:00</published><updated>2022-04-07T16:33:27-07:00</updated><id>https://hivedocs.info/news/core/development/2022/04/07/5th-update-of-2022-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2022/04/07/5th-update-of-2022-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-5th-update-of-2022-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)

Below are highlights of some of the Hive-related programming issues worked on by the BlockTrades team since my last post.

# Hived (blockchain node software) work

### Optimization of Resource Credit (RC) calculations
Changes were made and tested to add extra cost of custom operations that are used for RC delegations (while these aren’t strictly consensus, they do impose more costs on a hived node than other custom operations).

### Testing and fixing bugs in new wallet_api code
Found and fixed some more bugs in wallet_api code. I think this task will be closed out soon.

### Testing the new p2p code with testtools-based testnets
We fixed a minor race condition in the new sync code that got exposed during testing: https://gitlab.syncad.com/hive/hive/-/commit/c62803aa6a1627771ee0f05f95710154263a843c

Testing of the new p2p code also exposed a latent bug in the fork_database code: the fork database’s copy of the head block didn’t get updated properly when a fork switch occurred.  This could cause problems for the new p2p code during syncing, because it now uses this copy of the head block instead of the one in chainbase to reduce mutex contention on chainbase. Now the head block is properly updated during fork switches: https://gitlab.syncad.com/hive/hive/-/commit/5c0b7fc0e290859d6d1809234a2c87cedecc760c

During this testing, we also found a longstanding problem with the way the mutex locks that protect access to critical resources were being handled: many of the read locks include a timeout parameter (i.e. if a mutex read lock wasn’t obtained within 1s, the lock failed and the code would have to deal with this failure) and it turns out that these locks performed with a timeout can actually fail for no reason (even before the timeout time expires) and required that the code using the lock explicitly check the results of the lock attempt (checks that weren’t performed). This didn’t happen very often (maybe once in a million times), but over long periods of time this has no doubt resulted in occasional unexpected failures inside the code.

To fix this problem, by default, read locks are now “untimed” (they will block until they get the lock) and only the API server uses locks with timeouts (these calls are allowed to fail and using timeouts prevents API calls from taking up too much of the chainbase’s access time and potentially starving the critical execution of the write_queue that is writing blockchain data to chainbase). We also replaced the use of boost::interprocess locks with standard locks, as the locks are only used within a single hived process and there didn't seem to be a need for these presumably more expensive locks. This work was merged in here: https://gitlab.syncad.com/hive/hive/-/merge_requests/401


### Mirrornet (testnet that mirrors traffic from mainnet) to test p2p code

While trying to setup the mirrornet to test the new p2p code, we found some further problems in the mirrornet code, and these are currently being fixed. 

Once they are fixed, we’ll resume attempts to test the p2p code under heavy loading conditions, but in the meantime we decided to rely on the “tried-and-true” method of throwing the new code into our production environment (watching it carefully, of course) to test the above mentioned locking changes that were made.

We also exercised the new locking code using the new API benchmark/stress tests that we used to test account history nodes while the node was providing sync data to another node. Neither test exposed any bugs or performance regressions.

### Completed initial design of block finality protocol
We completed our design for the new code to improve block finality time and we’ve begun implementation of the new code (sometimes with distractions to work on other tasks, unfortunately). I’ll write some more on this topic in a separate post once we’ve proved out the design more.

# Hive Application Framework (HAF)

### Filtering of operations using regexes to allow for “small” HAF servers

I believe the code for using regexes to filter operations is complete or nearly so, and tests are now being developed for it, but I forgot to get an update today on the status of this work, so I’ll update on this point tomorrow.

### Benchmarking of alternative file systems for HAF-based
We’ve continued to benchmark HAF running in various hardware and software configurations to figure out optimal configurations for HAF servers in terms of performance and cost effectiveness. Among other things, we discovered that the location of PostgreSQL write-ahead logs (by default written to /var/lib/postgresql) can have a significant impact on the time it takes to reindex a HAF database (note that this is separately specified from the location of the HAF database itself).

# HAF account history app (aka hafah)
We implemented and tested the changes I mentioned last week to create a new index in the HAF database to speedup get_account_history calls (and probably other similar future calls as well that may be needed by other HAF apps). 

We’re now looking to see if we can speed up the performance of the next biggest bottleneck API call (get_ops_in_block), but performance of this call is already acceptable if we can’t further speed it up.

In order to speed up our optimization work, we also made a higher-level script to eliminate some of the manual steps that were previously required to perform a benchmark of HAfAH and get useful analytical data from the benchmark (this will probably be committed tomorrow). This script may serve as a useful starting point for other HAF apps looking to benchmark the performance of the app's API calls.


# Hivemind (social media middleware server used by web sites)
We continued to work on conversion of Hivemind to a HAF-based app with a slight detour: currently the code is being reviewed for possible improvements in coding style to meet best practices for Python.

We also updated the ujson package used by Hivemind because of security concerns about an older version of the package. 

And finally we merged in an old optimization we made to processing of custom_json operations.

# HAF-based block explorer
We're in the early stages of developing a HAF-based block explorer (open-source, of course). Two of our newer developers are getting introduced to the associated concepts and also reviewing HAF documentation as part of this work.

# Condenser (source code hive.blog and several other Hive-based web sites)
We’ve also been reviewing and merging in updates from @quochuy for condenser and we have a few more to merge in during the coming days (some fixes for Hive Authenticate and improvements to deter phishing attempts).

# What’s next?
 * Modify the one-step script for installing HAF to optionally download a trusted block_log and block_log.index file (or maybe just allow an option for fast-syncing using a checkpoint to reduce block processing time now that peer syncing process is faster and may actually perform better than downloading a block_log and replaying it). This task is on hold until we have someone free to work on it.
* Test filtering of operations by sql_serializer using regexs and account name to allow for smaller HAF server databases.
* Collect benchmarks for hafah operating in “irreversible block mode” and compare to a hafah operation in “normal” mode. Task is on hold until we’ve finished basic optimizations of HAfAH API.
* Further testing of hafah on production servers (api.hive.blog).
* Finish conversion of hivemind to a HAF-based app.
* More testing of new P2P code under forking conditions and various live mode scenarios and in a mirrornet testnet using only hived servers with the new P2P code.
* Complete work on improving block finality time.
* Complete work on resource credit rationalization.
* Continue benchmarking of HAF and Hafah on ZFS and EXT4 file systems with various hardware and software configurations.

I’m pushing the current expected date for the next hardfork to May, given the large number of testing and performance benchmarking tasks still facing us and a couple of key functional tasks still to be completed (RC rationalization and block finality improvement tasks).&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/5th-update-of-2022-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-5th-update-of-2022-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-5th-update-of-2022-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-5th-update-of-2022-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-5th-update-of-2022-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-5th-update-of-2022-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-5th-update-of-2022-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-5th-update-of-2022-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-5th-update-of-2022-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-5th-update-of-2022-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-5th-update-of-2022-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive/@blocktrades/5th-update-of-2022-on-blocktrades-work-on-hive-software&quot;&gt;5th update of 2022 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below are highlights of some of the Hive-related programming issues worked on by the BlockTrades team since my last post. # Hived (blockchain node software) work ### Optimization of Resource Credit (RC) calculations Changes were made and tested to add extra cost of custom operations that are used for RC delegations (while these aren’t strictly consensus, they do impose more costs on a hived node than other custom operations). ### Testing and fixing bugs in new wallet_api code Found and fixed some more bugs in wallet_api code. I think this task will be closed out soon. ### Testing the new p2p code with testtools-based testnets We fixed a minor race condition in the new sync code that got exposed during testing: https://gitlab.syncad.com/hive/hive/-/commit/c62803aa6a1627771ee0f05f95710154263a843c Testing of the new p2p code also exposed a latent bug in the fork_database code: the fork database’s copy of the head block didn’t get updated properly when a fork switch occurred. This could cause problems for the new p2p code during syncing, because it now uses this copy of the head block instead of the one in chainbase to reduce mutex contention on chainbase. Now the head block is properly updated during fork switches: https://gitlab.syncad.com/hive/hive/-/commit/5c0b7fc0e290859d6d1809234a2c87cedecc760c During this testing, we also found a longstanding problem with the way the mutex locks that protect access to critical resources were being handled: many of the read locks include a timeout parameter (i.e. if a mutex read lock wasn’t obtained within 1s, the lock failed and the code would have to deal with this failure) and it turns out that these locks performed with a timeout can actually fail for no reason (even before the timeout time expires) and required that the code using the lock explicitly check the results of the lock attempt (checks that weren’t performed). This didn’t happen very often (maybe once in a million times), but over long periods of time this has no doubt resulted in occasional unexpected failures inside the code. To fix this problem, by default, read locks are now “untimed” (they will block until they get the lock) and only the API server uses locks with timeouts (these calls are allowed to fail and using timeouts prevents API calls from taking up too much of the chainbase’s access time and potentially starving the critical execution of the write_queue that is writing blockchain data to chainbase). We also replaced the use of boost::interprocess locks with standard locks, as the locks are only used within a single hived process and there didn't seem to be a need for these presumably more expensive locks. This work was merged in here: https://gitlab.syncad.com/hive/hive/-/merge_requests/401 ### Mirrornet (testnet that mirrors traffic from mainnet) to test p2p code While trying to setup the mirrornet to test the new p2p code, we found some further problems in the mirrornet code, and these are currently being fixed. Once they are fixed, we’ll resume attempts to test the p2p code under heavy loading conditions, but in the meantime we decided to rely on the “tried-and-true” method of throwing the new code into our production environment (watching it carefully, of course) to test the above mentioned locking changes that were made. We also exercised the new locking code using the new API benchmark/stress tests that we used to test account history nodes while the node was providing sync data to another node. Neither test exposed any bugs or performance regressions. ### Completed initial design of block finality protocol We completed our design for the new code to improve block finality time and we’ve begun implementation of the new code (sometimes with distractions to work on other tasks, unfortunately). I’ll write some more on this topic in a separate post once we’ve proved out the design more. # Hive Application Framework (HAF) ### Filtering of operations using regexes to allow for “small” HAF servers I believe the code for using regexes to filter operations is complete or nearly so, and tests are now being developed for it, but I forgot to get an update today on the status of this work, so I’ll update on this point tomorrow. ### Benchmarking of alternative file systems for HAF-based We’ve continued to benchmark HAF running in various hardware and software configurations to figure out optimal configurations for HAF servers in terms of performance and cost effectiveness. Among other things, we discovered that the location of PostgreSQL write-ahead logs (by default written to /var/lib/postgresql) can have a significant impact on the time it takes to reindex a HAF database (note that this is separately specified from the location of the HAF database itself). # HAF account history app (aka hafah) We implemented and tested the changes I mentioned last week to create a new index in the HAF database to speedup get_account_history calls (and probably other similar future calls as well that may be needed by other HAF apps). We’re now looking to see if we can speed up the performance of the next biggest bottleneck API call (get_ops_in_block), but performance of this call is already acceptable if we can’t further speed it up. In order to speed up our optimization work, we also made a higher-level script to eliminate some of the manual steps that were previously required to perform a benchmark of HAfAH and get useful analytical data from the benchmark (this will probably be committed tomorrow). This script may serve as a useful starting point for other HAF apps looking to benchmark the performance of the app's API calls. # Hivemind (social media middleware server used by web sites) We continued to work on conversion of Hivemind to a HAF-based app with a slight detour: currently the code is being reviewed for possible improvements in coding style to meet best practices for Python. We also updated the ujson package used by Hivemind because of security concerns about an older version of the package. And finally we merged in an old optimization we made to processing of custom_json operations. # HAF-based block explorer We're in the early stages of developing a HAF-based block explorer (open-source, of course). Two of our newer developers are getting introduced to the associated concepts and also reviewing HAF documentation as part of this work. # Condenser (source code hive.blog and several other Hive-based web sites) We’ve also been reviewing and merging in updates from @quochuy for condenser and we have a few more to merge in during the coming days (some fixes for Hive Authenticate and improvements to deter phishing attempts). # What’s next? * Modify the one-step script for installing HAF to optionally download a trusted block_log and block_log.index file (or maybe just allow an option for fast-syncing using a checkpoint to reduce block processing time now that peer syncing process is faster and may actually perform better than downloading a block_log and replaying it). This task is on hold until we have someone free to work on it. * Test filtering of operations by sql_serializer using regexs and account name to allow for smaller HAF server databases. * Collect benchmarks for hafah operating in “irreversible block mode” and compare to a hafah operation in “normal” mode. Task is on hold until we’ve finished basic optimizations of HAfAH API. * Further testing of hafah on production servers (api.hive.blog). * Finish conversion of hivemind to a HAF-based app. * More testing of new P2P code under forking conditions and various live mode scenarios and in a mirrornet testnet using only hived servers with the new P2P code. * Complete work on improving block finality time. * Complete work on resource credit rationalization. * Continue benchmarking of HAF and Hafah on ZFS and EXT4 file systems with various hardware and software configurations. I’m pushing the current expected date for the next hardfork to May, given the large number of testing and performance benchmarking tasks still facing us and a couple of key functional tasks still to be completed (RC rationalization and block finality improvement tasks). See: 5th update of 2022 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">4th update of 2022 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2022/03/25/4th-update-of-2022-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="4th update of 2022 on BlockTrades work on Hive software" /><published>2022-03-25T16:43:09-07:00</published><updated>2022-03-25T16:43:09-07:00</updated><id>https://hivedocs.info/news/core/development/2022/03/25/4th-update-of-2022-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2022/03/25/4th-update-of-2022-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-4th-update-of-2022-on-blocktrades-work-on-hive-software&quot;&gt;Below are highlights of some of the Hive-related programming issues worked on by the BlockTrades team since my last post.

# Hived (blockchain node software) work

### Optimization of Resource Credit (RC) calculations
During our work on rationalizing the resource credit costs of blockchain operations, we discovered that the same resource credit calculations were being performed several times for a block, so the code has been optimized to avoid the unnecessary work.

### Testing and fixing bugs in new wallet_api code
We continued testing and fixing bugs that the tests uncovered in the new wallet_api code that I discussed last week. I think we’re nearly done with this task.

### Enhancing our testing system
As we get closer to a potential hardfork date, the capabilities of our testing system becomes more important, especially since we’ve made changes to some very low-level code in the upcoming release. So we’re expanding the capabilities of our “testtools” testing system to model complex systems that include not only varying numbers of witness nodes, but also multiple API nodes as well.

### Testing the new p2p code with testtools-based testnets
Using testtools, we’ve been creating new testnet-based tests to look for bugs in the updated peer-to-peer (p2p) network code that I discussed last week. 

The first bug we found turned out not to be a bug in the p2p code at all: the new p2p code was so much faster than the old code that it exposed a problem in the test itself. The test assumed that the node would take a certain amount of time before it received a block, but with the faster p2p network, the node got the block much earlier, breaking the test’s expectation and causing it to report a failure. So in this case, we ended up fixing the test itself.

### Testing the new p2p code using a mirrornet
In addition to using testtools, we’ve setup a mirrornet to test with. A mirrornet is a testnet setup with testtools that has a blockchain history that “mirrors” the blocks on the mainnet, and it also translates transactions it observes in real-time from the mainnet to the mirrornet, so that we can more accurately model real-world loading conditions in our testing. 

The mirrornet is particularly important when we want to do performance testing, as it allows us to do testing that we could otherwise not do easily, even on the mainnet (because on the mainnet, we can’t examine scenarios where all the nodes are running the new code, it is a “mixed-node” environment consistently of old nodes and new nodes and we can’t control which of these nodes are the block producers).

We found one bug already using the mirrornet (it was a latent bug in the fork database that got exposed by the p2p changes) and we’re planning to create further tests to look for more bugs.

### Planning to do flood testing next

We’re also planning to extend the mirrornet’s capability to allow for “flood testing” where we broadcast a large number of transactions to the API nodes in a short period of time. This will allow us to stress-test the network and look for more areas where we may be able to further speed up node performance.

### Speeding up build-and-test time (CI)
We’ve updated our build-and-test system (CI) for hive and HAF to use the [ninja build tool](https://ninja-build.org/) instead of “make” to speed up our build times. Among other things, ninja is better at determining real dependencies in the build process and therefore allows for more parallel execution of subtasks in the build process.

By switching to ninja and making a few other configuration changes to our CI runner systems, we were able to drop average build time ranges for the hive repo from a previous range of 40-60mins down to an average of 25mins. 

Similar changes to the HAF CI flow led to even better improvements in the overall build-and-test time: previously it took 60-90mins, now it is down to a range of 30-40mins to complete.

It is worth noting that devs can also use ninja for their local builds instead of make (this isn’t just for speeding up CI times), and we highly recommend it as a faster alternative.
 
# Hive Application Framework (HAF)

### Filtering of operations using regexes to allow for “small” HAF servers

We completed filtering of hive operations based on account names, and we’re now working on filtering of operations based on regexs in the sql_serializer plugin. This latter type of filtering should allow standalone HAF apps operate with very small HAF databases. I estimate that just filtering out splinterlands and hive-engine related custom_json operations should drop storage requirements for an app by 90%. I expect this work will be completed in the next week.

### Benchmarking of alternative file systems for HAF-based
Although filtering of operations should enable stand-alone HAF apps to operate with very small databases, public API nodes using HAF will still want to store all the blockchain data in order to support account history API calls (via hafah) and also to enable them to run any open-source HAF app.

Unfortunately, on an ext4 file system, a HAF database with the entire blockchain consumes around 2.7TB. This isn’t horrible, but it’s not small either, so we’ve started investigating using ZFS and compressed ZFS filesystems as an alternative.

We’re still deep into performing these benchmarks because they take a long time to run and consume a lot of hardware resources, but preliminary results are very encouraging. Using a 2TB fast nvme drive configured with a ZFS file system and lz4 compression, a 62M block HAF database was filled in only 12 hours and fit in 1.2TB. 

This is a particularly appealing hardware setup, because it would give full API nodes a relatively cheap option for their storage needs (and would likely fit on the existing servers used by most public API nodes).

There’s also a nice compression of the blockchain (block_log file) on these drives as well: compressed size of 361GB vs uncompressed size of 549GB.

We still need to do more performance testing on the serving side of HAF in this configuration  (i.e. API performance) and test more hardware and software configurations to determine the optimal setup, and we’ll be continuing that work in the coming week.


# HAF account history app (aka hafah)
We’re periodically testing hafah on our production system, then making improvements whenever this exposes a performance problem not discovered by automated testing. 

### Improving API performance
One performance problem that came up recently was an API call where hafah was taking 7 minutes to respond.  We found a relatively simple initial solution that speeded it up to an acceptable level, but still worse than a standard account history node. Nobody likes to lose to existing software on a performance metric however, so we looked further and found a slightly more code-intensive solution that dropped the time down to 0.3s (which we’re pretty happy about).

This solution will unfortunately require a little more data to be written to the HAF database by the sql_serializer plugin, so it won’t be fully completed until sometime next week.

### Dockerized HAF servers for app testing

We’re continuing to work on creating dockerized HAF servers and modifying the continuous integration process (i.e. automated testing) for all HAF-based apps to re-use existing dockerized HAF servers when possible. This work is primarily being done in the Hafah repo. 

We made some good progress on this task, but then got distracted by the need to optimize the performance of the slow queries mentioned above. The work will resume on this task shortly.

# Hivemind (social media middleware server used by web sites)
We continued to work on conversion of Hivemind to a HAF-based app. We now have a version of hivemind that no longer needs to make any API calls to a hived node (i.e. now all indexing needs are filled from the data inside the HAF database).

# What’s next?
 * Modify the one-step script for installing HAF to optionally download a trusted block_log and block_log.index file (or maybe just allow an option for fast-syncing using a checkpoint to reduce block processing time now that peer syncing process is faster and may actually perform better than downloading a block_log and replaying it). This task is on hold until we have someone free to work on it.
* Continue work on filtering of operations by sql_serializer using regexs to allow for smaller HAF server databases.
* Collect benchmarks for hafah operating in “irreversible block mode” and compare to a hafah operation in “normal” mode.
* Further testing of hafah on production servers (api.hive.blog).
* Finish conversion of hivemind to a HAF-based app.
* More testing of new P2P code under forking conditions and various live mode scenarios and in a mirrornet testnet using only hived servers with the new P2P code.
* Experiment with methods of improving block finality time.
* Complete work on resource credit rationalization.
* Continue benchmarking of HAF and Hafah on ZFS and EXT4 file systems with various hardware and software configurations.

Current expected date for the next hardfork is still the end of April assuming no serious problems are uncovered during testing over the next week or two. But that date is rapidly approaching, so it is going to take a fairly herculean effort if we are to complete the key tasks in this timeframe.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/4th-update-of-2022-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-4th-update-of-2022-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-4th-update-of-2022-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-4th-update-of-2022-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-4th-update-of-2022-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-4th-update-of-2022-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-4th-update-of-2022-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-4th-update-of-2022-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-4th-update-of-2022-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-4th-update-of-2022-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-4th-update-of-2022-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/4th-update-of-2022-on-blocktrades-work-on-hive-software&quot;&gt;4th update of 2022 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">Below are highlights of some of the Hive-related programming issues worked on by the BlockTrades team since my last post. # Hived (blockchain node software) work ### Optimization of Resource Credit (RC) calculations During our work on rationalizing the resource credit costs of blockchain operations, we discovered that the same resource credit calculations were being performed several times for a block, so the code has been optimized to avoid the unnecessary work. ### Testing and fixing bugs in new wallet_api code We continued testing and fixing bugs that the tests uncovered in the new wallet_api code that I discussed last week. I think we’re nearly done with this task. ### Enhancing our testing system As we get closer to a potential hardfork date, the capabilities of our testing system becomes more important, especially since we’ve made changes to some very low-level code in the upcoming release. So we’re expanding the capabilities of our “testtools” testing system to model complex systems that include not only varying numbers of witness nodes, but also multiple API nodes as well. ### Testing the new p2p code with testtools-based testnets Using testtools, we’ve been creating new testnet-based tests to look for bugs in the updated peer-to-peer (p2p) network code that I discussed last week. The first bug we found turned out not to be a bug in the p2p code at all: the new p2p code was so much faster than the old code that it exposed a problem in the test itself. The test assumed that the node would take a certain amount of time before it received a block, but with the faster p2p network, the node got the block much earlier, breaking the test’s expectation and causing it to report a failure. So in this case, we ended up fixing the test itself. ### Testing the new p2p code using a mirrornet In addition to using testtools, we’ve setup a mirrornet to test with. A mirrornet is a testnet setup with testtools that has a blockchain history that “mirrors” the blocks on the mainnet, and it also translates transactions it observes in real-time from the mainnet to the mirrornet, so that we can more accurately model real-world loading conditions in our testing. The mirrornet is particularly important when we want to do performance testing, as it allows us to do testing that we could otherwise not do easily, even on the mainnet (because on the mainnet, we can’t examine scenarios where all the nodes are running the new code, it is a “mixed-node” environment consistently of old nodes and new nodes and we can’t control which of these nodes are the block producers). We found one bug already using the mirrornet (it was a latent bug in the fork database that got exposed by the p2p changes) and we’re planning to create further tests to look for more bugs. ### Planning to do flood testing next We’re also planning to extend the mirrornet’s capability to allow for “flood testing” where we broadcast a large number of transactions to the API nodes in a short period of time. This will allow us to stress-test the network and look for more areas where we may be able to further speed up node performance. ### Speeding up build-and-test time (CI) We’ve updated our build-and-test system (CI) for hive and HAF to use the [ninja build tool](https://ninja-build.org/) instead of “make” to speed up our build times. Among other things, ninja is better at determining real dependencies in the build process and therefore allows for more parallel execution of subtasks in the build process. By switching to ninja and making a few other configuration changes to our CI runner systems, we were able to drop average build time ranges for the hive repo from a previous range of 40-60mins down to an average of 25mins. Similar changes to the HAF CI flow led to even better improvements in the overall build-and-test time: previously it took 60-90mins, now it is down to a range of 30-40mins to complete. It is worth noting that devs can also use ninja for their local builds instead of make (this isn’t just for speeding up CI times), and we highly recommend it as a faster alternative. # Hive Application Framework (HAF) ### Filtering of operations using regexes to allow for “small” HAF servers We completed filtering of hive operations based on account names, and we’re now working on filtering of operations based on regexs in the sql_serializer plugin. This latter type of filtering should allow standalone HAF apps operate with very small HAF databases. I estimate that just filtering out splinterlands and hive-engine related custom_json operations should drop storage requirements for an app by 90%. I expect this work will be completed in the next week. ### Benchmarking of alternative file systems for HAF-based Although filtering of operations should enable stand-alone HAF apps to operate with very small databases, public API nodes using HAF will still want to store all the blockchain data in order to support account history API calls (via hafah) and also to enable them to run any open-source HAF app. Unfortunately, on an ext4 file system, a HAF database with the entire blockchain consumes around 2.7TB. This isn’t horrible, but it’s not small either, so we’ve started investigating using ZFS and compressed ZFS filesystems as an alternative. We’re still deep into performing these benchmarks because they take a long time to run and consume a lot of hardware resources, but preliminary results are very encouraging. Using a 2TB fast nvme drive configured with a ZFS file system and lz4 compression, a 62M block HAF database was filled in only 12 hours and fit in 1.2TB. This is a particularly appealing hardware setup, because it would give full API nodes a relatively cheap option for their storage needs (and would likely fit on the existing servers used by most public API nodes). There’s also a nice compression of the blockchain (block_log file) on these drives as well: compressed size of 361GB vs uncompressed size of 549GB. We still need to do more performance testing on the serving side of HAF in this configuration (i.e. API performance) and test more hardware and software configurations to determine the optimal setup, and we’ll be continuing that work in the coming week. # HAF account history app (aka hafah) We’re periodically testing hafah on our production system, then making improvements whenever this exposes a performance problem not discovered by automated testing. ### Improving API performance One performance problem that came up recently was an API call where hafah was taking 7 minutes to respond. We found a relatively simple initial solution that speeded it up to an acceptable level, but still worse than a standard account history node. Nobody likes to lose to existing software on a performance metric however, so we looked further and found a slightly more code-intensive solution that dropped the time down to 0.3s (which we’re pretty happy about). This solution will unfortunately require a little more data to be written to the HAF database by the sql_serializer plugin, so it won’t be fully completed until sometime next week. ### Dockerized HAF servers for app testing We’re continuing to work on creating dockerized HAF servers and modifying the continuous integration process (i.e. automated testing) for all HAF-based apps to re-use existing dockerized HAF servers when possible. This work is primarily being done in the Hafah repo. We made some good progress on this task, but then got distracted by the need to optimize the performance of the slow queries mentioned above. The work will resume on this task shortly. # Hivemind (social media middleware server used by web sites) We continued to work on conversion of Hivemind to a HAF-based app. We now have a version of hivemind that no longer needs to make any API calls to a hived node (i.e. now all indexing needs are filled from the data inside the HAF database). # What’s next? * Modify the one-step script for installing HAF to optionally download a trusted block_log and block_log.index file (or maybe just allow an option for fast-syncing using a checkpoint to reduce block processing time now that peer syncing process is faster and may actually perform better than downloading a block_log and replaying it). This task is on hold until we have someone free to work on it. * Continue work on filtering of operations by sql_serializer using regexs to allow for smaller HAF server databases. * Collect benchmarks for hafah operating in “irreversible block mode” and compare to a hafah operation in “normal” mode. * Further testing of hafah on production servers (api.hive.blog). * Finish conversion of hivemind to a HAF-based app. * More testing of new P2P code under forking conditions and various live mode scenarios and in a mirrornet testnet using only hived servers with the new P2P code. * Experiment with methods of improving block finality time. * Complete work on resource credit rationalization. * Continue benchmarking of HAF and Hafah on ZFS and EXT4 file systems with various hardware and software configurations. Current expected date for the next hardfork is still the end of April assuming no serious problems are uncovered during testing over the next week or two. But that date is rapidly approaching, so it is going to take a fairly herculean effort if we are to complete the key tasks in this timeframe. See: 4th update of 2022 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hivedocs.info/assets/images/favicon.png" /><media:content medium="image" url="https://hivedocs.info/assets/images/favicon.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Happy Forkday to the Hive Community</title><link href="https://hivedocs.info/news/core/development/2022/03/19/happy-forkday-2022.html" rel="alternate" type="text/html" title="Happy Forkday to the Hive Community" /><published>2022-03-19T20:13:00-07:00</published><updated>2022-03-19T20:13:00-07:00</updated><id>https://hivedocs.info/news/core/development/2022/03/19/happy-forkday-2022</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2022/03/19/happy-forkday-2022.html">&lt;div id=&quot;content-arcange-happy-forkday-2022&quot;&gt;&lt;center&gt;![](https://i.imgur.com/L9Dm919.png)&lt;/center&gt;

It's been two years since, on March 20, 2020, our community decided to take its destiny into its own hands and move away from the ~~Steem blockchain~~ centralized database whose name shall not be said anymore.

Two years later, we can confidently say that it was the right choice. Our blockchain and its community have never been so active. The values that are so important to us, such as decentralization, freedom of expression and being free from censorship, have been the fuel of our movement. 

The number of blockchain users has exploded thanks to the success of Splinterlands. Developments, both at the blockchain core level and at the applications level have continued without interruption and every week we see new projects appear. New front-ends, new apps, new services, ... our community is buzzing like crazy!

Our blockchain is a revolutionary combination of technical, financial and social aspects. Many enthusiasts have joined the adventure: developers, bloggers, investors ... Some have joined us recently but others have been there from the start, and they are still here.

We have many good reasons to celebrate our success today.

I am delighted to be part of this community, to participate in its development and to share with you all, early adopters and pioneers who are still with us, newcomers who join us to discover and share this experience, those values that are so important to me.

# &lt;center&gt;I wish you all a Happy Forkday!&lt;/center&gt;

---
&lt;center&gt;

##### Check out my apps and services
&lt;a href=&quot;/hive/@hive.engage/stay-connected-with-your-hive-audience-and-catch-attention&quot;&gt;&lt;img src=&quot;https://i.imgur.com/GiNJqlm.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/@hivebuzz&quot;&gt;&lt;img src=&quot;https://i.imgur.com/B4UTun2.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/@hivesql&quot;&gt;&lt;img src=&quot;https://i.imgur.com/EPN8RW6.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/hive/@arcange/introducing-hive-account-recovery&quot;&gt;&lt;img src=&quot;https://i.imgur.com/6TWeW7V.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/@hive.autoclaim&quot;&gt;&lt;img src=&quot;https://i.imgur.com/ih2pEOw.png&quot; /&gt;&lt;/a&gt;&lt;/center&gt;
&lt;center&gt;

### [Vote for me as a witness ![](https://i.imgur.com/2bi4SnT.png)](https://vote.hive.uno/@arcange)&amp;lt;/div&amp;gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;
&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@arcange&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/happy-forkday-2022&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-arcange-happy-forkday-2022').html();
      const outputElem = $('#content-arcange-happy-forkday-2022');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;
&lt;style&gt;
  #content-arcange-happy-forkday-2022 {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-arcange-happy-forkday-2022 code {
    background: white;
  }
  #content-arcange-happy-forkday-2022 a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-arcange-happy-forkday-2022 a:hover {
    border-bottom: 0;
  }
  #content-arcange-happy-forkday-2022 h1 {
    font-size: 2.2em;
  }
  #content-arcange-happy-forkday-2022 h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-arcange-happy-forkday-2022 header small {
    color: #999;
    font-size: 50%;
  }
  #content-arcange-happy-forkday-2022 img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;
&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/hive-111111/@arcange/happy-forkday-2022&quot;&gt;Happy Forkday to the Hive Community&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@arcange&quot;&gt;@arcange&lt;/a&gt;
&lt;/p&gt;

&lt;/center&gt;&lt;/div&gt;</content><author><name>arcange</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![](https://i.imgur.com/L9Dm919.png) It's been two years since, on March 20, 2020, our community decided to take its destiny into its own hands and move away from the ~~Steem blockchain~~ centralized database whose name shall not be said anymore. Two years later, we can confidently say that it was the right choice. Our blockchain and its community have never been so active. The values that are so important to us, such as decentralization, freedom of expression and being free from censorship, have been the fuel of our movement. The number of blockchain users has exploded thanks to the success of Splinterlands. Developments, both at the blockchain core level and at the applications level have continued without interruption and every week we see new projects appear. New front-ends, new apps, new services, ... our community is buzzing like crazy! Our blockchain is a revolutionary combination of technical, financial and social aspects. Many enthusiasts have joined the adventure: developers, bloggers, investors ... Some have joined us recently but others have been there from the start, and they are still here. We have many good reasons to celebrate our success today. I am delighted to be part of this community, to participate in its development and to share with you all, early adopters and pioneers who are still with us, newcomers who join us to discover and share this experience, those values that are so important to me. # I wish you all a Happy Forkday! --- ##### Check out my apps and services ### [Vote for me as a witness ![](https://i.imgur.com/2bi4SnT.png)](https://vote.hive.uno/@arcange)&amp;lt;/div&amp;gt; See: Happy Forkday to the Hive Community by @arcange</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://i.imgur.com/L9Dm919.png" /><media:content medium="image" url="https://i.imgur.com/L9Dm919.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>