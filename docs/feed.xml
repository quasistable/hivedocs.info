<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://hivedocs.info/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hivedocs.info/" rel="alternate" type="text/html" /><updated>2021-08-19T08:40:57-07:00</updated><id>https://hivedocs.info/feed.xml</id><title type="html">Hive Chain Documentation</title><subtitle>Your resource for various levels of Hive Documentation.</subtitle><author><name>site curated by: @inertia</name></author><entry><title type="html">Hive core developer meeting #27</title><link href="https://hivedocs.info/news/core/development/2021/08/15/hive-core-developer-meeting-27.html" rel="alternate" type="text/html" title="Hive core developer meeting #27" /><published>2021-08-15T08:47:39-07:00</published><updated>2021-08-15T08:47:39-07:00</updated><id>https://hivedocs.info/news/core/development/2021/08/15/hive-core-developer-meeting-27</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/08/15/hive-core-developer-meeting-27.html">&lt;div id=&quot;content-howo-hive-core-developer-meeting-27&quot;&gt;https://www.youtube.com/watch?v=8DFXZPK9mQ0
This meeting was pushed back a few times because most of the team was in the middle of developments or busy improving the api nodes/gitlab so there weren't much to sync on.

Meeting points:

# Dev sync 

It's better to listen to this one.

# Rc delegations and potential performance hog

RC delegations, is now pushed and ready to be reviewed by blocktrade's team. Should this review be positive, I'll probably spin up a small testnet for everyone to play with it.

There is one thing regarding rc delegations (that was also present in the RC pools implementation) that can be worrysome, we discuss some of the potential solutions to it. But we didn't go too much in depth as blocktrades team didn't see the code yet so it's hard for them to truly discuss the solutions. 

# HAF progress and communities migration to it
(some expected times + is it worth to wait for the migration to do some developments on it)

I working on communities after rc delegations, so since there was a plan to migrate hivemind to HAF, and as per this comment by @blocktrades: https://peakd.com/hive/@blocktrades/qxdi2b 

```
In my opinion, it would be a mistake to start this work [work on communities] before we complete HAF work. The next step in hivemind's evolution is to port it to HAF. After that, it should be no big deal for someone to enhance communities further.
```

I wanted to confirm the best way forward, ultimately after discussing we decided that porting hivemind onto HAF would not be a blocker to some development on communities right now.

# Gitlab issues

The gitlab instance where most of the code, issues etc suffered a failure and all the backups also failed, so we didn't lose code but some of the metadata (issues/merge requests/comments) from may to august were lost. If you submitted something in that timespan, make sure to resubmit it.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@howo&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/hive-core-developer-meeting-27&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-howo-hive-core-developer-meeting-27').html();
      const outputElem = $('#content-howo-hive-core-developer-meeting-27');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-howo-hive-core-developer-meeting-27 {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-howo-hive-core-developer-meeting-27 code {
    background: white;
  }
  #content-howo-hive-core-developer-meeting-27 a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-howo-hive-core-developer-meeting-27 a:hover {
    border-bottom: 0;
  }
  #content-howo-hive-core-developer-meeting-27 h1 {
    font-size: 2.2em;
  }
  #content-howo-hive-core-developer-meeting-27 h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-howo-hive-core-developer-meeting-27 header small {
    color: #999;
    font-size: 50%;
  }
  #content-howo-hive-core-developer-meeting-27 img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/core/@howo/hive-core-developer-meeting-27&quot;&gt;Hive core developer meeting #27&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@howo&quot;&gt;@howo&lt;/a&gt;
&lt;/p&gt;</content><author><name>howo</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">https://www.youtube.com/watch?v=8DFXZPK9mQ0 This meeting was pushed back a few times because most of the team was in the middle of developments or busy improving the api nodes/gitlab so there weren't much to sync on. Meeting points: # Dev sync It's better to listen to this one. # Rc delegations and potential performance hog RC delegations, is now pushed and ready to be reviewed by blocktrade's team. Should this review be positive, I'll probably spin up a small testnet for everyone to play with it. There is one thing regarding rc delegations (that was also present in the RC pools implementation) that can be worrysome, we discuss some of the potential solutions to it. But we didn't go too much in depth as blocktrades team didn't see the code yet so it's hard for them to truly discuss the solutions. # HAF progress and communities migration to it (some expected times + is it worth to wait for the migration to do some developments on it) I working on communities after rc delegations, so since there was a plan to migrate hivemind to HAF, and as per this comment by @blocktrades: https://peakd.com/hive/@blocktrades/qxdi2b ``` In my opinion, it would be a mistake to start this work [work on communities] before we complete HAF work. The next step in hivemind's evolution is to port it to HAF. After that, it should be no big deal for someone to enhance communities further. ``` I wanted to confirm the best way forward, ultimately after discussing we decided that porting hivemind onto HAF would not be a blocker to some development on communities right now. # Gitlab issues The gitlab instance where most of the code, issues etc suffered a failure and all the backups also failed, so we didn't lose code but some of the metadata (issues/merge requests/comments) from may to august were lost. If you submitted something in that timespan, make sure to resubmit it. See: Hive core developer meeting #27 by @howo</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hivedocs.info/assets/images/favicon.png" /><media:content medium="image" url="https://hivedocs.info/assets/images/favicon.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Reconfiguration notice for api.hive.blog API node infrastructure</title><link href="https://hivedocs.info/news/core/development/2021/08/02/reconfiguration-notice-for-api-hive-blog-api-node-infrastructure.html" rel="alternate" type="text/html" title="Reconfiguration notice for api.hive.blog API node infrastructure" /><published>2021-08-02T14:58:54-07:00</published><updated>2021-08-02T14:58:54-07:00</updated><id>https://hivedocs.info/news/core/development/2021/08/02/reconfiguration-notice-for-api-hive-blog-api-node-infrastructure</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/08/02/reconfiguration-notice-for-api-hive-blog-api-node-infrastructure.html">&lt;div id=&quot;content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure&quot;&gt;As I mentioned in my last post on development progress at BlockTrades, we’ve been asking Hive-based apps to change the way they broadcast transactions. Transactions are used to add user-generated operations such as voting, posting, transferring funds, delegating HP, etcetera  to the Hive blockchain. 

In this post I’m going to briefly describe why we’re requesting this change and also describe a change we’ve made to our API node infrastructure  to better handle the increased traffic from bots playing Splinterlands with “bad” API calls. This latter information will likely be interesting to other API node operators.

# Apps should  use broadcast_transaction call

Most Hive-based apps previously used the call `broadcast_transaction_synchronous` to broadcast transactions. This call, as the name implies, waits for a transaction to be included into the blockchain before it returns to the calling application. The problem with this call is that we’ve measured it takes about 3 seconds on average to complete on our API node infrastructure even under normal loading conditions. That’s a long time to a computer.

So if one of our hived nodes gets a lot of these calls, the calls keep all of that hived’s worker threads busy, effectively slowing down all API calls made to that hived server. 

By adding additional logging to one of our hived nodes, we were able to observe that even read-only API calls arriving at the loaded hived node could wait 2 seconds (or even more under very heavy loads) before they got assigned a worker thread to process them. A few days ago, this was noticeable as a slowdown on not only web sites that broadcast transactions such as hive.blog, ecency, and peakd, but even on read-only Hive-based sites (e.g. Hive block explorers such as https://hiveblocks.com).

The ideal solution to this problem is for apps to replace all these slow calls with the newer, faster `broadcast_transaction` call. This call doesn’t wait for the transaction to be included into the blockchain, and it completes on api.hive.blog within an average of 0.027 seconds (more than 100x faster than the synchronous version).

## Most hive apps have already migrated to the better API call

Most of the major Hive apps have moved to the faster broadcast call within the past few days, as app devs saw how much more responsive their app became with the new calls (i.e. they became faster than they ever were before, even when we had less loading on the Hive network). 

And I expect the few remaining big apps, such as hive.blog, to convert to using the new calls within the next couple of days (we have an engineer working now to fix one known issue preventing us from rolling out the latest version of condenser with the broadcast_transaction fix).

# Mitigating bad traffic from misbehaving bots

But despite movement of the major Hive apps to the faster call, we still see a lot of broadcast_transaction_synchronous traffic on our node being generated by presumably home-grown bots playing Splinterlands for their users. I suppose these bot devs will eventually fix their bots, but in the meantime, we have no easy way to contact them, so we’ve made a change to our API node infrastructure so that this “bad” traffic doesn’t impact the apps generating “good traffic”. 

We have redirected all incoming broadcast_transaction_synchronous traffic to a single hived node that only processes this type of API traffic, and all other API calls (including the “good” broadcast_transaction) are routed to our other hived nodes. This means that apps using the proper calls will not be slowed down by the bad traffic. And it will probably ultimately encourage the bad traffic generators to change their bots as well, although I’m not holding my breath for when that will happen.

# Add an extra consensus hived to manage broadcast_transaction_synchronous traffic
If other API nodes want to be capable of serving all the traffic from the Hive network right now, here’s the relatively easy way to do it:
* add one additional consensus (not account history) hived node to your server. Since consensus nodes don’t require much memory (around 4GB), the main additional resource cost is around 370GB of disk space for the additional block_log file. You only need a consensus node because it is only going to be processing broadcast calls and nothing else.
* Modify your jussi config file to redirect all below types of traffic to your consensus node:
** steemd.network_broadcast_api.broadcast_transaction_synchronous
** appbase.condenser_api.broadcast_transaction_synchronous
** appbase.network_broadcast_api.broadcast_transaction_synchronous

With the above steps, your regular account_history node(s) will process all the good traffic and the bad traffic will be offloaded to the light weight consensus node. Also, if you want to further improve quality of service for the bad traffic, you can increase the web-threads setting in your consensus node’s configuration file from the default value of 32 to 64 (or higher), at the cost of increased memory usage.

# Quick Update
We swapped over hive.blog to use the new calls earlier today and we swapped over wallet.hive.blog about an hour ago. The UX feels much more responsive now.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/reconfiguration-notice-for-api-hive-blog-api-node-infrastructure&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure').html();
      const outputElem = $('#content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure code {
    background: white;
  }
  #content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-102930/@blocktrades/reconfiguration-notice-for-api-hive-blog-api-node-infrastructure&quot;&gt;Reconfiguration notice for api.hive.blog API node infrastructure&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">As I mentioned in my last post on development progress at BlockTrades, we’ve been asking Hive-based apps to change the way they broadcast transactions. Transactions are used to add user-generated operations such as voting, posting, transferring funds, delegating HP, etcetera to the Hive blockchain. In this post I’m going to briefly describe why we’re requesting this change and also describe a change we’ve made to our API node infrastructure to better handle the increased traffic from bots playing Splinterlands with “bad” API calls. This latter information will likely be interesting to other API node operators. # Apps should use broadcast_transaction call Most Hive-based apps previously used the call `broadcast_transaction_synchronous` to broadcast transactions. This call, as the name implies, waits for a transaction to be included into the blockchain before it returns to the calling application. The problem with this call is that we’ve measured it takes about 3 seconds on average to complete on our API node infrastructure even under normal loading conditions. That’s a long time to a computer. So if one of our hived nodes gets a lot of these calls, the calls keep all of that hived’s worker threads busy, effectively slowing down all API calls made to that hived server. By adding additional logging to one of our hived nodes, we were able to observe that even read-only API calls arriving at the loaded hived node could wait 2 seconds (or even more under very heavy loads) before they got assigned a worker thread to process them. A few days ago, this was noticeable as a slowdown on not only web sites that broadcast transactions such as hive.blog, ecency, and peakd, but even on read-only Hive-based sites (e.g. Hive block explorers such as https://hiveblocks.com). The ideal solution to this problem is for apps to replace all these slow calls with the newer, faster `broadcast_transaction` call. This call doesn’t wait for the transaction to be included into the blockchain, and it completes on api.hive.blog within an average of 0.027 seconds (more than 100x faster than the synchronous version). ## Most hive apps have already migrated to the better API call Most of the major Hive apps have moved to the faster broadcast call within the past few days, as app devs saw how much more responsive their app became with the new calls (i.e. they became faster than they ever were before, even when we had less loading on the Hive network). And I expect the few remaining big apps, such as hive.blog, to convert to using the new calls within the next couple of days (we have an engineer working now to fix one known issue preventing us from rolling out the latest version of condenser with the broadcast_transaction fix). # Mitigating bad traffic from misbehaving bots But despite movement of the major Hive apps to the faster call, we still see a lot of broadcast_transaction_synchronous traffic on our node being generated by presumably home-grown bots playing Splinterlands for their users. I suppose these bot devs will eventually fix their bots, but in the meantime, we have no easy way to contact them, so we’ve made a change to our API node infrastructure so that this “bad” traffic doesn’t impact the apps generating “good traffic”. We have redirected all incoming broadcast_transaction_synchronous traffic to a single hived node that only processes this type of API traffic, and all other API calls (including the “good” broadcast_transaction) are routed to our other hived nodes. This means that apps using the proper calls will not be slowed down by the bad traffic. And it will probably ultimately encourage the bad traffic generators to change their bots as well, although I’m not holding my breath for when that will happen. # Add an extra consensus hived to manage broadcast_transaction_synchronous traffic If other API nodes want to be capable of serving all the traffic from the Hive network right now, here’s the relatively easy way to do it: * add one additional consensus (not account history) hived node to your server. Since consensus nodes don’t require much memory (around 4GB), the main additional resource cost is around 370GB of disk space for the additional block_log file. You only need a consensus node because it is only going to be processing broadcast calls and nothing else. * Modify your jussi config file to redirect all below types of traffic to your consensus node: ** steemd.network_broadcast_api.broadcast_transaction_synchronous ** appbase.condenser_api.broadcast_transaction_synchronous ** appbase.network_broadcast_api.broadcast_transaction_synchronous With the above steps, your regular account_history node(s) will process all the good traffic and the bad traffic will be offloaded to the light weight consensus node. Also, if you want to further improve quality of service for the bad traffic, you can increase the web-threads setting in your consensus node’s configuration file from the default value of 32 to 64 (or higher), at the cost of increased memory usage. # Quick Update We swapped over hive.blog to use the new calls earlier today and we swapped over wallet.hive.blog about an hour ago. The UX feels much more responsive now. See: Reconfiguration notice for api.hive.blog API node infrastructure by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hivedocs.info/assets/images/favicon.png" /><media:content medium="image" url="https://hivedocs.info/assets/images/favicon.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Everything can be faster on Hive - hive-js &amp;amp; dhive new changes</title><link href="https://hivedocs.info/howto/nodejs/2021/08/02/everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes.html" rel="alternate" type="text/html" title="Everything can be faster on Hive - hive-js &amp;amp; dhive new changes" /><published>2021-08-02T00:07:03-07:00</published><updated>2021-08-02T00:07:03-07:00</updated><id>https://hivedocs.info/howto/nodejs/2021/08/02/everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes</id><content type="html" xml:base="https://hivedocs.info/howto/nodejs/2021/08/02/everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes.html">&lt;div id=&quot;content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes&quot;&gt;&lt;center&gt;![alien-hive.jpg](https://files.peakd.com/file/peakd-hive/mahdiyari/23zS3SZnJwHE4qThdmvWRdRpLARpcFMm1Vq8LfDLqscHen53JFeNqq7EEGVfvhd2xcpTo.jpg)&lt;/center&gt;

Explaining why there were problems with RPC nodes and what was the problem and why you must update your application.

### Target audience
I explained everything as simple as I could. The main call is for the app owners to update their applications with the updated libraries.

After the recent timeout problems, it was necessary to take these steps and update the libraries.

I updated 2 major JS libraries, hive-js and dhive, to help resolve the timeout problems.

### The &quot;call&quot; method
The &quot;call&quot; method was a way of talking to hived (the hive blockchain's application). This method got deprecated and removed from hived with the appbase update (HF19?).

So what has happened since then? Well, we were still using the &quot;call&quot; method for most of our applications. But the &quot;call&quot; method is not known to hived anymore, so the requests have to go through a proxy called &quot;jussi&quot;. Jussi is installed in front of all the public RPC nodes.

Jussi takes the &quot;call&quot; method and translates it into the correct format then sends it to the hived. The problem is, you are adding an extra step for the API requests and it slows them down.

An example of the &quot;call&quot; method:
```
{&quot;id&quot;:0,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;method&quot;:&quot;call&quot;,&quot;params&quot;:[&quot;database_api&quot;,&quot;get_dynamic_global_properties&quot;,[]]}
```
Notice `&quot;method&quot;:&quot;call&quot;`. So jussi takes that and translates it into the new format:
```
{&quot;id&quot;:0,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;method&quot;:&quot;condenser_api.get_dynamic_global_properties&quot;,&quot;params&quot;: []}
```
See what happened? The user thinks the call was going to `database_api` but actually, jussi translates it into `condenser_api`.

To be honest, this is not the best way of doing things. The idea behind jussi was good but implementation was a mess.

***
### What is the &quot;new&quot; format?
The &quot;appbase&quot; style calls are the calls that are supported directly by hived and don't need a translation.

It's not new and has been there for years but we didn't bother updating our libraries to use the supported methods.

With switching to the appbase calls, every request to the RPC nodes becomes faster.


This is the first change to both hive-js and dhive libraries. They both were using the deprecated &quot;call&quot; method.
***
### You might not need jussi
Till now, hive-js wouldn't work with hived nodes until you added jussi in front of them. But with this update, hive-js will work with hived nodes. But remember, you will still need jussi for the hivemind calls. Also, there might be caching benefits in using jussi.
***
### Broadcasting transactions
This is the important part. There are two methods for broadcasting a transaction.
1. `broadcast_transaction_synchronous`
2. `broadcast_transaction`

The first method sends the transaction to the RPC node, the RPC node validates the transaction, then waits for the transaction to be included in a block, then sends back a response including the transaction id and block number.

The second method sends the transaction to the RPC node, the RPC node validates the transaction, then sends an empty success response back.

In the second method, the library has to generate the transaction id locally. Which is way too much faster. In this method, the only way for a transaction to fail is expiration or micro forking maybe. So in most cases, it's good enough.

Of course, hive-js and dhive both were using the first method. When many people connect to the RPC node and wait there, it becomes problematic. The best way for handling huge traffic is to release a connection as soon as possible so other connections can be made.

Both libraries now use the second method for broadcasting transactions. You may already notice its huge impact on the speed of your votes and comments.

Peakd is already updated. Keychain is waiting for google approval and the mobile version will be soon updated. Some apps are catching up like Hivesigner, and Ecency. There was a problem updating hive.blog and that will be also updated.

***
### Why bother updating
- Everything becomes faster for the end-user
- RPC nodes don't suffer overloading
- Apps become future proof
- It's easy as running a command

***
### How to update your apps
**hvie-js v2.0.1** - [Release notes](https://gitlab.syncad.com/hive/hive-js/-/releases)
Used in hive.blog, peakd, keychain, and many other apps.
```
npm install @hiveio/hive-js@latest --save
```
The minified JS for browsers:
```
https://cdn.jsdelivr.net/npm/@hiveio/hive-js@2/dist/hive.min.js
```
***
**dhive v1.0.0** - [Release notes](https://gitlab.syncad.com/hive/dhive/-/releases)
Used in ecency (mobile &amp;amp; web) and many other apps.
```
npm install @hiveio/dhive@latest --save
```
The minified JS for browsers:
```
https://cdn.jsdelivr.net/npm/@hiveio/dhive@1/dist/dhive.js
```
***
**hive-tx v4.0.0** - [Release notes](https://github.com/mahdiyari/hive-tx-js/releases)
Used in keychain mobile and other apps.

hive-tx already is using the appbase methods and there is a function called `.broadcastNoResult()` for the fast transaction broadcasting. But I did update hive-tx so now the `.broadcast()` function is also updated.
```
npm install hive-tx@latest --save
```
The minified JS for browsers:
```
https://cdn.jsdelivr.net/npm/hive-tx@4/dist/hive-tx.min.js
```


***
Note:
@foxon and @emrebeyler confirmed that `beem` doesn't need an update.
***

### Scaling problems?
The recent problem doesn't seem to be a scaling problem. It's more related to the libraries and apps using the deprecated methods.

The recent library changes seem to be solving most of the problems. Although there are investigations still going on by @blocktrades regarding the timeout problems.

Hive blockchain in theory can handle much more activity than this. Some say like 1,000 transactions per second. But there might be minor problems because those scenarios are not actually tested.
***

### Final words
I'm sure all the problems will be solved and we will see much better growth. I'm bullish on Hive and will continue my contributions as far as I can.

Thanks to the other devs who updated their applications on time to help with the problem.

***
&lt;sub&gt;Image source: pixabay.com&lt;/sub&gt;&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@mahdiyari&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes').html();
      const outputElem = $('#content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes code {
    background: white;
  }
  #content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes a:hover {
    border-bottom: 0;
  }
  #content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes h1 {
    font-size: 2.2em;
  }
  #content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes header small {
    color: #999;
    font-size: 50%;
  }
  #content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/hive-139531/@mahdiyari/everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes&quot;&gt;Everything can be faster on Hive - hive-js &amp;amp; dhive new changes&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@mahdiyari&quot;&gt;@mahdiyari&lt;/a&gt;
&lt;/p&gt;</content><author><name>mahdiyari</name></author><category term="howto" /><category term="nodejs" /><summary type="html">![alien-hive.jpg](https://files.peakd.com/file/peakd-hive/mahdiyari/23zS3SZnJwHE4qThdmvWRdRpLARpcFMm1Vq8LfDLqscHen53JFeNqq7EEGVfvhd2xcpTo.jpg) Explaining why there were problems with RPC nodes and what was the problem and why you must update your application. ### Target audience I explained everything as simple as I could. The main call is for the app owners to update their applications with the updated libraries. After the recent timeout problems, it was necessary to take these steps and update the libraries. I updated 2 major JS libraries, hive-js and dhive, to help resolve the timeout problems. ### The &quot;call&quot; method The &quot;call&quot; method was a way of talking to hived (the hive blockchain's application). This method got deprecated and removed from hived with the appbase update (HF19?). So what has happened since then? Well, we were still using the &quot;call&quot; method for most of our applications. But the &quot;call&quot; method is not known to hived anymore, so the requests have to go through a proxy called &quot;jussi&quot;. Jussi is installed in front of all the public RPC nodes. Jussi takes the &quot;call&quot; method and translates it into the correct format then sends it to the hived. The problem is, you are adding an extra step for the API requests and it slows them down. An example of the &quot;call&quot; method: ``` {&quot;id&quot;:0,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;method&quot;:&quot;call&quot;,&quot;params&quot;:[&quot;database_api&quot;,&quot;get_dynamic_global_properties&quot;,[]]} ``` Notice `&quot;method&quot;:&quot;call&quot;`. So jussi takes that and translates it into the new format: ``` {&quot;id&quot;:0,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;method&quot;:&quot;condenser_api.get_dynamic_global_properties&quot;,&quot;params&quot;: []} ``` See what happened? The user thinks the call was going to `database_api` but actually, jussi translates it into `condenser_api`. To be honest, this is not the best way of doing things. The idea behind jussi was good but implementation was a mess. *** ### What is the &quot;new&quot; format? The &quot;appbase&quot; style calls are the calls that are supported directly by hived and don't need a translation. It's not new and has been there for years but we didn't bother updating our libraries to use the supported methods. With switching to the appbase calls, every request to the RPC nodes becomes faster. This is the first change to both hive-js and dhive libraries. They both were using the deprecated &quot;call&quot; method. *** ### You might not need jussi Till now, hive-js wouldn't work with hived nodes until you added jussi in front of them. But with this update, hive-js will work with hived nodes. But remember, you will still need jussi for the hivemind calls. Also, there might be caching benefits in using jussi. *** ### Broadcasting transactions This is the important part. There are two methods for broadcasting a transaction. 1. `broadcast_transaction_synchronous` 2. `broadcast_transaction` The first method sends the transaction to the RPC node, the RPC node validates the transaction, then waits for the transaction to be included in a block, then sends back a response including the transaction id and block number. The second method sends the transaction to the RPC node, the RPC node validates the transaction, then sends an empty success response back. In the second method, the library has to generate the transaction id locally. Which is way too much faster. In this method, the only way for a transaction to fail is expiration or micro forking maybe. So in most cases, it's good enough. Of course, hive-js and dhive both were using the first method. When many people connect to the RPC node and wait there, it becomes problematic. The best way for handling huge traffic is to release a connection as soon as possible so other connections can be made. Both libraries now use the second method for broadcasting transactions. You may already notice its huge impact on the speed of your votes and comments. Peakd is already updated. Keychain is waiting for google approval and the mobile version will be soon updated. Some apps are catching up like Hivesigner, and Ecency. There was a problem updating hive.blog and that will be also updated. *** ### Why bother updating - Everything becomes faster for the end-user - RPC nodes don't suffer overloading - Apps become future proof - It's easy as running a command *** ### How to update your apps **hvie-js v2.0.1** - [Release notes](https://gitlab.syncad.com/hive/hive-js/-/releases) Used in hive.blog, peakd, keychain, and many other apps. ``` npm install @hiveio/hive-js@latest --save ``` The minified JS for browsers: ``` https://cdn.jsdelivr.net/npm/@hiveio/hive-js@2/dist/hive.min.js ``` *** **dhive v1.0.0** - [Release notes](https://gitlab.syncad.com/hive/dhive/-/releases) Used in ecency (mobile &amp;amp; web) and many other apps. ``` npm install @hiveio/dhive@latest --save ``` The minified JS for browsers: ``` https://cdn.jsdelivr.net/npm/@hiveio/dhive@1/dist/dhive.js ``` *** **hive-tx v4.0.0** - [Release notes](https://github.com/mahdiyari/hive-tx-js/releases) Used in keychain mobile and other apps. hive-tx already is using the appbase methods and there is a function called `.broadcastNoResult()` for the fast transaction broadcasting. But I did update hive-tx so now the `.broadcast()` function is also updated. ``` npm install hive-tx@latest --save ``` The minified JS for browsers: ``` https://cdn.jsdelivr.net/npm/hive-tx@4/dist/hive-tx.min.js ``` *** Note: @foxon and @emrebeyler confirmed that `beem` doesn't need an update. *** ### Scaling problems? The recent problem doesn't seem to be a scaling problem. It's more related to the libraries and apps using the deprecated methods. The recent library changes seem to be solving most of the problems. Although there are investigations still going on by @blocktrades regarding the timeout problems. Hive blockchain in theory can handle much more activity than this. Some say like 1,000 transactions per second. But there might be minor problems because those scenarios are not actually tested. *** ### Final words I'm sure all the problems will be solved and we will see much better growth. I'm bullish on Hive and will continue my contributions as far as I can. Thanks to the other devs who updated their applications on time to help with the problem. *** Image source: pixabay.com See: Everything can be faster on Hive - hive-js &amp;amp; dhive new changes by @mahdiyari</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://files.peakd.com/file/peakd-hive/mahdiyari/23zS3SZnJwHE4qThdmvWRdRpLARpcFMm1Vq8LfDLqscHen53JFeNqq7EEGVfvhd2xcpTo.jpg" /><media:content medium="image" url="https://files.peakd.com/file/peakd-hive/mahdiyari/23zS3SZnJwHE4qThdmvWRdRpLARpcFMm1Vq8LfDLqscHen53JFeNqq7EEGVfvhd2xcpTo.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">20th update of 2021 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2021/07/29/20th-update-of-2021-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="20th update of 2021 on BlockTrades work on Hive software" /><published>2021-07-29T14:42:09-07:00</published><updated>2021-07-29T14:42:09-07:00</updated><id>https://hivedocs.info/news/core/development/2021/07/29/20th-update-of-2021-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/07/29/20th-update-of-2021-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)

Below is a list of Hive-related programming issues worked on by BlockTrades team during last week or so:

# Hived work (blockchain node software)

### Continued development of TestTools framework
We made a substantial number of changes to the new TestTools framework that we are now using to do blackbox testing on hived: https://gitlab.syncad.com/hive/hive/-/merge_requests/278

There are too many changes to discuss individually (see the merge request for full details), but at a high-level, the new testing framework allows us to perform more complex tests faster and more efficiently than the previous framework.


### Command-line interface (CLI) wallet enhancements
We continued work on improvements to CLI wallet (refactoring to remove need for duplicate code to support legacy operations, upgrading cli wallet tests to use TestTools, support for signing of transactions based on authority) and I expect we’ll merge in these changes in the coming week.

### Continuing work on blockhain converter tool
We’re also continuing work on the blockchain converter that generates a testnet blockchain configuration form an existing blocklog. Most recently, we added multithreading support to speed it up and those changes are being tested now. You can follow the work on this task here: https://gitlab.syncad.com/hive/hive/-/commits/tm-blockchain-converter/

# Hivemind (2nd layer applications + social media middleware)


### Reduced peak memory consumption
We reviewed and merged in the changes to reduce memory usage of hive sync (just over 4GB footprint now, whereas previous peak usage was approaching 14GB). Finalized changes that were merged in are here: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/527/diffs


### Optimized update_post_rshares

We completed and merged in the code that I mentioned last week that creates a temporary index for faster execution of the update_post_rshares function and we were finally able to achieve the same speedup in a full sync of hivemind on one of our production systems after some tweaks to the code (function with temporary index processed 52 million blocks in 19 minutes). 

To achieve the same results that we saw in our development testing, we found we had to perform a `vacuum analyze` on the `hive_votes` table prior to running the update_post_rshares function to ensure the query planner had the proper statistics to generate an efficient query plan. The merge request for the optimized code is here:
https://gitlab.syncad.com/hive/hivemind/-/merge_requests/521

### Optimized process of effective_comment_vote operation during massive sync

When we know that a particular post will be paid out before end of massive sync block processing, we can skip processing of effective_comment_vote_operations for such posts. This optimization reduced the amount of post records that we need to flush to the database during massive sync by more than 50%. The merge request for this optimization is here: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/525


### Merged in new hive sync option --max-retries
We also finished review and testing of the new –max-retries option that allows configuring how many retries (or an indefinite number of retries) before the hive sync process will shutdown if it loses contact with the hived serving blockchain data to it: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/526

# Optimized api.hive.blog servers (BlockTrades-supported API node)

While there are number of API nodes available nowadays, many Hive apps default to using our node (which is often useful, since it allows us to quickly spot any scaling issue that might be arising as Hive API traffic increases with time). Here is a graph of how that traffic has increased over the last year:

![image.png](https://images.hive.blog/DQmWoWKQctKxSMTj5YXm7JfmVdZN42c2NCJt28StVKqwLvd/image.png)

As you can see, our incoming API traffic (the top part of the graph) has roughly tripled over the year.

### Solved issue with transaction timeouts

Yesterday we started getting some reports of timeouts on transactions processed via api.hive.blog. The immediate suspicion, of course, was that this was due to the increased traffic coming from splinterlands servers, which turned out to be the case. 

But we weren’t seeing much CPU or IO loading on our servers, despite the increased traffic (in fact, we had substantial headroom there and we could likely easily handle 12x or more than the traffic we were receiving based on CPU bottlenecking just with our existing servers), so this lead us to suspect a network-related problem.

As a quick fix, we tried making some changes to the network configuration parameters of our servers based on recommendations from @mahdiyari, and we had a report from him that this helped some, but we were still getting reports of a fair number of timeouts, so we decided to do a more thorough analysis of the issue.

To properly analyze the network traffic, we first had to make some improvements to the jussi traffic analyzer that we use to analyze loading, because most of the increased traffic was encoding the nature of the request in the post bodies, which means the details of the request weren’t seen by the analyzer tool (e.g. we couldn’t see exactly what types of requests were creating the network problems). With these changes, the jussi analyzer can now distinguish what type of request is being made, allowing us to see which type of requests were slow and/or timing out.

After making this change, we found that the requests that were timing out were mostly transaction broadcasts using the old style “pre-appbase” format. Requests of this type can’t be directly processed by a hived node, but we run a jussi gateway on api.hive.blog that converts these legacy-formatted requests into appbase-formatted requests (effectively making our node backwards-compatible with these old style requests). 

So this was our first clue as to what the real problem was. After investigating how our jussi gateway was configured, we found that the converted requests were being sent to our hived nodes using the websocket protocol, unlike most other requests, which were sent via http. So, on a hunch, we modified the configuration of our jussi process to send translated requests as http requests instead of web socket requests and this eliminated the timeouts (as confirmed by both the jussi analyzer, beacon.peakd.com, and individual script testing by devs).

At this point, our node is operating very smoothly, despite signs that traffic has further increased beyond even yesterday’s traffic, so I’m confident we’ve solved all immediate issues.

### Replacement of calls to broadcast_transaction_synchronous with broadcast_transaction

Despite handling traffic fine now, I’m also a bit concerned about the use of broadcast_transaction_synchronous calls. These are blocking calls that tend to hold a connection open for 1.5s or more on average (half a block interval) because they wait for the transaction be included in the blockchain before returning. 

I’ve asked the library devs to look into changing their libraries to begin relying on the newer broadcast_transaction operation (a non-blocking call) and use the transaction_status API call to determine when their transaction has been included into the block, which should eliminate the large number of open connections that can occur when many transactions are being broadcast at once.

@mahdiyari has already made changes to hive-js (the Javascript library for hive apps) along these lines (as well as replacing the use of legacy-formatted API calls with appbase formatted API calls) and apps developers are now beginning to test their apps with this beta version of hive-js. Assuming this process goes smoothly, I anticipate that other library devs will swiftly follow suit.

# Hive Application Framework (HAF)

Some enhancements to HAF have been made to support synchronizing of multiple Hive apps operating on a HAF server. This allows an app that relies on the data of other apps to be sure that those apps have   processed all blocks up to the point where the dependent app is currently working at. This enhancement also involved support for secure sharing of data between HAF apps on a HAF server (for example, a dependent app can read, but not write, data in the tables of the app it depends on).

As I understand it, we now have a sample application built using HAF, but I haven’t had a chance to review it yet, as I was busy yesterday analyzing and optimizing our web infrastructure with our infrastructure team as discussed above. But this is high on my personal priority list, so I plan to review the work done here soon.

# What’s next?

We’ve resumed work on the sql_serializer plugin, which is one of the last key pieces needed before we can release HAF. Once those changes are completed, we’ll be able to do an end-to-end test with hived→sql_serializer→hivemind with a full sync of the blockchain. Perhaps I’m optimistic about the resulting speedup, but it’s possible we could have results as early as next Monday. I’m hoping we have some form of HAF ready for early beta testers within a week or two, but bear in mind that is a best case scenario.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/20th-update-of-2021-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/20th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;20th update of 2021 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below is a list of Hive-related programming issues worked on by BlockTrades team during last week or so: # Hived work (blockchain node software) ### Continued development of TestTools framework We made a substantial number of changes to the new TestTools framework that we are now using to do blackbox testing on hived: https://gitlab.syncad.com/hive/hive/-/merge_requests/278 There are too many changes to discuss individually (see the merge request for full details), but at a high-level, the new testing framework allows us to perform more complex tests faster and more efficiently than the previous framework. ### Command-line interface (CLI) wallet enhancements We continued work on improvements to CLI wallet (refactoring to remove need for duplicate code to support legacy operations, upgrading cli wallet tests to use TestTools, support for signing of transactions based on authority) and I expect we’ll merge in these changes in the coming week. ### Continuing work on blockhain converter tool We’re also continuing work on the blockchain converter that generates a testnet blockchain configuration form an existing blocklog. Most recently, we added multithreading support to speed it up and those changes are being tested now. You can follow the work on this task here: https://gitlab.syncad.com/hive/hive/-/commits/tm-blockchain-converter/ # Hivemind (2nd layer applications + social media middleware) ### Reduced peak memory consumption We reviewed and merged in the changes to reduce memory usage of hive sync (just over 4GB footprint now, whereas previous peak usage was approaching 14GB). Finalized changes that were merged in are here: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/527/diffs ### Optimized update_post_rshares We completed and merged in the code that I mentioned last week that creates a temporary index for faster execution of the update_post_rshares function and we were finally able to achieve the same speedup in a full sync of hivemind on one of our production systems after some tweaks to the code (function with temporary index processed 52 million blocks in 19 minutes). To achieve the same results that we saw in our development testing, we found we had to perform a `vacuum analyze` on the `hive_votes` table prior to running the update_post_rshares function to ensure the query planner had the proper statistics to generate an efficient query plan. The merge request for the optimized code is here: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/521 ### Optimized process of effective_comment_vote operation during massive sync When we know that a particular post will be paid out before end of massive sync block processing, we can skip processing of effective_comment_vote_operations for such posts. This optimization reduced the amount of post records that we need to flush to the database during massive sync by more than 50%. The merge request for this optimization is here: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/525 ### Merged in new hive sync option --max-retries We also finished review and testing of the new –max-retries option that allows configuring how many retries (or an indefinite number of retries) before the hive sync process will shutdown if it loses contact with the hived serving blockchain data to it: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/526 # Optimized api.hive.blog servers (BlockTrades-supported API node) While there are number of API nodes available nowadays, many Hive apps default to using our node (which is often useful, since it allows us to quickly spot any scaling issue that might be arising as Hive API traffic increases with time). Here is a graph of how that traffic has increased over the last year: ![image.png](https://images.hive.blog/DQmWoWKQctKxSMTj5YXm7JfmVdZN42c2NCJt28StVKqwLvd/image.png) As you can see, our incoming API traffic (the top part of the graph) has roughly tripled over the year. ### Solved issue with transaction timeouts Yesterday we started getting some reports of timeouts on transactions processed via api.hive.blog. The immediate suspicion, of course, was that this was due to the increased traffic coming from splinterlands servers, which turned out to be the case. But we weren’t seeing much CPU or IO loading on our servers, despite the increased traffic (in fact, we had substantial headroom there and we could likely easily handle 12x or more than the traffic we were receiving based on CPU bottlenecking just with our existing servers), so this lead us to suspect a network-related problem. As a quick fix, we tried making some changes to the network configuration parameters of our servers based on recommendations from @mahdiyari, and we had a report from him that this helped some, but we were still getting reports of a fair number of timeouts, so we decided to do a more thorough analysis of the issue. To properly analyze the network traffic, we first had to make some improvements to the jussi traffic analyzer that we use to analyze loading, because most of the increased traffic was encoding the nature of the request in the post bodies, which means the details of the request weren’t seen by the analyzer tool (e.g. we couldn’t see exactly what types of requests were creating the network problems). With these changes, the jussi analyzer can now distinguish what type of request is being made, allowing us to see which type of requests were slow and/or timing out. After making this change, we found that the requests that were timing out were mostly transaction broadcasts using the old style “pre-appbase” format. Requests of this type can’t be directly processed by a hived node, but we run a jussi gateway on api.hive.blog that converts these legacy-formatted requests into appbase-formatted requests (effectively making our node backwards-compatible with these old style requests). So this was our first clue as to what the real problem was. After investigating how our jussi gateway was configured, we found that the converted requests were being sent to our hived nodes using the websocket protocol, unlike most other requests, which were sent via http. So, on a hunch, we modified the configuration of our jussi process to send translated requests as http requests instead of web socket requests and this eliminated the timeouts (as confirmed by both the jussi analyzer, beacon.peakd.com, and individual script testing by devs). At this point, our node is operating very smoothly, despite signs that traffic has further increased beyond even yesterday’s traffic, so I’m confident we’ve solved all immediate issues. ### Replacement of calls to broadcast_transaction_synchronous with broadcast_transaction Despite handling traffic fine now, I’m also a bit concerned about the use of broadcast_transaction_synchronous calls. These are blocking calls that tend to hold a connection open for 1.5s or more on average (half a block interval) because they wait for the transaction be included in the blockchain before returning. I’ve asked the library devs to look into changing their libraries to begin relying on the newer broadcast_transaction operation (a non-blocking call) and use the transaction_status API call to determine when their transaction has been included into the block, which should eliminate the large number of open connections that can occur when many transactions are being broadcast at once. @mahdiyari has already made changes to hive-js (the Javascript library for hive apps) along these lines (as well as replacing the use of legacy-formatted API calls with appbase formatted API calls) and apps developers are now beginning to test their apps with this beta version of hive-js. Assuming this process goes smoothly, I anticipate that other library devs will swiftly follow suit. # Hive Application Framework (HAF) Some enhancements to HAF have been made to support synchronizing of multiple Hive apps operating on a HAF server. This allows an app that relies on the data of other apps to be sure that those apps have processed all blocks up to the point where the dependent app is currently working at. This enhancement also involved support for secure sharing of data between HAF apps on a HAF server (for example, a dependent app can read, but not write, data in the tables of the app it depends on). As I understand it, we now have a sample application built using HAF, but I haven’t had a chance to review it yet, as I was busy yesterday analyzing and optimizing our web infrastructure with our infrastructure team as discussed above. But this is high on my personal priority list, so I plan to review the work done here soon. # What’s next? We’ve resumed work on the sql_serializer plugin, which is one of the last key pieces needed before we can release HAF. Once those changes are completed, we’ll be able to do an end-to-end test with hived→sql_serializer→hivemind with a full sync of the blockchain. Perhaps I’m optimistic about the resulting speedup, but it’s possible we could have results as early as next Monday. I’m hoping we have some form of HAF ready for early beta testers within a week or two, but bear in mind that is a best case scenario. See: 20th update of 2021 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Hive libraries for Microsoft .NET now fully support broadcasting transactions</title><link href="https://hivedocs.info/news/dotnet/2021/07/26/msnet-libraries-transactions.html" rel="alternate" type="text/html" title="Hive libraries for Microsoft .NET now fully support broadcasting transactions" /><published>2021-07-26T05:56:27-07:00</published><updated>2021-07-26T05:56:27-07:00</updated><id>https://hivedocs.info/news/dotnet/2021/07/26/msnet-libraries-transactions</id><content type="html" xml:base="https://hivedocs.info/news/dotnet/2021/07/26/msnet-libraries-transactions.html">&lt;div id=&quot;content-arcange-msnet-libraries-transactions&quot;&gt;![](https://i.imgur.com/uBDjiIT.png)

Months ago, I introduced the [Hive library for Microsoft .NET  developers](/hive-139531/@arcange/msnet-libraries). This library provides developers used to VB.NET and C# languages with easy-to-use .NET classes, exposing almost all the APIs of the `hived` and `cli_wallet` daemons.

It's also available as a &quot;COM library&quot;  (aka .dll) so people can use it directly in their favorites COM-enabled Windows applications like Word, Excel, ...

### A missing feature

Although the existing library was versatile enough to read information from the blockchain and even record transactions in it, it was however limited to operations that can be sent to the blockchain through the interfaces exposed by the cli_wallet.

Although most of the usual operations are available through that interface, many new operations have been supported by the blockchain but not added to the cli_wallet API (yet).

On the other hand, this way of sending transactions to the blockchain requires that you have a running daemon available at hand, and in case of failure you cannot easily switch to another.

I had wanted for years to have the ability to connect my library directly to API servers that several Witnesses operate (including myself), but so far I have managed to postpone this work. Time to cut it out!

### Taking matters into my own hands

Broadcasting transactions directly to the blockchain means serializing the transaction and signing it before sending it to an RPC node.

Easier said than done because it is an area that was new to me and I know several who have broken their teeth there. Never mind, I like to take up challenges and I often tell myself that nothing is impossible when approached with calm and patience.

Thanks to the Hive developers portal [tutorial about transactions](https://developers.hive.io/tutorials-recipes/how-to-serialize-and-sign-using-js.html), I gradually integrated the understanding of the serialization and cryptography processes required to sign transactions.

As a result, it is no longer necessary to go through cli_wallet to write operations into the blockchain, and **all** available operations can now be broadcasted using the hive.net library.

### Code example

The library is really easy to use. Here is a simple C# code example that broadcast a vote operation to the blockchain:

```
string strPosting = &quot;5...&quot;;   // Posting private key
string strActive = &quot;5...&quot;;    // Active private key

HttpClient oHTTP = new HttpClient();
CHived oHived = new CHived(oHTTP, &quot;https://api.hive.blog&quot;);

COperations.vote oVote = new COperations.vote {
    voter = &quot;arcange&quot;, 
    author = &quot;author&quot;, 
    permlink = &quot;permlink&quot;,
    weight = 100 
    };

try 
{

    string txid = oHived.broadcast_transaction(
        new object[] { oVote }, 
        new string[] { strPosting }
        );

    Console.Write(txid);
}
catch (Exception e)
{
    Console.Write(e.Message);
}
```

Broadcasting multiple operations at once with a single transaction is also very easy:

```
COperations.transfer oTransfer1 = new COperations.transfer { 
    from = &quot;arcange&quot;, 
    to = &quot;account1&quot;, 
    amount = new Asset(&quot;0.001 HIVE&quot;), 
    memo = &quot;A first transfer&quot; 
    };
    
COperations.transfer oTransfer2 = new COperations.transfer { 
    from = &quot;arcange&quot;, 
    to = &quot;account2&quot;, 
    amount = new Asset(&quot;1.000 HBD&quot;), 
    memo = &quot;A second transfer&quot; 
    };
    
try
{
    string txid = oHived.broadcast_transaction(
        new object[] { oTransfer1, oTransfer2 }, 
        new string[] { strActive }
        );
    Console.Write(txid);
}
catch (Exception e)
{
    Console.Write(e.Message);
}
```

Here is another example in VB.NET:

```
Dim strActive = &quot;5....&quot;    ' Posting private key
Dim strPosting = &quot;5....&quot;   ' Active Private key

Dim oHttp As HttpClient = New HttpClient()
Dim oHiveAPI = New CHived(oHttp, &quot;https://api.hive.blog&quot;)

Dim oWitnessUpdate As New COperations.witness_update With {
    .owner = &quot;arcange&quot;,
    .url = &quot;https://....&quot;,
    .block_signing_key = New PublicKey(&quot;STM....&quot;),
    .props = New ChainProperties With {
        .account_creation_fee = New Asset(3, &quot;HIVE&quot;),
        .maximum_block_size = 65536,
        .hbd_interest_rate = 1000
        },
    .fee = New Asset(3, &quot;HIVE&quot;)
    }

Try
    Dim txid = oHiveAPI.broadcast_transaction({oWitnessUpdate}, {strPosting})
    Console.Write(txid)
Catch ex As Exception
    Console.Write(ex.Message)
End Try
```

### What's next?

There is still some work to do:

- add a set of functions to simplify the generation of the most used operations.
- create documentation
- create some tutorials

I plan to tackle these tasks in the next weeks.

### Open source

The libraries are open source and can be found on [GitLab](https://gitlab.syncad.com/hive/hive-net).

If you have any comments or requests, please create an issue on GitLab too. You can also contact me on [Discord](https://discordapp.com/channels/@me/237480770933882881) or [Telegram](https://t.me/the_arcange)

---
&lt;center&gt;

### Check out my apps and services
&lt;a href=&quot;/hive/@hive.engage/stay-connected-with-your-hive-audience-and-catch-attention&quot;&gt;&lt;img src=&quot;https://i.imgur.com/GiNJqlm.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/@hivebuzz&quot;&gt;&lt;img src=&quot;https://i.imgur.com/B4UTun2.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/@hivesql&quot;&gt;&lt;img src=&quot;https://i.imgur.com/EPN8RW6.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/hive/@arcange/introducing-hive-account-recovery&quot;&gt;&lt;img src=&quot;https://i.imgur.com/6TWeW7V.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/@hive.autoclaim&quot;&gt;&lt;img src=&quot;https://i.imgur.com/ih2pEOw.png&quot; /&gt;&lt;/a&gt;&lt;/center&gt;
&lt;center&gt;

### [Vote for me as a witness ![](https://i.imgur.com/2bi4SnT.png)](https://hivesigner.com/sign/account-witness-vote?witness=arcange&amp;amp;approve=1)&amp;lt;/div&amp;gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;
&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@arcange&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/msnet-libraries-transactions&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-arcange-msnet-libraries-transactions').html();
      const outputElem = $('#content-arcange-msnet-libraries-transactions');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;
&lt;style&gt;
  #content-arcange-msnet-libraries-transactions {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-arcange-msnet-libraries-transactions code {
    background: white;
  }
  #content-arcange-msnet-libraries-transactions a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-arcange-msnet-libraries-transactions a:hover {
    border-bottom: 0;
  }
  #content-arcange-msnet-libraries-transactions h1 {
    font-size: 2.2em;
  }
  #content-arcange-msnet-libraries-transactions h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-arcange-msnet-libraries-transactions header small {
    color: #999;
    font-size: 50%;
  }
  #content-arcange-msnet-libraries-transactions img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;
&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/hive-139531/@arcange/msnet-libraries-transactions&quot;&gt;Hive libraries for Microsoft .NET now fully support broadcasting transactions&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@arcange&quot;&gt;@arcange&lt;/a&gt;
&lt;/p&gt;

&lt;/center&gt;&lt;/div&gt;</content><author><name>arcange</name></author><category term="news" /><category term="dotnet" /><summary type="html">![](https://i.imgur.com/uBDjiIT.png) Months ago, I introduced the [Hive library for Microsoft .NET developers](/hive-139531/@arcange/msnet-libraries). This library provides developers used to VB.NET and C# languages with easy-to-use .NET classes, exposing almost all the APIs of the `hived` and `cli_wallet` daemons. It's also available as a &quot;COM library&quot; (aka .dll) so people can use it directly in their favorites COM-enabled Windows applications like Word, Excel, ... ### A missing feature Although the existing library was versatile enough to read information from the blockchain and even record transactions in it, it was however limited to operations that can be sent to the blockchain through the interfaces exposed by the cli_wallet. Although most of the usual operations are available through that interface, many new operations have been supported by the blockchain but not added to the cli_wallet API (yet). On the other hand, this way of sending transactions to the blockchain requires that you have a running daemon available at hand, and in case of failure you cannot easily switch to another. I had wanted for years to have the ability to connect my library directly to API servers that several Witnesses operate (including myself), but so far I have managed to postpone this work. Time to cut it out! ### Taking matters into my own hands Broadcasting transactions directly to the blockchain means serializing the transaction and signing it before sending it to an RPC node. Easier said than done because it is an area that was new to me and I know several who have broken their teeth there. Never mind, I like to take up challenges and I often tell myself that nothing is impossible when approached with calm and patience. Thanks to the Hive developers portal [tutorial about transactions](https://developers.hive.io/tutorials-recipes/how-to-serialize-and-sign-using-js.html), I gradually integrated the understanding of the serialization and cryptography processes required to sign transactions. As a result, it is no longer necessary to go through cli_wallet to write operations into the blockchain, and **all** available operations can now be broadcasted using the hive.net library. ### Code example The library is really easy to use. Here is a simple C# code example that broadcast a vote operation to the blockchain: ``` string strPosting = &quot;5...&quot;; // Posting private key string strActive = &quot;5...&quot;; // Active private key HttpClient oHTTP = new HttpClient(); CHived oHived = new CHived(oHTTP, &quot;https://api.hive.blog&quot;); COperations.vote oVote = new COperations.vote { voter = &quot;arcange&quot;, author = &quot;author&quot;, permlink = &quot;permlink&quot;, weight = 100 }; try { string txid = oHived.broadcast_transaction( new object[] { oVote }, new string[] { strPosting } ); Console.Write(txid); } catch (Exception e) { Console.Write(e.Message); } ``` Broadcasting multiple operations at once with a single transaction is also very easy: ``` COperations.transfer oTransfer1 = new COperations.transfer { from = &quot;arcange&quot;, to = &quot;account1&quot;, amount = new Asset(&quot;0.001 HIVE&quot;), memo = &quot;A first transfer&quot; }; COperations.transfer oTransfer2 = new COperations.transfer { from = &quot;arcange&quot;, to = &quot;account2&quot;, amount = new Asset(&quot;1.000 HBD&quot;), memo = &quot;A second transfer&quot; }; try { string txid = oHived.broadcast_transaction( new object[] { oTransfer1, oTransfer2 }, new string[] { strActive } ); Console.Write(txid); } catch (Exception e) { Console.Write(e.Message); } ``` Here is another example in VB.NET: ``` Dim strActive = &quot;5....&quot; ' Posting private key Dim strPosting = &quot;5....&quot; ' Active Private key Dim oHttp As HttpClient = New HttpClient() Dim oHiveAPI = New CHived(oHttp, &quot;https://api.hive.blog&quot;) Dim oWitnessUpdate As New COperations.witness_update With { .owner = &quot;arcange&quot;, .url = &quot;https://....&quot;, .block_signing_key = New PublicKey(&quot;STM....&quot;), .props = New ChainProperties With { .account_creation_fee = New Asset(3, &quot;HIVE&quot;), .maximum_block_size = 65536, .hbd_interest_rate = 1000 }, .fee = New Asset(3, &quot;HIVE&quot;) } Try Dim txid = oHiveAPI.broadcast_transaction({oWitnessUpdate}, {strPosting}) Console.Write(txid) Catch ex As Exception Console.Write(ex.Message) End Try ``` ### What's next? There is still some work to do: - add a set of functions to simplify the generation of the most used operations. - create documentation - create some tutorials I plan to tackle these tasks in the next weeks. ### Open source The libraries are open source and can be found on [GitLab](https://gitlab.syncad.com/hive/hive-net). If you have any comments or requests, please create an issue on GitLab too. You can also contact me on [Discord](https://discordapp.com/channels/@me/237480770933882881) or [Telegram](https://t.me/the_arcange) --- ### Check out my apps and services ### [Vote for me as a witness ![](https://i.imgur.com/2bi4SnT.png)](https://hivesigner.com/sign/account-witness-vote?witness=arcange&amp;amp;approve=1)&amp;lt;/div&amp;gt; See: Hive libraries for Microsoft .NET now fully support broadcasting transactions by @arcange</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://i.imgur.com/uBDjiIT.png" /><media:content medium="image" url="https://i.imgur.com/uBDjiIT.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">19th update of 2021 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2021/07/20/19th-update-of-2021-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="19th update of 2021 on BlockTrades work on Hive software" /><published>2021-07-20T12:56:42-07:00</published><updated>2021-07-20T12:56:42-07:00</updated><id>https://hivedocs.info/news/core/development/2021/07/20/19th-update-of-2021-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/07/20/19th-update-of-2021-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)

Below is a list of Hive-related programming issues worked on by BlockTrades team during last week or so:

# Hived work (blockchain node software)

### Cli-wallet signing enhancement and bug fix
We’ve enhanced the command-line interface wallet to allow it to sign transactions using account authority. We’re currently testing this. 

We fixed an erroneous error message from cli wallet that could occur when calling `list_my_accounts`: https://gitlab.syncad.com/hive/hive/-/issues/173

### Performance metrics for continuous integration system
We’re also adding performance metrics to our automated build-and-test (CI) system: https://gitlab.syncad.com/hive/tests_api/-/tree/request-execution-time

These changes are also being made for hivemind tests:
https://gitlab.syncad.com/hive/tests_api/-/tree/request-execution-time 

This work is still in progress.

### Fixed final issues associated with account history and last irreversible block
We also completed a few fixes related to account history and the last irreversible block and added some new tests:
https://gitlab.syncad.com/hive/hive/-/merge_requests/275

### Continuing work on blockhain converter tool
We’re also continuing work on the blockchain converter that generates a testnet blockchain configuration form an existing blocklog. Most recently, we added multithreading support to speed it up. You can follow the work on this task here: https://gitlab.syncad.com/hive/hive/-/commits/tm-blockchain-converter/

# Hivemind (2nd layer applications + social media middleware)

### Dramatically reduced memory consumption
We’ve made some progress on hivemind’s memory consumption issue. While it appears the leak issue is gone (probably fixed when we pinned versions of library dependencies used by hivemind), we still saw some higher-than-desirable memory usage during massive sync. We made several changes (queues used by consumer/provider were too long, used prepared queries where possible, explicitly cleared some python containers)  to reduce this number and were able to **reduce hivemind’s peak memory usage from over 13GB to just over 4GB**. Changes are here: https://gitlab.syncad.com/hive/hivemind/-/commits/mt-memory-leak/

Here’s a graph of memory usage before and after the above changes:

![Clipboard - July 19, 2021 2_38 PM.png](https://images.hive.blog/DQmcRro9hkeXG7eRivaE6Jp4hbg2BHbHXbW3fDMMaoxEg11/Clipboard%20-%20July%2019,%202021%202_38%20PM.png)


### Optimized update_post_rshares down from 11.4hrs to ~15m
We’re also continuing work on optimizing the `update_post_rshares` function that is executed after massive sync. Originally this function took 11.4 hours and we’ve reduced it to around 15 minutes by adding an index. Initially this index was pretty large (around 25GB), but we’ve also made other optimizations to reduce database writes wrelated to posts that paid out during massigive sync, and this has not only reduced IO usage, it also reduced the size required by this index. It’s also worth noting that this index can be dropped after update_post_rshares has completed. 

### Made hivemind shutdown time configurable when it loses contact with hived
Hivemind had an annoying habit of shutting down completely if it lost contact with the hived node that it was using to get blockchain data (it retried 25 times, then shutdown). This was problematic because this meant that temporary network disruptions could leave hivemind dead in the water. We’ve added a new option, --max-retries (or –max-allowed-retries, to be determined) which will default to a value of -1 (infinitely retry).

We’re also moving hivemind tests out of the tests-api repo to hivemind repo as part of some general restructuring of the test system.

# Hive Application Framework (HAF)

Our primary developer for the forkresolver code in HAF is back as of yesterday and has resumed work on this project. Our next step is to begin developing some sample applications for HAF. I hope to be able to officially release HAF in about a month. Once we have HAF as a foundation, we can begin building our 2nd layer smart contract system on top of it.

# What’s next?

For the rest of this week, we’ll be focused on testing associated with above tasks.  In the week thereafter, we’ll begin planning what tasks will be scheduled for hardfork 26 as well as other tasks that we plan to complete which can be released sooner (as they don’t require protocol changes).  Of such non-hardfork tasks include development of common-use HAF applications (e.g a HAF application to generate tables about Hive accounts and custom_json).&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/19th-update-of-2021-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/19th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;19th update of 2021 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below is a list of Hive-related programming issues worked on by BlockTrades team during last week or so: # Hived work (blockchain node software) ### Cli-wallet signing enhancement and bug fix We’ve enhanced the command-line interface wallet to allow it to sign transactions using account authority. We’re currently testing this. We fixed an erroneous error message from cli wallet that could occur when calling `list_my_accounts`: https://gitlab.syncad.com/hive/hive/-/issues/173 ### Performance metrics for continuous integration system We’re also adding performance metrics to our automated build-and-test (CI) system: https://gitlab.syncad.com/hive/tests_api/-/tree/request-execution-time These changes are also being made for hivemind tests: https://gitlab.syncad.com/hive/tests_api/-/tree/request-execution-time This work is still in progress. ### Fixed final issues associated with account history and last irreversible block We also completed a few fixes related to account history and the last irreversible block and added some new tests: https://gitlab.syncad.com/hive/hive/-/merge_requests/275 ### Continuing work on blockhain converter tool We’re also continuing work on the blockchain converter that generates a testnet blockchain configuration form an existing blocklog. Most recently, we added multithreading support to speed it up. You can follow the work on this task here: https://gitlab.syncad.com/hive/hive/-/commits/tm-blockchain-converter/ # Hivemind (2nd layer applications + social media middleware) ### Dramatically reduced memory consumption We’ve made some progress on hivemind’s memory consumption issue. While it appears the leak issue is gone (probably fixed when we pinned versions of library dependencies used by hivemind), we still saw some higher-than-desirable memory usage during massive sync. We made several changes (queues used by consumer/provider were too long, used prepared queries where possible, explicitly cleared some python containers) to reduce this number and were able to **reduce hivemind’s peak memory usage from over 13GB to just over 4GB**. Changes are here: https://gitlab.syncad.com/hive/hivemind/-/commits/mt-memory-leak/ Here’s a graph of memory usage before and after the above changes: ![Clipboard - July 19, 2021 2_38 PM.png](https://images.hive.blog/DQmcRro9hkeXG7eRivaE6Jp4hbg2BHbHXbW3fDMMaoxEg11/Clipboard%20-%20July%2019,%202021%202_38%20PM.png) ### Optimized update_post_rshares down from 11.4hrs to ~15m We’re also continuing work on optimizing the `update_post_rshares` function that is executed after massive sync. Originally this function took 11.4 hours and we’ve reduced it to around 15 minutes by adding an index. Initially this index was pretty large (around 25GB), but we’ve also made other optimizations to reduce database writes wrelated to posts that paid out during massigive sync, and this has not only reduced IO usage, it also reduced the size required by this index. It’s also worth noting that this index can be dropped after update_post_rshares has completed. ### Made hivemind shutdown time configurable when it loses contact with hived Hivemind had an annoying habit of shutting down completely if it lost contact with the hived node that it was using to get blockchain data (it retried 25 times, then shutdown). This was problematic because this meant that temporary network disruptions could leave hivemind dead in the water. We’ve added a new option, --max-retries (or –max-allowed-retries, to be determined) which will default to a value of -1 (infinitely retry). We’re also moving hivemind tests out of the tests-api repo to hivemind repo as part of some general restructuring of the test system. # Hive Application Framework (HAF) Our primary developer for the forkresolver code in HAF is back as of yesterday and has resumed work on this project. Our next step is to begin developing some sample applications for HAF. I hope to be able to officially release HAF in about a month. Once we have HAF as a foundation, we can begin building our 2nd layer smart contract system on top of it. # What’s next? For the rest of this week, we’ll be focused on testing associated with above tasks. In the week thereafter, we’ll begin planning what tasks will be scheduled for hardfork 26 as well as other tasks that we plan to complete which can be released sooner (as they don’t require protocol changes). Of such non-hardfork tasks include development of common-use HAF applications (e.g a HAF application to generate tables about Hive accounts and custom_json). See: 19th update of 2021 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Hive core developer meeting #26</title><link href="https://hivedocs.info/news/core/development/2021/07/19/hive-core-developer-meeting-26.html" rel="alternate" type="text/html" title="Hive core developer meeting #26" /><published>2021-07-19T13:21:12-07:00</published><updated>2021-07-19T13:21:12-07:00</updated><id>https://hivedocs.info/news/core/development/2021/07/19/hive-core-developer-meeting-26</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/07/19/hive-core-developer-meeting-26.html">&lt;div id=&quot;content-howo-hive-core-developer-meeting-26&quot;&gt;https://www.youtube.com/watch?v=UF-t2ra9kQI

If you're listening to the whole thing, please post timestamps as a comment, the first one to do so (and correctly) will get a 100% upvote from me :)


meeting points:

# Dev sync:

Listen to that one

# direct rc delegations:

Work is well underway on my end, I basically talk about what I said last week there: https://peakd.com/core/@howo/core-development-report-12-good-progress-on-direct-rc-delegations I fixed the api problem and now am building automated test cases. It's going to take a little while

# VOP for hive -&amp;gt; DHF conversion:

 https://gitlab.syncad.com/hive/hive/-/issues/174

I'll do it. it's a change with little to no impact on performance. It's an issue today because if you send hive to the dhf then there is nothing that's stored in the blockchchain to tell future users how much hbd we got out. And with the rise of hbdstablizer by @smooth, for a lot 

# VOP for proposals creation containing the ID hive

https://gitlab.syncad.com/hive/hive/-/issues/177

It's an issue because if you parse the blockchain you can't get the id of a proposal after it was created unless you actively call an api call, which doesn't scale. Same it's something with little impact on performance that I'll tackle.

# cli_wallet command history persistence (commands are saved when quitting and restarting the program)

Something that I will tackle sooner or later, it's mostly for power users where it's annoying to lose your progress once you close cli_wallet.

# plans for hf26 ? 

I figured we would throw some ideas around and talk about them, like dynamic rc costs, automated actions, NAI etc. There are a bunch of talks in there. Keep in mind nothing is final it's mostly thoughts we had on things we'd like to see for hf26. But hf26 is very far away so everything might change, or be rejected by the community.

I heavily encourage your to listen to that one, but basically there a few ideas thrown around improving hive as a core like peer to peer connectivity (block sync) and the forking logic, dynamic rcs costs (a small post should cost less rc than a big post) etc
As for some of the more &quot;controversial&quot;/ things that people notice changes:

Discussions about accelerated power down time for a fee (@theycallmedan talked about it in length on how we could implement this) 
less than 10 votes a day (maybe 5), basically allow 200% votes. 
Use NAI for assets


Then there was a bunch of questions from @arcange that I encourage you to listen to because we jump from subject to subject. 

Cheers, @howo&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@howo&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/hive-core-developer-meeting-26&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-howo-hive-core-developer-meeting-26').html();
      const outputElem = $('#content-howo-hive-core-developer-meeting-26');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-howo-hive-core-developer-meeting-26 {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-howo-hive-core-developer-meeting-26 code {
    background: white;
  }
  #content-howo-hive-core-developer-meeting-26 a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-howo-hive-core-developer-meeting-26 a:hover {
    border-bottom: 0;
  }
  #content-howo-hive-core-developer-meeting-26 h1 {
    font-size: 2.2em;
  }
  #content-howo-hive-core-developer-meeting-26 h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-howo-hive-core-developer-meeting-26 header small {
    color: #999;
    font-size: 50%;
  }
  #content-howo-hive-core-developer-meeting-26 img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/core/@howo/hive-core-developer-meeting-26&quot;&gt;Hive core developer meeting #26&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@howo&quot;&gt;@howo&lt;/a&gt;
&lt;/p&gt;</content><author><name>howo</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">https://www.youtube.com/watch?v=UF-t2ra9kQI If you're listening to the whole thing, please post timestamps as a comment, the first one to do so (and correctly) will get a 100% upvote from me :) meeting points: # Dev sync: Listen to that one # direct rc delegations: Work is well underway on my end, I basically talk about what I said last week there: https://peakd.com/core/@howo/core-development-report-12-good-progress-on-direct-rc-delegations I fixed the api problem and now am building automated test cases. It's going to take a little while # VOP for hive -&amp;gt; DHF conversion: https://gitlab.syncad.com/hive/hive/-/issues/174 I'll do it. it's a change with little to no impact on performance. It's an issue today because if you send hive to the dhf then there is nothing that's stored in the blockchchain to tell future users how much hbd we got out. And with the rise of hbdstablizer by @smooth, for a lot # VOP for proposals creation containing the ID hive https://gitlab.syncad.com/hive/hive/-/issues/177 It's an issue because if you parse the blockchain you can't get the id of a proposal after it was created unless you actively call an api call, which doesn't scale. Same it's something with little impact on performance that I'll tackle. # cli_wallet command history persistence (commands are saved when quitting and restarting the program) Something that I will tackle sooner or later, it's mostly for power users where it's annoying to lose your progress once you close cli_wallet. # plans for hf26 ? I figured we would throw some ideas around and talk about them, like dynamic rc costs, automated actions, NAI etc. There are a bunch of talks in there. Keep in mind nothing is final it's mostly thoughts we had on things we'd like to see for hf26. But hf26 is very far away so everything might change, or be rejected by the community. I heavily encourage your to listen to that one, but basically there a few ideas thrown around improving hive as a core like peer to peer connectivity (block sync) and the forking logic, dynamic rcs costs (a small post should cost less rc than a big post) etc As for some of the more &quot;controversial&quot;/ things that people notice changes: Discussions about accelerated power down time for a fee (@theycallmedan talked about it in length on how we could implement this) less than 10 votes a day (maybe 5), basically allow 200% votes. Use NAI for assets Then there was a bunch of questions from @arcange that I encourage you to listen to because we jump from subject to subject. Cheers, @howo See: Hive core developer meeting #26 by @howo</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hivedocs.info/assets/images/favicon.png" /><media:content medium="image" url="https://hivedocs.info/assets/images/favicon.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Hive Pressure #3: Catching up with the head block.</title><link href="https://hivedocs.info/howto/devops/node/2021/07/14/hive-pressure-3-catching-up-with-the-head-block.html" rel="alternate" type="text/html" title="Hive Pressure #3: Catching up with the head block." /><published>2021-07-14T13:15:09-07:00</published><updated>2021-07-14T13:15:09-07:00</updated><id>https://hivedocs.info/howto/devops/node/2021/07/14/hive-pressure-3-catching-up-with-the-head-block</id><content type="html" xml:base="https://hivedocs.info/howto/devops/node/2021/07/14/hive-pressure-3-catching-up-with-the-head-block.html">&lt;div id=&quot;content-gtg-hive-pressure-3-catching-up-with-the-head-block&quot;&gt;Basic Hive node has a very simple configuration and with minor changes it can serve as a seed node, a witness node, a broadcaster node, a private node for your wallet (that’s what exchanges are using) or even a simple API node for your Hive microservices.

Regardless of its role, as long as a node has unrestricted network access, it will be part of the Hive p2p network, thus supporting Hive reliability and resilience.

Before your node becomes fully functional, it has to reach the head block of the blockchain.

# Get the Hive daemon
- Build it yourself from these sources:
https://github.com/openhive-network/hive (push mirror)
https://gitlab.syncad.com/hive/hive
- Use docker:
`docker pull hiveio/hive`
- Or get the binary from a trusted source:
https://gtg.openhive.network/get/bin/

# Get the blocks
The easy way or the fast way.

- Sync from the p2p network
By default, when a fresh Hive node starts, it connects to the Hive p2p network and retrieves blocks from it.
See: `--resync-blockchain`

- Get blocks yourself
Hive node can use an existing `block_log` either from another instance or from a public source such as https://gtg.openhive.network/get/blockchain
Our goal is to reach the head block as soon as possible so we chose that way.
`block_log` currently takes over 350GB, so depending on your connection and source, downloading it might take less than an hour or even half a day (for 1Gbps and 100Mbps respectively).
By default it’s expected to be located at `~/.hived/blockchain/block_log`.

# Configure your node
Configuration settings are by default in `~/.hived/config.ini`
This should be enough:
```
plugin = witness
plugin = rc

shared-file-dir = &quot;/run/hive&quot;
shared-file-size = 24G

flush-state-interval = 0
```
Please note that I’m using a custom location for `shared_memory.bin` file, keeping it on a `tmpfs` volume for maximum performance, make sure you have enough space there if you are going to use it.

# Process the blocks
Having all the blocks is not enough, your node needs to be aware of the current state of Hive.
Live nodes get blocks from the p2p network and process them updating state one block at a time (every three seconds), but when you start from scratch, you have to catch up.

- Snapshot
Snapshot is the fastest way because most of the job is already done.
That however will work only for compatible configurations.
We will play with snapshots another time.

- Replay
Once you have a `block_log` and `config.ini` files in place, you need to start `hived` with `--replay-blockchain`.
Replay uses the existing `block_log` to build up the shared memory file up to the highest block stored there, and then it continues with sync, up to the head block.
There's very little use of multi-threading here because every block depends on the previous one.
A lot of data is being processed, so your hardware specs really do matter here.
Not long ago Hive crossed the 55 millions block mark.
Let’s see how long does it take to replay that many blocks using different hardware specs.

`hived --force-replay --set-benchmark-interval 100000`
# Test Setups
### Alpha
&amp;gt; A popular workstation setup. Good enough but will run out of storage soon.
```
Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz
64GB RAM (DDR4 4x16GB 2133MHz)
2x256GB SSD in RAID0 (SAMSUNG MZ7LN256HMJP)
```
### Bravo
&amp;gt; Old but not obsolete. CPU released in 2014. New disks after the old ones failed.
```
Intel(R) Core(TM) i7-4790K CPU @ 4.00GHz
32GB RAM (DDR3 4x8GB 1600MHz)
2x480GB SSD in RAID0 (KINGSTON SEDC500M480G)
```
### Charlie
&amp;gt; The newest and the most expensive CPU in my list. Also the only AMD.
```
AMD Ryzen 5 3600
64GB RAM (DDR4 4x16GB 2666MHz)
2x512GB NVMe in RAID0 (SAMSUNG MZVLB512HBJQ)
```
### Delta
&amp;gt; My favorite, high quality components for serious tasks.
```
Intel(R) Xeon(R) E-2136 CPU @ 3.30GHz
64GB RAM (DDR4 4x16GB 2666MHz ECC)
2x512GB NVMe in RAID0 (WD CL SN720)
```
# Warning: spoilers ahead
What do you think? Which one will win the race?

&lt;center&gt;https://www.youtube.com/watch?v=vlW9lDE3DuI&lt;/center&gt;

# Results

|Server  | [s] |  H:M:S |
|--------|-----|--------|
|Alpha   |28120|7h48m40s|
|Bravo   |26280|7h18m00s|
|Charlie |25032|6h57m12s|
|Delta   |23314|6h28m34s|

What are your `--replay` times?&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@gtg&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/hive-pressure-3-catching-up-with-the-head-block&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-gtg-hive-pressure-3-catching-up-with-the-head-block').html();
      const outputElem = $('#content-gtg-hive-pressure-3-catching-up-with-the-head-block');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-gtg-hive-pressure-3-catching-up-with-the-head-block {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-gtg-hive-pressure-3-catching-up-with-the-head-block code {
    background: white;
  }
  #content-gtg-hive-pressure-3-catching-up-with-the-head-block a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-gtg-hive-pressure-3-catching-up-with-the-head-block a:hover {
    border-bottom: 0;
  }
  #content-gtg-hive-pressure-3-catching-up-with-the-head-block h1 {
    font-size: 2.2em;
  }
  #content-gtg-hive-pressure-3-catching-up-with-the-head-block h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-gtg-hive-pressure-3-catching-up-with-the-head-block header small {
    color: #999;
    font-size: 50%;
  }
  #content-gtg-hive-pressure-3-catching-up-with-the-head-block img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-160391/@gtg/hive-pressure-3-catching-up-with-the-head-block&quot;&gt;Hive Pressure #3: Catching up with the head block.&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@gtg&quot;&gt;@gtg&lt;/a&gt;
&lt;/p&gt;</content><author><name>gtg</name></author><category term="howto" /><category term="devops" /><category term="node" /><summary type="html">Basic Hive node has a very simple configuration and with minor changes it can serve as a seed node, a witness node, a broadcaster node, a private node for your wallet (that’s what exchanges are using) or even a simple API node for your Hive microservices. Regardless of its role, as long as a node has unrestricted network access, it will be part of the Hive p2p network, thus supporting Hive reliability and resilience. Before your node becomes fully functional, it has to reach the head block of the blockchain. # Get the Hive daemon - Build it yourself from these sources: https://github.com/openhive-network/hive (push mirror) https://gitlab.syncad.com/hive/hive - Use docker: `docker pull hiveio/hive` - Or get the binary from a trusted source: https://gtg.openhive.network/get/bin/ # Get the blocks The easy way or the fast way. - Sync from the p2p network By default, when a fresh Hive node starts, it connects to the Hive p2p network and retrieves blocks from it. See: `--resync-blockchain` - Get blocks yourself Hive node can use an existing `block_log` either from another instance or from a public source such as https://gtg.openhive.network/get/blockchain Our goal is to reach the head block as soon as possible so we chose that way. `block_log` currently takes over 350GB, so depending on your connection and source, downloading it might take less than an hour or even half a day (for 1Gbps and 100Mbps respectively). By default it’s expected to be located at `~/.hived/blockchain/block_log`. # Configure your node Configuration settings are by default in `~/.hived/config.ini` This should be enough: ``` plugin = witness plugin = rc shared-file-dir = &quot;/run/hive&quot; shared-file-size = 24G flush-state-interval = 0 ``` Please note that I’m using a custom location for `shared_memory.bin` file, keeping it on a `tmpfs` volume for maximum performance, make sure you have enough space there if you are going to use it. # Process the blocks Having all the blocks is not enough, your node needs to be aware of the current state of Hive. Live nodes get blocks from the p2p network and process them updating state one block at a time (every three seconds), but when you start from scratch, you have to catch up. - Snapshot Snapshot is the fastest way because most of the job is already done. That however will work only for compatible configurations. We will play with snapshots another time. - Replay Once you have a `block_log` and `config.ini` files in place, you need to start `hived` with `--replay-blockchain`. Replay uses the existing `block_log` to build up the shared memory file up to the highest block stored there, and then it continues with sync, up to the head block. There's very little use of multi-threading here because every block depends on the previous one. A lot of data is being processed, so your hardware specs really do matter here. Not long ago Hive crossed the 55 millions block mark. Let’s see how long does it take to replay that many blocks using different hardware specs. `hived --force-replay --set-benchmark-interval 100000` # Test Setups ### Alpha &amp;gt; A popular workstation setup. Good enough but will run out of storage soon. ``` Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz 64GB RAM (DDR4 4x16GB 2133MHz) 2x256GB SSD in RAID0 (SAMSUNG MZ7LN256HMJP) ``` ### Bravo &amp;gt; Old but not obsolete. CPU released in 2014. New disks after the old ones failed. ``` Intel(R) Core(TM) i7-4790K CPU @ 4.00GHz 32GB RAM (DDR3 4x8GB 1600MHz) 2x480GB SSD in RAID0 (KINGSTON SEDC500M480G) ``` ### Charlie &amp;gt; The newest and the most expensive CPU in my list. Also the only AMD. ``` AMD Ryzen 5 3600 64GB RAM (DDR4 4x16GB 2666MHz) 2x512GB NVMe in RAID0 (SAMSUNG MZVLB512HBJQ) ``` ### Delta &amp;gt; My favorite, high quality components for serious tasks. ``` Intel(R) Xeon(R) E-2136 CPU @ 3.30GHz 64GB RAM (DDR4 4x16GB 2666MHz ECC) 2x512GB NVMe in RAID0 (WD CL SN720) ``` # Warning: spoilers ahead What do you think? Which one will win the race? https://www.youtube.com/watch?v=vlW9lDE3DuI # Results |Server | [s] | H:M:S | |--------|-----|--------| |Alpha |28120|7h48m40s| |Bravo |26280|7h18m00s| |Charlie |25032|6h57m12s| |Delta |23314|6h28m34s| What are your `--replay` times? See: Hive Pressure #3: Catching up with the head block. by @gtg</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://img.youtube.com/vi/vlW9lDE3DuI/0.jpg" /><media:content medium="image" url="https://img.youtube.com/vi/vlW9lDE3DuI/0.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Core development report #12: Good progress on direct rc delegations</title><link href="https://hivedocs.info/news/core/development/2021/07/12/core-development-report-12-good-progress-on-direct-rc-delegations.html" rel="alternate" type="text/html" title="Core development report #12: Good progress on direct rc delegations" /><published>2021-07-12T13:30:15-07:00</published><updated>2021-07-12T13:30:15-07:00</updated><id>https://hivedocs.info/news/core/development/2021/07/12/core-development-report-12-good-progress-on-direct-rc-delegations</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/07/12/core-development-report-12-good-progress-on-direct-rc-delegations.html">&lt;div id=&quot;content-howo-core-development-report-12-good-progress-on-direct-rc-delegations&quot;&gt;
![image.png](https://files.peakd.com/file/peakd-hive/howo/AJeaeqp3EAX2QSK997FwxcZdJuACo817654t7riwiGf3aSNSLCLRwP5yGvpwrcb.png)

Hey,

I usually do my reports via voice during the hive developers meeting, but due to some vacations here and there and rush with hard fork 25, we decided to postpone it a few times so I figured I'd make an update about my progress via text.

Nowadays I am working on direct rc delegations, if you didn't follow, the initial implementation was judged too complex and there was a fear that this complexity could hide bug in the edge cases. Plus it makes it less user friendly. So I made a new specification here https://gitlab.syncad.com/hive/hive/-/issues/152.

And now I'm implementing it, work is in progress on that branch https://gitlab.syncad.com/hive/hive/-/tree/feature/direct_rc_delegations

The core core is done, you can delegate rc, update an existing delegation, delete a delegation and rc delegations are taken into account when calculating rc. I have also added some api endpoint so that when you call find_rc_account and list_rc_account the amount of RC received/delegated is reflected there.

I also added an endpoint to fetch direct rc delegations but I'm having some issues with the indexes (it induces a uniqueness constraint in the blockchain which is a problem because then if you want to delegate to one person and then another person you end up with a conflict because &quot;from&quot; is unique).

I've also included everything above in the cli_wallet for easy use. I'll update hive-js later on too so that most dapps will be able to play with it.

Apart from that I need to handle the edge cases where you lose RC (you delegated all your rc but then you power downs so you can no longer sustain those delegations) and write a mountain of automated tests.

Once that's done, I'll go through a bunch of reviews with @blocktrades' team (which I'm assuming won't happen until a few weeks as some of the core team is in vacations). Then we'll launch a testnet (or update the ongoing testnet, that part is unclear) so that some of the power users/ dapp users can test it out.

@howo&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@howo&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/core-development-report-12-good-progress-on-direct-rc-delegations&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-howo-core-development-report-12-good-progress-on-direct-rc-delegations').html();
      const outputElem = $('#content-howo-core-development-report-12-good-progress-on-direct-rc-delegations');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-howo-core-development-report-12-good-progress-on-direct-rc-delegations {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-howo-core-development-report-12-good-progress-on-direct-rc-delegations code {
    background: white;
  }
  #content-howo-core-development-report-12-good-progress-on-direct-rc-delegations a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-howo-core-development-report-12-good-progress-on-direct-rc-delegations a:hover {
    border-bottom: 0;
  }
  #content-howo-core-development-report-12-good-progress-on-direct-rc-delegations h1 {
    font-size: 2.2em;
  }
  #content-howo-core-development-report-12-good-progress-on-direct-rc-delegations h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-howo-core-development-report-12-good-progress-on-direct-rc-delegations header small {
    color: #999;
    font-size: 50%;
  }
  #content-howo-core-development-report-12-good-progress-on-direct-rc-delegations img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/core/@howo/core-development-report-12-good-progress-on-direct-rc-delegations&quot;&gt;Core development report #12: Good progress on direct rc delegations&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@howo&quot;&gt;@howo&lt;/a&gt;
&lt;/p&gt;</content><author><name>howo</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![image.png](https://files.peakd.com/file/peakd-hive/howo/AJeaeqp3EAX2QSK997FwxcZdJuACo817654t7riwiGf3aSNSLCLRwP5yGvpwrcb.png) Hey, I usually do my reports via voice during the hive developers meeting, but due to some vacations here and there and rush with hard fork 25, we decided to postpone it a few times so I figured I'd make an update about my progress via text. Nowadays I am working on direct rc delegations, if you didn't follow, the initial implementation was judged too complex and there was a fear that this complexity could hide bug in the edge cases. Plus it makes it less user friendly. So I made a new specification here https://gitlab.syncad.com/hive/hive/-/issues/152. And now I'm implementing it, work is in progress on that branch https://gitlab.syncad.com/hive/hive/-/tree/feature/direct_rc_delegations The core core is done, you can delegate rc, update an existing delegation, delete a delegation and rc delegations are taken into account when calculating rc. I have also added some api endpoint so that when you call find_rc_account and list_rc_account the amount of RC received/delegated is reflected there. I also added an endpoint to fetch direct rc delegations but I'm having some issues with the indexes (it induces a uniqueness constraint in the blockchain which is a problem because then if you want to delegate to one person and then another person you end up with a conflict because &quot;from&quot; is unique). I've also included everything above in the cli_wallet for easy use. I'll update hive-js later on too so that most dapps will be able to play with it. Apart from that I need to handle the edge cases where you lose RC (you delegated all your rc but then you power downs so you can no longer sustain those delegations) and write a mountain of automated tests. Once that's done, I'll go through a bunch of reviews with @blocktrades' team (which I'm assuming won't happen until a few weeks as some of the core team is in vacations). Then we'll launch a testnet (or update the ongoing testnet, that part is unclear) so that some of the power users/ dapp users can test it out. @howo See: Core development report #12: Good progress on direct rc delegations by @howo</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://files.peakd.com/file/peakd-hive/howo/AJeaeqp3EAX2QSK997FwxcZdJuACo817654t7riwiGf3aSNSLCLRwP5yGvpwrcb.png" /><media:content medium="image" url="https://files.peakd.com/file/peakd-hive/howo/AJeaeqp3EAX2QSK997FwxcZdJuACo817654t7riwiGf3aSNSLCLRwP5yGvpwrcb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">18th update of 2021 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2021/07/09/18th-update-of-2021-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="18th update of 2021 on BlockTrades work on Hive software" /><published>2021-07-09T14:26:30-07:00</published><updated>2021-07-09T14:26:30-07:00</updated><id>https://hivedocs.info/news/core/development/2021/07/09/18th-update-of-2021-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/07/09/18th-update-of-2021-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)

Below is a list of Hive-related programming issues worked on by BlockTrades team since my last report. 

# Hive network upgraded to Equilibrium
The most significant accomplishment was successful upgrade of Hive via hardfork 25 (the Equilibrium release), and a lot of time was spent monitoring the upgrade and supporting Hive apps as needed during the transition.

The Equilibrium upgrade did have one unanticipated side-effect: the change in curation rules resulted in votes cast after the hardfork being much stronger than votes cast before the hardfork with respect to curation weight, which meant that votes cast in the days before the hardfork generally didn’t receive much in the way of curation rewards. This was a temporary effect that has now been resolved since all posts being actively voted on now were created after the hardfork, but hopefully we’ll have enough traffic on our next iteration of the testnet that we’ll be able to detect such issues ahead of time.

# Hived work (blockchain node software)

Improvements to Testtools and tests used to verify hived functionality: https://gitlab.syncad.com/hive/hive/-/merge_requests/272

Added new –exit-before-sync flag to hived’s command-line interface (useful for dumping a snapshot without then syncing, see https://gitlab.syncad.com/hive/hive/-/issues/66 for more details on why this option was added):
https://gitlab.syncad.com/hive/hive/-/merge_requests/232
https://gitlab.syncad.com/hive/hive/-/merge_requests/273

We fixed the previously reported bug that requires a hived to be restarted after loading a snapshot:
https://gitlab.syncad.com/hive/hive/-/merge_requests/274

We have been analyzing the performance of our new “blockchain converter” tool for creating testnets quickly that mirror the mainnet and we’ve identified some code associated with nonces as the potential bottleneck.

# Hivemind (social media middleware)

We’ve added a new programmer to our hivemind team and he’ll initially be working on testing and minor bug fixes as a means of learning the code base. His first merge request is here:
https://gitlab.syncad.com/hive/hivemind/-/merge_requests/518

We added code for checking consistency of the hive_blocks table (this is part of previously mentioned plan to ensure robust operation in the case where hivemind’s postgres process shuts down suddenly or a rollback fails):
https://gitlab.syncad.com/hive/hivemind/-/merge_requests/516


We’re continuing work on improving performance of the update_rshares function immediately after massive sync of a hivemind instance. We’re trying two different alternatives to improve overall performance: 1) changing massive sync so that it updates rshares for paid posts on-the-fly, reducing the work of update_rshares to only updating rshares for unpaid posts (this approach requires introducing a new hived virtual_operation) and 2) adding an index (at least temporarily just after massive sync) to speed up update_rshares. Both approaches are currently undergoing testing.

We also resumed research into the reason why some hivemind nodes consume more memory than others. It has been suggested that it may be related to differences in python or python-library installations on the different systems, which I’m inclined to believe at this point, as we’re no longer seeing unexpected memory consumption on any of our production nodes running the latest official hivemind version. So if we’re unable to replicate this issue in our forthcoming tests, we’ll likely drop this issue soon, after merging in our diagnostic changes that identify sources of memory usage better.

# Hive Application Framework (HAF)

Our primary dev for this work is currently on vacation. 

I had hoped we would still be able to work on the psql_serializer plugin (which feeds data from hived to hivemind under the HAF system) in the meantime, but the dev tasked with that was tied up with other issues (e.g. fix of snapshot problem). A new dev has been assigned to work on psql_serializer this week (the previously tasked one is going on vacation for two weeks).

# Condenser and wallet (open-source code base for https://hive.blog and other similar frontends)

We reviewed and merged in a number of community-contributed upgrades to condenser and its wallet.

From @quochuy: https://gitlab.syncad.com/hive/wallet/-/merge_requests/102

From @guiltyparties: 
https://gitlab.syncad.com/hive/wallet/-/merge_requests/101
https://gitlab.syncad.com/hive/condenser/-/merge_requests/268

From @eonwarped: https://gitlab.syncad.com/hive/condenser/-/merge_requests/269


# What’s next?

Several of our Hive devs are either on vacation now or going on vacation this coming week (they had been delaying their vacations to be available for the hardfork and any potential problems that might arise afterwards). So we’ll only have 8 BlockTrades devs working on Hive for the next two weeks, and our progress will inevitably slow some during this time. 

After all our Hive devs return from vacation, we’ll take a couple of weeks to begin planning what work to schedule for the next hardfork (HF26). I have some preliminary ideas for improvements that our team will work on, but we’ll make a full list of proposed changes, then begin to prioritize what we want to fit on the roadmap for HF26. My plan at this time is to stick to our existing “upgrade Hive protocol every six months” schedule if possible.

Also, as during previous hardforks, our roadmaps aren’t fixed in stone, so we may consider making other proposed changes even after the initial roadmap is published, assuming the changes aren’t too big.

Note that the above process doesn’t mean we don’t have clear development goals prior to the completion of the HF26 roadmap. For one thing, we will be making performance upgrades to hived that don’t require an actual hardfork, and these changes will generally be released as they are completed.

Even more importantly, at this point our highest priority tasks revolve around the creation of the HAF framework and HAF-based applications, and this is all layer 2 work that doesn’t require any Hive protocol changes that would necessitate a hardfork. In other words, we can also release HAF and associated apps to the dev community as soon as they are ready, without the labor and scheduling issues involved in getting nodes to upgrade as part of a hardfork.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/18th-update-of-2021-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/18th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;18th update of 2021 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below is a list of Hive-related programming issues worked on by BlockTrades team since my last report. # Hive network upgraded to Equilibrium The most significant accomplishment was successful upgrade of Hive via hardfork 25 (the Equilibrium release), and a lot of time was spent monitoring the upgrade and supporting Hive apps as needed during the transition. The Equilibrium upgrade did have one unanticipated side-effect: the change in curation rules resulted in votes cast after the hardfork being much stronger than votes cast before the hardfork with respect to curation weight, which meant that votes cast in the days before the hardfork generally didn’t receive much in the way of curation rewards. This was a temporary effect that has now been resolved since all posts being actively voted on now were created after the hardfork, but hopefully we’ll have enough traffic on our next iteration of the testnet that we’ll be able to detect such issues ahead of time. # Hived work (blockchain node software) Improvements to Testtools and tests used to verify hived functionality: https://gitlab.syncad.com/hive/hive/-/merge_requests/272 Added new –exit-before-sync flag to hived’s command-line interface (useful for dumping a snapshot without then syncing, see https://gitlab.syncad.com/hive/hive/-/issues/66 for more details on why this option was added): https://gitlab.syncad.com/hive/hive/-/merge_requests/232 https://gitlab.syncad.com/hive/hive/-/merge_requests/273 We fixed the previously reported bug that requires a hived to be restarted after loading a snapshot: https://gitlab.syncad.com/hive/hive/-/merge_requests/274 We have been analyzing the performance of our new “blockchain converter” tool for creating testnets quickly that mirror the mainnet and we’ve identified some code associated with nonces as the potential bottleneck. # Hivemind (social media middleware) We’ve added a new programmer to our hivemind team and he’ll initially be working on testing and minor bug fixes as a means of learning the code base. His first merge request is here: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/518 We added code for checking consistency of the hive_blocks table (this is part of previously mentioned plan to ensure robust operation in the case where hivemind’s postgres process shuts down suddenly or a rollback fails): https://gitlab.syncad.com/hive/hivemind/-/merge_requests/516 We’re continuing work on improving performance of the update_rshares function immediately after massive sync of a hivemind instance. We’re trying two different alternatives to improve overall performance: 1) changing massive sync so that it updates rshares for paid posts on-the-fly, reducing the work of update_rshares to only updating rshares for unpaid posts (this approach requires introducing a new hived virtual_operation) and 2) adding an index (at least temporarily just after massive sync) to speed up update_rshares. Both approaches are currently undergoing testing. We also resumed research into the reason why some hivemind nodes consume more memory than others. It has been suggested that it may be related to differences in python or python-library installations on the different systems, which I’m inclined to believe at this point, as we’re no longer seeing unexpected memory consumption on any of our production nodes running the latest official hivemind version. So if we’re unable to replicate this issue in our forthcoming tests, we’ll likely drop this issue soon, after merging in our diagnostic changes that identify sources of memory usage better. # Hive Application Framework (HAF) Our primary dev for this work is currently on vacation. I had hoped we would still be able to work on the psql_serializer plugin (which feeds data from hived to hivemind under the HAF system) in the meantime, but the dev tasked with that was tied up with other issues (e.g. fix of snapshot problem). A new dev has been assigned to work on psql_serializer this week (the previously tasked one is going on vacation for two weeks). # Condenser and wallet (open-source code base for https://hive.blog and other similar frontends) We reviewed and merged in a number of community-contributed upgrades to condenser and its wallet. From @quochuy: https://gitlab.syncad.com/hive/wallet/-/merge_requests/102 From @guiltyparties: https://gitlab.syncad.com/hive/wallet/-/merge_requests/101 https://gitlab.syncad.com/hive/condenser/-/merge_requests/268 From @eonwarped: https://gitlab.syncad.com/hive/condenser/-/merge_requests/269 # What’s next? Several of our Hive devs are either on vacation now or going on vacation this coming week (they had been delaying their vacations to be available for the hardfork and any potential problems that might arise afterwards). So we’ll only have 8 BlockTrades devs working on Hive for the next two weeks, and our progress will inevitably slow some during this time. After all our Hive devs return from vacation, we’ll take a couple of weeks to begin planning what work to schedule for the next hardfork (HF26). I have some preliminary ideas for improvements that our team will work on, but we’ll make a full list of proposed changes, then begin to prioritize what we want to fit on the roadmap for HF26. My plan at this time is to stick to our existing “upgrade Hive protocol every six months” schedule if possible. Also, as during previous hardforks, our roadmaps aren’t fixed in stone, so we may consider making other proposed changes even after the initial roadmap is published, assuming the changes aren’t too big. Note that the above process doesn’t mean we don’t have clear development goals prior to the completion of the HF26 roadmap. For one thing, we will be making performance upgrades to hived that don’t require an actual hardfork, and these changes will generally be released as they are completed. Even more importantly, at this point our highest priority tasks revolve around the creation of the HAF framework and HAF-based applications, and this is all layer 2 work that doesn’t require any Hive protocol changes that would necessitate a hardfork. In other words, we can also release HAF and associated apps to the dev community as soon as they are ready, without the labor and scheduling issues involved in getting nodes to upgrade as part of a hardfork. See: 18th update of 2021 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>