<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://hivedocs.info/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hivedocs.info/" rel="alternate" type="text/html" /><updated>2021-09-08T13:30:09-07:00</updated><id>https://hivedocs.info/feed.xml</id><title type="html">Hive Chain Documentation</title><subtitle>Your resource for various levels of Hive Documentation.</subtitle><author><name>site curated by: @inertia</name></author><entry><title type="html">22nd update of 2021 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2021/09/07/22nd-update-of-2021-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="22nd update of 2021 on BlockTrades work on Hive software" /><published>2021-09-07T14:31:30-07:00</published><updated>2021-09-07T14:31:30-07:00</updated><id>https://hivedocs.info/news/core/development/2021/09/07/22nd-update-of-2021-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/09/07/22nd-update-of-2021-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-22nd-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)
Below is a list of Hive-related programming issues worked on by BlockTrades team during last week or so:

# Hived work (blockchain node software)

### Further optimization of sql_serializer plugin

The sql_serializer plugin is responsible for reading blockchain data and pushing that data to a HAF server’s Postgres database. As a practical matter, this means the speed of this plugin sets an upper limit on how fast a HAF app can operate (especially during a replay of a hived node), so it is important that this process is as fast as possible. 

After [our latest optimizations (and bug fixes)](https://gitlab.syncad.com/hive/hive/-/merge_requests/257), we’ve brought the time required to do a full replay with the sql_serializer plugin down to 5.5 hours (previously about 7 hours, so about 17% speedup). Note that both the old and the new benchmarks were performed on one of our fastest systems (a Ryzen 9 5950X with 4 NVME drives in a raid0 configuration).

The latest changes for the sql_serializer are now merged into the develop branch.

### Cleanup display of results for several hive CLI wallet commands

The hive CLI wallet is a command-line interface for creating hive transactions. It is mostly used by Hive apps, exchanges, and Hive “power users”.

The [most recent improvements](https://gitlab.syncad.com/hive/hive/-/merge_requests/252) include minor fixes to the display of wallet command results, such as case consistency, indentation for tables, headers and other misc. display fixes for `get_withdraw_routes`, `get_open_orders`, `get_order_book`, etc. There were also various improvements and fixes to CLI wallet internal docs.

### Miscellaneous changes

We eliminated some false errors generated by GCC 9.3.0 linter and fixed a Cmake-related issue (compilation of the sql_serializer plugin requires Postgres dev libraries as a dependency):
https://gitlab.syncad.com/hive/hive/-/merge_requests/254
https://gitlab.syncad.com/hive/hive/-/merge_requests/255


# Hivemind (2nd layer applications + social media middleware)

The most recent hivemind work was focused on optimization of the hivemind indexer (the process that builds social data from the raw blockchain data).

### Reduced peak database disk usage (20% reduction in peak storage requirement)

We noticed that postgres database size temporarily grew to around 700GB during the massive sync phase (this is when a hivemind server is first being initialized with old blockchain data). We determined that this temporary increase occurred during record cleanup resulting from the reputation calculation process, and by changing from using DELETE calls to `DROP TABLE` and `TRUNCATE` calls we were able to eliminate this temporary peak usage requirement, resulting in a 20% reduction in peak storage requirements (approximate 141GB less storage used at current headblock). We’re also hoping this will lead to further speedup in the post-massive sync initialization of table indexes, but we haven’t had a chance to benchmark that yet. https://gitlab.syncad.com/hive/hivemind/-/merge_requests/509

### Elimination of post’s `active` property speeded up hivemind sync by 20%

We discovered that none of the Hive apps used the `active` field for posts (this field indicated that a post was still actively competing for rewards from the reward pool) so we removed the field from the database schema and eliminated it from API responses. This turned out to be surprisingly beneficial to hivemind’s full sync time, at least according to our latest benchmarks: we completed a full sync in 10.5 hours, approximately 20% faster than our previous time on the same system. https://gitlab.syncad.com/hive/hivemind/-/merge_requests/511

## Completed optimization of update_post_rshares for Postgres 12

We’re planning to move to Ubuntu 20 (from Ubuntu 18) as the recommended Hive development platform soon, and this also entails a move to Postgres 12 (from Postgres 10) because that’s the default version of Postgres that ships with Ubuntu 20. So we’re working thru performance regressions associated with differences in the way Postgres 12 query planner works. Most recently we changed the query for `update_post_rshares` to fix a performance killer and changed when we executed some vacuum analyze calls:
https://gitlab.syncad.com/hive/hivemind/-/merge_requests/510
https://gitlab.syncad.com/hive/hivemind/-/merge_requests/512

# Condenser (open-source codebase for hive.blog, etc)

We tested and deployed various fixes by @quochuy to https://hive.blog:
https://gitlab.syncad.com/hive/condenser/-/merge_requests/259
https://gitlab.syncad.com/hive/condenser/-/merge_requests/260
https://gitlab.syncad.com/hive/condenser/-/merge_requests/261
https://gitlab.syncad.com/hive/condenser/-/merge_requests/262

# Hive Image Server

We’re in the process of doing a major upgrade to the hardware that runs the image server for Hive. The new system has a LOT more disk space, faster drives, more CPUs, more memory, etc. Currently we’re in the process of moving over the huge amount of images to the new server.

# Hive Application Framework: framework for building robust and scalable Hive apps

### Optimizing HAF-based account history app

We’re currently optimizing and testing our first HAF-based app (codenamed Hafah) that emulates the functionality of hived’s account history plugin (and ultimately will replace it). Our initial benchmarks at 5M blocks had good performance, but we’ve seen a slowdown in indexing performance when operating with a fully populated database (i.e. 50M blocks) so we’re working now to optimize the speed of the queries. We’re also preparing benchmarks for the speed of the API calls.

### Database diff tool for testing HAF-based apps

We're developing a new testing system to test the port of hivemind to a HAF-based application: essentially it is a &quot;database diff&quot; tool that will allow us to detect differences (via SQL statements) between the tables created by the old hivemind app and the upcoming HAF-based hivemind.

# Upcoming work

On the hived side, we’ll be adding support to the sql_serializer for directly injecting “impacted account” data. After that, we can compare the relative performance of this method of inserting the data into postgres versus using the C-based postgres extension for computing this data, in order to make a decision about the best design alternative. We’ll also likely merge in the first hardfork-related changes that 1) allow more than one vote per three seconds by an account and 2) don’t kill curation rewards entirely when someone edits their vote strength on a post.

Our HAF work will continue to focus on our first example HAF apps (account history app and HAF-based hivemind implementation).&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/22nd-update-of-2021-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-22nd-update-of-2021-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-22nd-update-of-2021-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-22nd-update-of-2021-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-22nd-update-of-2021-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-22nd-update-of-2021-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-22nd-update-of-2021-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-22nd-update-of-2021-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-22nd-update-of-2021-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-22nd-update-of-2021-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-22nd-update-of-2021-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/22nd-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;22nd update of 2021 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below is a list of Hive-related programming issues worked on by BlockTrades team during last week or so: # Hived work (blockchain node software) ### Further optimization of sql_serializer plugin The sql_serializer plugin is responsible for reading blockchain data and pushing that data to a HAF server’s Postgres database. As a practical matter, this means the speed of this plugin sets an upper limit on how fast a HAF app can operate (especially during a replay of a hived node), so it is important that this process is as fast as possible. After [our latest optimizations (and bug fixes)](https://gitlab.syncad.com/hive/hive/-/merge_requests/257), we’ve brought the time required to do a full replay with the sql_serializer plugin down to 5.5 hours (previously about 7 hours, so about 17% speedup). Note that both the old and the new benchmarks were performed on one of our fastest systems (a Ryzen 9 5950X with 4 NVME drives in a raid0 configuration). The latest changes for the sql_serializer are now merged into the develop branch. ### Cleanup display of results for several hive CLI wallet commands The hive CLI wallet is a command-line interface for creating hive transactions. It is mostly used by Hive apps, exchanges, and Hive “power users”. The [most recent improvements](https://gitlab.syncad.com/hive/hive/-/merge_requests/252) include minor fixes to the display of wallet command results, such as case consistency, indentation for tables, headers and other misc. display fixes for `get_withdraw_routes`, `get_open_orders`, `get_order_book`, etc. There were also various improvements and fixes to CLI wallet internal docs. ### Miscellaneous changes We eliminated some false errors generated by GCC 9.3.0 linter and fixed a Cmake-related issue (compilation of the sql_serializer plugin requires Postgres dev libraries as a dependency): https://gitlab.syncad.com/hive/hive/-/merge_requests/254 https://gitlab.syncad.com/hive/hive/-/merge_requests/255 # Hivemind (2nd layer applications + social media middleware) The most recent hivemind work was focused on optimization of the hivemind indexer (the process that builds social data from the raw blockchain data). ### Reduced peak database disk usage (20% reduction in peak storage requirement) We noticed that postgres database size temporarily grew to around 700GB during the massive sync phase (this is when a hivemind server is first being initialized with old blockchain data). We determined that this temporary increase occurred during record cleanup resulting from the reputation calculation process, and by changing from using DELETE calls to `DROP TABLE` and `TRUNCATE` calls we were able to eliminate this temporary peak usage requirement, resulting in a 20% reduction in peak storage requirements (approximate 141GB less storage used at current headblock). We’re also hoping this will lead to further speedup in the post-massive sync initialization of table indexes, but we haven’t had a chance to benchmark that yet. https://gitlab.syncad.com/hive/hivemind/-/merge_requests/509 ### Elimination of post’s `active` property speeded up hivemind sync by 20% We discovered that none of the Hive apps used the `active` field for posts (this field indicated that a post was still actively competing for rewards from the reward pool) so we removed the field from the database schema and eliminated it from API responses. This turned out to be surprisingly beneficial to hivemind’s full sync time, at least according to our latest benchmarks: we completed a full sync in 10.5 hours, approximately 20% faster than our previous time on the same system. https://gitlab.syncad.com/hive/hivemind/-/merge_requests/511 ## Completed optimization of update_post_rshares for Postgres 12 We’re planning to move to Ubuntu 20 (from Ubuntu 18) as the recommended Hive development platform soon, and this also entails a move to Postgres 12 (from Postgres 10) because that’s the default version of Postgres that ships with Ubuntu 20. So we’re working thru performance regressions associated with differences in the way Postgres 12 query planner works. Most recently we changed the query for `update_post_rshares` to fix a performance killer and changed when we executed some vacuum analyze calls: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/510 https://gitlab.syncad.com/hive/hivemind/-/merge_requests/512 # Condenser (open-source codebase for hive.blog, etc) We tested and deployed various fixes by @quochuy to https://hive.blog: https://gitlab.syncad.com/hive/condenser/-/merge_requests/259 https://gitlab.syncad.com/hive/condenser/-/merge_requests/260 https://gitlab.syncad.com/hive/condenser/-/merge_requests/261 https://gitlab.syncad.com/hive/condenser/-/merge_requests/262 # Hive Image Server We’re in the process of doing a major upgrade to the hardware that runs the image server for Hive. The new system has a LOT more disk space, faster drives, more CPUs, more memory, etc. Currently we’re in the process of moving over the huge amount of images to the new server. # Hive Application Framework: framework for building robust and scalable Hive apps ### Optimizing HAF-based account history app We’re currently optimizing and testing our first HAF-based app (codenamed Hafah) that emulates the functionality of hived’s account history plugin (and ultimately will replace it). Our initial benchmarks at 5M blocks had good performance, but we’ve seen a slowdown in indexing performance when operating with a fully populated database (i.e. 50M blocks) so we’re working now to optimize the speed of the queries. We’re also preparing benchmarks for the speed of the API calls. ### Database diff tool for testing HAF-based apps We're developing a new testing system to test the port of hivemind to a HAF-based application: essentially it is a &quot;database diff&quot; tool that will allow us to detect differences (via SQL statements) between the tables created by the old hivemind app and the upcoming HAF-based hivemind. # Upcoming work On the hived side, we’ll be adding support to the sql_serializer for directly injecting “impacted account” data. After that, we can compare the relative performance of this method of inserting the data into postgres versus using the C-based postgres extension for computing this data, in order to make a decision about the best design alternative. We’ll also likely merge in the first hardfork-related changes that 1) allow more than one vote per three seconds by an account and 2) don’t kill curation rewards entirely when someone edits their vote strength on a post. Our HAF work will continue to focus on our first example HAF apps (account history app and HAF-based hivemind implementation). See: 22nd update of 2021 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">hive core dev meeting #28</title><link href="https://hivedocs.info/news/core/development/2021/09/06/hive-core-dev-meeting-28.html" rel="alternate" type="text/html" title="hive core dev meeting #28" /><published>2021-09-06T13:12:54-07:00</published><updated>2021-09-06T13:12:54-07:00</updated><id>https://hivedocs.info/news/core/development/2021/09/06/hive-core-dev-meeting-28</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/09/06/hive-core-dev-meeting-28.html">&lt;div id=&quot;content-howo-hive-core-dev-meeting-28&quot;&gt;https://www.youtube.com/watch?v=aD83Jpkg_iU

## Dev sync

It's better to listen to this one

## direct rc delegation review timeline

Direct RC delegations are pending review for a few weeks now as it's all hands on deck to finish off HAF, but @blocktrades' team is hoping to have time to provide a review this week.

## Communities v2 discussions and optimizations with HAF

Loooooooots of discussions there, mostly on the technical side and on how I can implement some features (python vs SQL). On top of it we went through some of the feature requests raised by the community.
Feel free to listen to it if you want the full picture but basically, we will make a post to raise some awareness on exactly what features are wanted, sort them in terms of &quot;work / reward&quot; and work on it. We already have a pretty substiantial list so I'll outline them in the initial post. 

## from @arcange HBD debt ratio parameters for printing

Basically a bunch of discussions regarding the debt ratio, with HBD being over 1$ for a bit now, a lot of HBD is being printed from the HIVE -&amp;gt; HBD conversion, which could result in payouts being in liquid hive instead of HBD. So the idea was to be able to raise the debt ratio % (currently at 10%) at will by the witnesses without requiring a hard fork.

Ultimately, the decision was to not do it to provide more faith for investors that the economic params won't change. On the other side as @gtg pointed out, the economic rules changed with the new op, so it definitely makes sense to update that debt ratio in the next hard fork.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@howo&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/hive-core-dev-meeting-28&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-howo-hive-core-dev-meeting-28').html();
      const outputElem = $('#content-howo-hive-core-dev-meeting-28');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-howo-hive-core-dev-meeting-28 {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-howo-hive-core-dev-meeting-28 code {
    background: white;
  }
  #content-howo-hive-core-dev-meeting-28 a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-howo-hive-core-dev-meeting-28 a:hover {
    border-bottom: 0;
  }
  #content-howo-hive-core-dev-meeting-28 h1 {
    font-size: 2.2em;
  }
  #content-howo-hive-core-dev-meeting-28 h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-howo-hive-core-dev-meeting-28 header small {
    color: #999;
    font-size: 50%;
  }
  #content-howo-hive-core-dev-meeting-28 img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/core/@howo/hive-core-dev-meeting-28&quot;&gt;hive core dev meeting #28&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@howo&quot;&gt;@howo&lt;/a&gt;
&lt;/p&gt;</content><author><name>howo</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">https://www.youtube.com/watch?v=aD83Jpkg_iU ## Dev sync It's better to listen to this one ## direct rc delegation review timeline Direct RC delegations are pending review for a few weeks now as it's all hands on deck to finish off HAF, but @blocktrades' team is hoping to have time to provide a review this week. ## Communities v2 discussions and optimizations with HAF Loooooooots of discussions there, mostly on the technical side and on how I can implement some features (python vs SQL). On top of it we went through some of the feature requests raised by the community. Feel free to listen to it if you want the full picture but basically, we will make a post to raise some awareness on exactly what features are wanted, sort them in terms of &quot;work / reward&quot; and work on it. We already have a pretty substiantial list so I'll outline them in the initial post. ## from @arcange HBD debt ratio parameters for printing Basically a bunch of discussions regarding the debt ratio, with HBD being over 1$ for a bit now, a lot of HBD is being printed from the HIVE -&amp;gt; HBD conversion, which could result in payouts being in liquid hive instead of HBD. So the idea was to be able to raise the debt ratio % (currently at 10%) at will by the witnesses without requiring a hard fork. Ultimately, the decision was to not do it to provide more faith for investors that the economic params won't change. On the other side as @gtg pointed out, the economic rules changed with the new op, so it definitely makes sense to update that debt ratio in the next hard fork. See: hive core dev meeting #28 by @howo</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hivedocs.info/assets/images/favicon.png" /><media:content medium="image" url="https://hivedocs.info/assets/images/favicon.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">gtg witness log</title><link href="https://hivedocs.info/howto/witness/node/2021/09/04/6pbblq-gtg-witness-log.html" rel="alternate" type="text/html" title="gtg witness log" /><published>2021-09-04T13:06:51-07:00</published><updated>2021-09-04T13:06:51-07:00</updated><id>https://hivedocs.info/howto/witness/node/2021/09/04/6pbblq-gtg-witness-log</id><content type="html" xml:base="https://hivedocs.info/howto/witness/node/2021/09/04/6pbblq-gtg-witness-log.html">&lt;div id=&quot;content-gtg-6pbblq-gtg-witness-log&quot;&gt;Updates once a week? I don’t think it’s possible, but I’ll keep trying.
Same with [“Hive Power Up Month”](/@hivebuzz/pum). So far so good.

(it's like recent [_&quot;gtg's abbrs.&quot;_](/@gtg/gtg-s-abbrs) but with more meaningful name, so I don't have to explain the title)

&lt;center&gt;https://www.youtube.com/watch?v=sq5mPQkHYKs&lt;sup&gt; - Yet another, freshly uploaded, fancy Hive logo reveal video&lt;/sup&gt;&lt;/center&gt;
# Hivemind
I’m not a 2nd layer wizard, but I’m doing my best to help with Hivemind’s QA. Maybe you can't see that on the outside, but Hivemind is improving its underlying tech a lot.

## [OpenHive-Network Github](https://github.com/openhive-network)
Improved [CodeQL](https://securitylab.github.com/tools/codeql/) workflow. It's used for code scanning, which then lets us improve code quality and security.

## Performance
A lot is going on this battlefield. While reviewing the latest changes I’ve found some issues that were preventing `hive sync` to catch up with the head block in reasonable time (`#162`)

## Testing
- Full hive sync tests. I’m hoping for the next stable release to be ready soon.
- Testing upgrade procedures, between `47a41c96` and current `develop` (`12efa972`)
- Testing SQL Serializer and HAF. Still a lot work to do but both projects behind those buzz-words are very promising.

# Updated snapshots and dumps
I’ve just made available for download a recent hivemind database dump and a hived API snapshot.
https://gtg.openhive.network/get

If you wonder about sizes:
- Blocks - 385G
- Hived API Snapshot - 352G (uncompressed 518G)
- Hivemind's database dump - 54G

## Optional hived flavor
Please note that `hived-v1.25.0` that is available for download is a general-purpose binary that is good for all use cases, i.e. for seed-node and fully featured API node.
However, on some of my nodes I’m running a different, witness-specific flavor of `hived` build, that was created with `-DSTORE_ACCOUNT_METADATA=OFF`.
It makes it incompatible with API nodes, as it has all account’s metadata records empty, but it still has everything that’s needed for seed nodes and witness nodes, and results in slightly smaller `shared_memory.bin` file.

# Miscellaneous
- Small fix for docker building instructions
- Revived `cli_wallet` meta-issue that I’ve mentioned in the last update, which includes recently added issues:
  - [Add support for http/https connections](https://gitlab.syncad.com/hive/hive/-/issues/168)
  - [Add support for offline use](https://gitlab.syncad.com/hive/hive/-/issues/169)
- Identified issues with building latest develop in certain environments

# Hive public seed nodes
One of the hard-coded public seed nodes was recently decommissioned so I’ve updated the list and while I’m at it I’d like to ask around for new/updated entries.

Here’s a current list:
```
api.hive.blog:2001              # blocktrades
seed.openhive.network:2001      # gtg
rpc.ausbit.dev:2001             # ausbitbank
hive-seed.roelandp.nl:2001      # roelandp
hive-seed.arcange.eu:2001       # arcange
anyx.io:2001                    # anyx
hived.splinterlands.com:2001    # aggroed
node.mahdiyari.info:2001        # mahdiyari
hive-seed.lukestokes.info:2001  # lukestokes.mhth
seed.liondani.com:2016          # liondani
hiveseed-se.privex.io:2001      # privex
```
This list of nodes is available in `doc/seednodes.txt` (for scripting) and hardcoded into `hived` binary.
Such a list doesn’t have to be long, few entries are enough for a node to quickly discover and connect to dozens of others, but it’s essential to have a reliable set of publicly available ones.
You don’t have to be a witness to run such a node, however, because witnesses’ job is to ensure security and reliability of the Hive network (and they are compensated for it) it’s expected that they run them.

If you run such node and your node is missing on that list please let me know in comments or on [OpenHive.Chat](https://openhive.chat) - I’ll either add it to the list or add to the waiting queue (to ensure its reliability before adding to repository)

##### Best practices for public seed nodes: 
- It has to be owned and operated by a well known Hive user (either individual or organization)
- It needs to have a fully qualified domain name that the given Hive user owns and is in full control.
- Preferred port is `2001`
- It should not be the same node as the witness node, although it can serve as a backup node in emergency situations.
- It can be the same node as an API node.
- At least 100Mbps network and 99% uptime.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@gtg&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/6pbblq-gtg-witness-log&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-gtg-6pbblq-gtg-witness-log').html();
      const outputElem = $('#content-gtg-6pbblq-gtg-witness-log');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-gtg-6pbblq-gtg-witness-log {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-gtg-6pbblq-gtg-witness-log code {
    background: white;
  }
  #content-gtg-6pbblq-gtg-witness-log a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-gtg-6pbblq-gtg-witness-log a:hover {
    border-bottom: 0;
  }
  #content-gtg-6pbblq-gtg-witness-log h1 {
    font-size: 2.2em;
  }
  #content-gtg-6pbblq-gtg-witness-log h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-gtg-6pbblq-gtg-witness-log header small {
    color: #999;
    font-size: 50%;
  }
  #content-gtg-6pbblq-gtg-witness-log img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-160391/@gtg/6pbblq-gtg-witness-log&quot;&gt;gtg witness log&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@gtg&quot;&gt;@gtg&lt;/a&gt;
&lt;/p&gt;</content><author><name>gtg</name></author><category term="howto" /><category term="witness" /><category term="node" /><summary type="html">Updates once a week? I don’t think it’s possible, but I’ll keep trying. Same with [“Hive Power Up Month”](/@hivebuzz/pum). So far so good. (it's like recent [_&quot;gtg's abbrs.&quot;_](/@gtg/gtg-s-abbrs) but with more meaningful name, so I don't have to explain the title) https://www.youtube.com/watch?v=sq5mPQkHYKs - Yet another, freshly uploaded, fancy Hive logo reveal video # Hivemind I’m not a 2nd layer wizard, but I’m doing my best to help with Hivemind’s QA. Maybe you can't see that on the outside, but Hivemind is improving its underlying tech a lot. ## [OpenHive-Network Github](https://github.com/openhive-network) Improved [CodeQL](https://securitylab.github.com/tools/codeql/) workflow. It's used for code scanning, which then lets us improve code quality and security. ## Performance A lot is going on this battlefield. While reviewing the latest changes I’ve found some issues that were preventing `hive sync` to catch up with the head block in reasonable time (`#162`) ## Testing - Full hive sync tests. I’m hoping for the next stable release to be ready soon. - Testing upgrade procedures, between `47a41c96` and current `develop` (`12efa972`) - Testing SQL Serializer and HAF. Still a lot work to do but both projects behind those buzz-words are very promising. # Updated snapshots and dumps I’ve just made available for download a recent hivemind database dump and a hived API snapshot. https://gtg.openhive.network/get If you wonder about sizes: - Blocks - 385G - Hived API Snapshot - 352G (uncompressed 518G) - Hivemind's database dump - 54G ## Optional hived flavor Please note that `hived-v1.25.0` that is available for download is a general-purpose binary that is good for all use cases, i.e. for seed-node and fully featured API node. However, on some of my nodes I’m running a different, witness-specific flavor of `hived` build, that was created with `-DSTORE_ACCOUNT_METADATA=OFF`. It makes it incompatible with API nodes, as it has all account’s metadata records empty, but it still has everything that’s needed for seed nodes and witness nodes, and results in slightly smaller `shared_memory.bin` file. # Miscellaneous - Small fix for docker building instructions - Revived `cli_wallet` meta-issue that I’ve mentioned in the last update, which includes recently added issues: - [Add support for http/https connections](https://gitlab.syncad.com/hive/hive/-/issues/168) - [Add support for offline use](https://gitlab.syncad.com/hive/hive/-/issues/169) - Identified issues with building latest develop in certain environments # Hive public seed nodes One of the hard-coded public seed nodes was recently decommissioned so I’ve updated the list and while I’m at it I’d like to ask around for new/updated entries. Here’s a current list: ``` api.hive.blog:2001 # blocktrades seed.openhive.network:2001 # gtg rpc.ausbit.dev:2001 # ausbitbank hive-seed.roelandp.nl:2001 # roelandp hive-seed.arcange.eu:2001 # arcange anyx.io:2001 # anyx hived.splinterlands.com:2001 # aggroed node.mahdiyari.info:2001 # mahdiyari hive-seed.lukestokes.info:2001 # lukestokes.mhth seed.liondani.com:2016 # liondani hiveseed-se.privex.io:2001 # privex ``` This list of nodes is available in `doc/seednodes.txt` (for scripting) and hardcoded into `hived` binary. Such a list doesn’t have to be long, few entries are enough for a node to quickly discover and connect to dozens of others, but it’s essential to have a reliable set of publicly available ones. You don’t have to be a witness to run such a node, however, because witnesses’ job is to ensure security and reliability of the Hive network (and they are compensated for it) it’s expected that they run them. If you run such node and your node is missing on that list please let me know in comments or on [OpenHive.Chat](https://openhive.chat) - I’ll either add it to the list or add to the waiting queue (to ensure its reliability before adding to repository) ##### Best practices for public seed nodes: - It has to be owned and operated by a well known Hive user (either individual or organization) - It needs to have a fully qualified domain name that the given Hive user owns and is in full control. - Preferred port is `2001` - It should not be the same node as the witness node, although it can serve as a backup node in emergency situations. - It can be the same node as an API node. - At least 100Mbps network and 99% uptime. See: gtg witness log by @gtg</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://img.youtube.com/vi/sq5mPQkHYKs/0.jpg" /><media:content medium="image" url="https://img.youtube.com/vi/sq5mPQkHYKs/0.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Hivemind dev environment setup guide (according to my findings, it may or may not be the best way)</title><link href="https://hivedocs.info/howto/hivemind/2021/09/04/hivemind-dev-environment-setup-guide-according-to-my-findings-it-may-or-may-not-be-the-best-way.html" rel="alternate" type="text/html" title="Hivemind dev environment setup guide (according to my findings, it may or may not be the best way)" /><published>2021-09-04T02:41:03-07:00</published><updated>2021-09-04T02:41:03-07:00</updated><id>https://hivedocs.info/howto/hivemind/2021/09/04/hivemind-dev-environment-setup-guide-according-to-my-findings-it-may-or-may-not-be-the-best-way</id><content type="html" xml:base="https://hivedocs.info/howto/hivemind/2021/09/04/hivemind-dev-environment-setup-guide-according-to-my-findings-it-may-or-may-not-be-the-best-way.html">&lt;div id=&quot;content-howo-hivemind-dev-environment-setup-guide-according-to-my-findings-it-may-or-may-not-be-the-best-way&quot;&gt;
![image.png](https://files.peakd.com/file/peakd-hive/howo/23tbHAEfLVug1GpxE6vi5LhibWoQ88SeqNsKWio9vGeEWtPtw8EgBzsGQxkkKVdT7pj6v.png)


Hello !

As some of you may know, I am working on hivemind (wohoo communities v2 coming soon:tm:)
Setting up a dev environment can be a bit tricky so I figured I'd share a bit of the knowledge I got while doing it. 

For the record it's what I found, it may not be the best way to do it, if you know ways to improve it please comment below :)

# Postgresql

First we need a postgres database, I think docker is best here because it allows us to have exactly the version we want, and not struggle installing it. 

you will need to install docker and docker compose.

one you have that create a docker-compose.yml file with this content

```
version: '3'
services:
    dbpostgres:
        image: postgres:11
        container_name: hivemind-postgres-dev-container
        ports:
            - &quot;5532:5432&quot;
        environment:
            - POSTGRES_DB=hive
            - POSTGRES_PASSWORD=root
```

Then in the directory run 
`docker compose up -d `


then connect to it using your favorite tool (I use datagrip) and install the intarray extension: (you could improve this with a post install script with the dockerfile but I didnt' have the motivation to do it for now)

`CREATE EXTENSION IF NOT EXISTS intarray;`


# Hive setup:

(if hive got another update check https://gtg.openhive.network/get/bin/)
(it's better to build hived yourself but we can assume @gtg is trustworthy)
`wget https://gtg.openhive.network/get/bin/hived-v1.25.0`
``mv hived-v1.25.0 hived &amp;amp;&amp;amp; chmod +x hived``
``mkdir data``

execute hived for two seconds to create the directory structure

`./hived -d data`
then exit and delete the blockchain files 
`rm -rf ./data/blockchain/*`
then we'll download the first 5 million block_log from gtg and put it on the right directory
`wget https://gtg.openhive.network/get/blockchain/block_log.5M -P data/blockchain/ &amp;amp;&amp;amp; mv data/blockchain/block_log.5M block_log`
 
then update the hived config.ini:

nano data/config.ini
```
log-appender = {&quot;appender&quot;:&quot;stderr&quot;,&quot;stream&quot;:&quot;std_error&quot;}
log-logger = {&quot;name&quot;:&quot;default&quot;,&quot;level&quot;:&quot;info&quot;,&quot;appender&quot;:&quot;stderr&quot;}
backtrace = yes
plugin = webserver p2p json_rpc
plugin = database_api
# condenser_api enabled per abw request
plugin = condenser_api
plugin = block_api
# gandalf enabled witness + rc
plugin = witness
plugin = rc

# market_history enabled per abw request
plugin = market_history
plugin = market_history_api

plugin = account_history_rocksdb
plugin = account_history_api

# gandalf enabled transaction status
plugin = transaction_status
plugin = transaction_status_api

# gandalf enabled account by key
plugin = account_by_key
plugin = account_by_key_api

# and few apis
plugin = block_api network_broadcast_api rc_api

history-disable-pruning = 1
account-history-rocksdb-path = &quot;blockchain/account-history-rocksdb-storage&quot;

# shared-file-dir = &quot;/run/hive&quot;
shared-file-size = 20G
shared-file-full-threshold = 9500
shared-file-scale-rate = 1000

flush-state-interval = 0

market-history-bucket-size = [15,60,300,3600,86400]
market-history-buckets-per-size = 5760

p2p-endpoint = 0.0.0.0:2001
p2p-seed-node =
# gtg.openhive.network:2001

transaction-status-block-depth = 64000
transaction-status-track-after-block = 42000000

webserver-http-endpoint = 0.0.0.0:8091
webserver-ws-endpoint = 0.0.0.0:8090

webserver-thread-pool-size = 8
```
finally replay your node, and stop at 5 million blocks

`./hived --replay-blockchain --stop-replay-at-block 5000000 -d data
`
this will cook for 10-20 minutes depending on your hardware


# Hivemind setup

First, clone hivemind
`git clone git@gitlab.syncad.com:hive/hivemind.git`
then switch to the develop branch (usually better for...developping)
`git checkout develop`
and then install the dependencies
`python3 -m pip install --no-cache-dir --verbose --user -e .[dev] 2&amp;gt;&amp;amp;1 | tee pip_install.log`

Then if your hived node is done replaying, you can do your first sync:

`./cli.py --database-url postgresql://postgres:root@localhost:5532/hive --test-max-block=4999998 --steemd-url={&quot;default&quot;:&quot;http://localhost:8091&quot;}`

this process will take quite a bit ( ~20 minutes or more depending on your hardware)

Then what I like to do is dump the database so I can get back to this state easily without having to resync everything:

`PGPASSWORD=root  pg_dump -h localhost -p 5532 -U postgres -d hive -f dump.dump -format=custom`

and if I want to restore:

`PGPASSWORD=root pg_restore -h localhost -p 5532 -U postgres -d hive  dump.dump -j12 --clean`

Finally if you want to test some specific applications, mocks are your friends ! look into the `mock_data` folder for examples.

In order to do a sync and add the mock data you can do this:

`./cli.py --database-url postgresql://postgres:root@localhost:5532/hive --test-max-block=4999998 --steemd-url={&quot;default&quot;:&quot;http://localhost:8091&quot;}  --mock-block-data-path /home/howo/projects/hivemind/mock_data/block_data/community_op/mock_block_data_community_test.json` (replace the path with whatever mock file you have obviously)

Slight note on  --test-max-block, it needs to be the height of the highest block of your mocks + 2, because hivemind trails the real blockchain by two blocks, so if you set  --test-max-block as 2 and your mocks end at block 2, they won't be picked up. &lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@howo&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/hivemind-dev-environment-setup-guide-according-to-my-findings-it-may-or-may-not-be-the-best-way&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-howo-hivemind-dev-environment-setup-guide-according-to-my-findings-it-may-or-may-not-be-the-best-way').html();
      const outputElem = $('#content-howo-hivemind-dev-environment-setup-guide-according-to-my-findings-it-may-or-may-not-be-the-best-way');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-howo-hivemind-dev-environment-setup-guide-according-to-my-findings-it-may-or-may-not-be-the-best-way {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-howo-hivemind-dev-environment-setup-guide-according-to-my-findings-it-may-or-may-not-be-the-best-way code {
    background: white;
  }
  #content-howo-hivemind-dev-environment-setup-guide-according-to-my-findings-it-may-or-may-not-be-the-best-way a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-howo-hivemind-dev-environment-setup-guide-according-to-my-findings-it-may-or-may-not-be-the-best-way a:hover {
    border-bottom: 0;
  }
  #content-howo-hivemind-dev-environment-setup-guide-according-to-my-findings-it-may-or-may-not-be-the-best-way h1 {
    font-size: 2.2em;
  }
  #content-howo-hivemind-dev-environment-setup-guide-according-to-my-findings-it-may-or-may-not-be-the-best-way h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-howo-hivemind-dev-environment-setup-guide-according-to-my-findings-it-may-or-may-not-be-the-best-way header small {
    color: #999;
    font-size: 50%;
  }
  #content-howo-hivemind-dev-environment-setup-guide-according-to-my-findings-it-may-or-may-not-be-the-best-way img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/hivemind/@howo/hivemind-dev-environment-setup-guide-according-to-my-findings-it-may-or-may-not-be-the-best-way&quot;&gt;Hivemind dev environment setup guide (according to my findings, it may or may not be the best way)&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@howo&quot;&gt;@howo&lt;/a&gt;
&lt;/p&gt;</content><author><name>howo</name></author><category term="howto" /><category term="hivemind" /><summary type="html">![image.png](https://files.peakd.com/file/peakd-hive/howo/23tbHAEfLVug1GpxE6vi5LhibWoQ88SeqNsKWio9vGeEWtPtw8EgBzsGQxkkKVdT7pj6v.png) Hello ! As some of you may know, I am working on hivemind (wohoo communities v2 coming soon:tm:) Setting up a dev environment can be a bit tricky so I figured I'd share a bit of the knowledge I got while doing it. For the record it's what I found, it may not be the best way to do it, if you know ways to improve it please comment below :) # Postgresql First we need a postgres database, I think docker is best here because it allows us to have exactly the version we want, and not struggle installing it. you will need to install docker and docker compose. one you have that create a docker-compose.yml file with this content ``` version: '3' services: dbpostgres: image: postgres:11 container_name: hivemind-postgres-dev-container ports: - &quot;5532:5432&quot; environment: - POSTGRES_DB=hive - POSTGRES_PASSWORD=root ``` Then in the directory run `docker compose up -d ` then connect to it using your favorite tool (I use datagrip) and install the intarray extension: (you could improve this with a post install script with the dockerfile but I didnt' have the motivation to do it for now) `CREATE EXTENSION IF NOT EXISTS intarray;` # Hive setup: (if hive got another update check https://gtg.openhive.network/get/bin/) (it's better to build hived yourself but we can assume @gtg is trustworthy) `wget https://gtg.openhive.network/get/bin/hived-v1.25.0` ``mv hived-v1.25.0 hived &amp;amp;&amp;amp; chmod +x hived`` ``mkdir data`` execute hived for two seconds to create the directory structure `./hived -d data` then exit and delete the blockchain files `rm -rf ./data/blockchain/*` then we'll download the first 5 million block_log from gtg and put it on the right directory `wget https://gtg.openhive.network/get/blockchain/block_log.5M -P data/blockchain/ &amp;amp;&amp;amp; mv data/blockchain/block_log.5M block_log` then update the hived config.ini: nano data/config.ini ``` log-appender = {&quot;appender&quot;:&quot;stderr&quot;,&quot;stream&quot;:&quot;std_error&quot;} log-logger = {&quot;name&quot;:&quot;default&quot;,&quot;level&quot;:&quot;info&quot;,&quot;appender&quot;:&quot;stderr&quot;} backtrace = yes plugin = webserver p2p json_rpc plugin = database_api # condenser_api enabled per abw request plugin = condenser_api plugin = block_api # gandalf enabled witness + rc plugin = witness plugin = rc # market_history enabled per abw request plugin = market_history plugin = market_history_api plugin = account_history_rocksdb plugin = account_history_api # gandalf enabled transaction status plugin = transaction_status plugin = transaction_status_api # gandalf enabled account by key plugin = account_by_key plugin = account_by_key_api # and few apis plugin = block_api network_broadcast_api rc_api history-disable-pruning = 1 account-history-rocksdb-path = &quot;blockchain/account-history-rocksdb-storage&quot; # shared-file-dir = &quot;/run/hive&quot; shared-file-size = 20G shared-file-full-threshold = 9500 shared-file-scale-rate = 1000 flush-state-interval = 0 market-history-bucket-size = [15,60,300,3600,86400] market-history-buckets-per-size = 5760 p2p-endpoint = 0.0.0.0:2001 p2p-seed-node = # gtg.openhive.network:2001 transaction-status-block-depth = 64000 transaction-status-track-after-block = 42000000 webserver-http-endpoint = 0.0.0.0:8091 webserver-ws-endpoint = 0.0.0.0:8090 webserver-thread-pool-size = 8 ``` finally replay your node, and stop at 5 million blocks `./hived --replay-blockchain --stop-replay-at-block 5000000 -d data ` this will cook for 10-20 minutes depending on your hardware # Hivemind setup First, clone hivemind `git clone git@gitlab.syncad.com:hive/hivemind.git` then switch to the develop branch (usually better for...developping) `git checkout develop` and then install the dependencies `python3 -m pip install --no-cache-dir --verbose --user -e .[dev] 2&amp;gt;&amp;amp;1 | tee pip_install.log` Then if your hived node is done replaying, you can do your first sync: `./cli.py --database-url postgresql://postgres:root@localhost:5532/hive --test-max-block=4999998 --steemd-url={&quot;default&quot;:&quot;http://localhost:8091&quot;}` this process will take quite a bit ( ~20 minutes or more depending on your hardware) Then what I like to do is dump the database so I can get back to this state easily without having to resync everything: `PGPASSWORD=root pg_dump -h localhost -p 5532 -U postgres -d hive -f dump.dump -format=custom` and if I want to restore: `PGPASSWORD=root pg_restore -h localhost -p 5532 -U postgres -d hive dump.dump -j12 --clean` Finally if you want to test some specific applications, mocks are your friends ! look into the `mock_data` folder for examples. In order to do a sync and add the mock data you can do this: `./cli.py --database-url postgresql://postgres:root@localhost:5532/hive --test-max-block=4999998 --steemd-url={&quot;default&quot;:&quot;http://localhost:8091&quot;} --mock-block-data-path /home/howo/projects/hivemind/mock_data/block_data/community_op/mock_block_data_community_test.json` (replace the path with whatever mock file you have obviously) Slight note on --test-max-block, it needs to be the height of the highest block of your mocks + 2, because hivemind trails the real blockchain by two blocks, so if you set --test-max-block as 2 and your mocks end at block 2, they won't be picked up. See: Hivemind dev environment setup guide (according to my findings, it may or may not be the best way) by @howo</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://files.peakd.com/file/peakd-hive/howo/23tbHAEfLVug1GpxE6vi5LhibWoQ88SeqNsKWio9vGeEWtPtw8EgBzsGQxkkKVdT7pj6v.png" /><media:content medium="image" url="https://files.peakd.com/file/peakd-hive/howo/23tbHAEfLVug1GpxE6vi5LhibWoQ88SeqNsKWio9vGeEWtPtw8EgBzsGQxkkKVdT7pj6v.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">gtg’s abbrs.</title><link href="https://hivedocs.info/news/witness/node/2021/08/28/gtg-s-abbrs.html" rel="alternate" type="text/html" title="gtg’s abbrs." /><published>2021-08-28T10:43:00-07:00</published><updated>2021-08-28T10:43:00-07:00</updated><id>https://hivedocs.info/news/witness/node/2021/08/28/gtg-s-abbrs</id><content type="html" xml:base="https://hivedocs.info/news/witness/node/2021/08/28/gtg-s-abbrs.html">&lt;div id=&quot;content-gtg-gtg-s-abbrs&quot;&gt;A low-effort post (not really) about some of the stuff that I’ve been busy with.
These posts are meant to be a quick #witness-update. Remember? We used to write those.

&amp;gt; abbr. (plural abbrs.)
&amp;gt; Abbreviation of abbreviation.

&lt;center&gt;
![HiveText.png](https://images.hive.blog/DQmWkmhRAFcqarq7snYRGDBtxpeJGfHBQY1BfAYWHFWgYX9/HiveText.png)&lt;/center&gt;

# [OpenHive.Chat](https://openhive.chat)
The chat that used to use the old name that I don’t even want to mention is now gone. For good. It became obsolete when we moved, but we wanted to keep it alive for those who don’t log in often enough so that they could still find their way to Hive.

![OpenHive.Chat](https://images.hive.blog/DQmeeFxenxSzcX4xYC2UBxfZSrYEEMWXdU32736R1r6CHkZ/hive.gif)


If you are new to this: [OpenHive.Chat](https://openhive.chat) is based on [Rocket.Chat](https://rocket.chat). It’s free and OpenSource. Our instance has Hive authentication (via [HiveSigner](https://hivesigner.com/)).
You don’t need to add dApp's authority to use it (you don’t send your posting key anywhere, you just use your key on the client side to sign a message to prove that it’s you&lt;sup&gt;(\*)&lt;/sup&gt;).
&lt;sub&gt;&lt;sup&gt;(\*)&lt;/sup&gt; someone who controls a given private posting key&lt;/sub&gt;

# OpenHive-Network on GitHub
I re-enabled push mirrors for [`hive`](https://github.com/openhive-network/hive) and [`hivemind`](https://github.com/openhive-network/hivemind)
As you may have noticed, it played a big role in the recent GitLab breakdowns.

Reminder:
We use it as a push mirror for GitLab repository, mostly for visibility and decentralization. If you have an account on [GitHub](https://github.com/), please “Fork” at least [hive](https://github.com/openhive-network/hive) and [hivemind](https://github.com/openhive-network/hivemind) and “Star” them if you haven’t done so yet. We haven't paid much attention to it, but apparently it's important for some outside metrics, i.e. how others perceive our project.

If you are actually reading my post, you can join a little game to win an upvote ;-)
- Go to https://github.com/openhive-network/hive
- Click “Star” and “Fork” 
- Do the same with https://github.com/openhive-network/hivemind
- Post a comment with your GitHub username
- People who did that in the past are also eligible, just post a comment with your GitHub username

Since last time, we’ve gained 25 stars and 13 forks for Hive. Currently: 185 / 55. Please, do some clicks.

# cli_wallet
This piece of software needs more love. For many users, it’s the only software that will ever have access to their privileged keys (Active, Owner). It’s what exchanges, whales, and smart people rely on to sign their transactions. 
I’m doing some maintenance work around it, looking for bugs and missing features, doing small fixes, cleanups, and docs improvements. But don’t be mistaken, it still requires tons of work.

One of the bigger tasks to be done (and being currently handled by @blocktrades team) is: “[Signing with an account authority is currently not supported](https://gitlab.syncad.com/hive/hive/-/issues/150)”

# Miscellaneous
- Restored some missing metadata (issues, comments) in GitLab related to hive / hivemind development (that were recently lost)
- Found sneaky “[errors during snapshot load (Invalid argument: You have to open all column families)](https://gitlab.syncad.com/hive/hive/-/issues/161)”
- Found “[unexpected requirement for `list_my_accounts`](https://gitlab.syncad.com/hive/hive/-/issues/163)| and fixed it. It was just misleading `FC_ASSERT` that erroneously claimed that `account_history_api plugin` was needed.
- Staying up to date with the current develop of hive and hivemind, which has enabled me to find such issues as “[current develop build fails on Ubuntu 20.04 LTS](https://gitlab.syncad.com/hive/hive/-/issues/164)”
- Or that it’s “[not possible to reach head block in live sync after resuming from failed `after-initial-sync`](https://gitlab.syncad.com/hive/hivemind/-/issues/162)”
- Created a PEP517 workaround to fix [Hivemind install issues](https://gitlab.syncad.com/hive/hivemind/-/issues/161)
Apparently, Python people were like:
&amp;gt; 
&amp;gt; \- _We have a dozen of ways to install software and that might be confusing for sane people..._
&amp;gt;
&amp;gt; \- _\*thinking\*_
&amp;gt;
&amp;gt; \- _Let's make another one!_
&amp;gt; 

# Governance voting expiration 
Do you know that the last Hard Fork brought governance voting expiration?
It means that if you are not active governance-wise (voting for proposals, voting for witnesses), your votes will expire after some (long) time. For example, for my account, expiration is currently set to `2022-07-06 12:06:24`.
Vote for one or many HBD stabilizer proposal(s), approve me as a witness, unapprove inactive witnesses. Those are examples of many good ways to extend your expiration date and help Hive to grow :-) 

# Bonus
ASCII version of the logo:

```
                                    
               ___    ___           
          /\   \HI\   \VE\          
         /HI\   \VE\   \HI\         
        /HIVE\   \HI\   \VE\        
       /HIVEHI\   \VE\   \HI\       
      /HIVEHIVE\          \LO\      
      \HIVEHIVE/   ___    /VE/      
       \HIVEHI/   /VE/   /HI/       
        \HIVE/   /HI/   /VE/        
         \HI/   /VE/   /HI/         
          \/   /HI/   /VE/          
                                    
                                    
```&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@gtg&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/gtg-s-abbrs&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-gtg-gtg-s-abbrs').html();
      const outputElem = $('#content-gtg-gtg-s-abbrs');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-gtg-gtg-s-abbrs {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-gtg-gtg-s-abbrs code {
    background: white;
  }
  #content-gtg-gtg-s-abbrs a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-gtg-gtg-s-abbrs a:hover {
    border-bottom: 0;
  }
  #content-gtg-gtg-s-abbrs h1 {
    font-size: 2.2em;
  }
  #content-gtg-gtg-s-abbrs h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-gtg-gtg-s-abbrs header small {
    color: #999;
    font-size: 50%;
  }
  #content-gtg-gtg-s-abbrs img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-160391/@gtg/gtg-s-abbrs&quot;&gt;gtg’s abbrs.&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@gtg&quot;&gt;@gtg&lt;/a&gt;
&lt;/p&gt;</content><author><name>gtg</name></author><category term="news" /><category term="witness" /><category term="node" /><summary type="html">A low-effort post (not really) about some of the stuff that I’ve been busy with. These posts are meant to be a quick #witness-update. Remember? We used to write those. &amp;gt; abbr. (plural abbrs.) &amp;gt; Abbreviation of abbreviation. ![HiveText.png](https://images.hive.blog/DQmWkmhRAFcqarq7snYRGDBtxpeJGfHBQY1BfAYWHFWgYX9/HiveText.png) # [OpenHive.Chat](https://openhive.chat) The chat that used to use the old name that I don’t even want to mention is now gone. For good. It became obsolete when we moved, but we wanted to keep it alive for those who don’t log in often enough so that they could still find their way to Hive. ![OpenHive.Chat](https://images.hive.blog/DQmeeFxenxSzcX4xYC2UBxfZSrYEEMWXdU32736R1r6CHkZ/hive.gif) If you are new to this: [OpenHive.Chat](https://openhive.chat) is based on [Rocket.Chat](https://rocket.chat). It’s free and OpenSource. Our instance has Hive authentication (via [HiveSigner](https://hivesigner.com/)). You don’t need to add dApp's authority to use it (you don’t send your posting key anywhere, you just use your key on the client side to sign a message to prove that it’s you(\*)). (\*) someone who controls a given private posting key # OpenHive-Network on GitHub I re-enabled push mirrors for [`hive`](https://github.com/openhive-network/hive) and [`hivemind`](https://github.com/openhive-network/hivemind) As you may have noticed, it played a big role in the recent GitLab breakdowns. Reminder: We use it as a push mirror for GitLab repository, mostly for visibility and decentralization. If you have an account on [GitHub](https://github.com/), please “Fork” at least [hive](https://github.com/openhive-network/hive) and [hivemind](https://github.com/openhive-network/hivemind) and “Star” them if you haven’t done so yet. We haven't paid much attention to it, but apparently it's important for some outside metrics, i.e. how others perceive our project. If you are actually reading my post, you can join a little game to win an upvote ;-) - Go to https://github.com/openhive-network/hive - Click “Star” and “Fork” - Do the same with https://github.com/openhive-network/hivemind - Post a comment with your GitHub username - People who did that in the past are also eligible, just post a comment with your GitHub username Since last time, we’ve gained 25 stars and 13 forks for Hive. Currently: 185 / 55. Please, do some clicks. # cli_wallet This piece of software needs more love. For many users, it’s the only software that will ever have access to their privileged keys (Active, Owner). It’s what exchanges, whales, and smart people rely on to sign their transactions. I’m doing some maintenance work around it, looking for bugs and missing features, doing small fixes, cleanups, and docs improvements. But don’t be mistaken, it still requires tons of work. One of the bigger tasks to be done (and being currently handled by @blocktrades team) is: “[Signing with an account authority is currently not supported](https://gitlab.syncad.com/hive/hive/-/issues/150)” # Miscellaneous - Restored some missing metadata (issues, comments) in GitLab related to hive / hivemind development (that were recently lost) - Found sneaky “[errors during snapshot load (Invalid argument: You have to open all column families)](https://gitlab.syncad.com/hive/hive/-/issues/161)” - Found “[unexpected requirement for `list_my_accounts`](https://gitlab.syncad.com/hive/hive/-/issues/163)| and fixed it. It was just misleading `FC_ASSERT` that erroneously claimed that `account_history_api plugin` was needed. - Staying up to date with the current develop of hive and hivemind, which has enabled me to find such issues as “[current develop build fails on Ubuntu 20.04 LTS](https://gitlab.syncad.com/hive/hive/-/issues/164)” - Or that it’s “[not possible to reach head block in live sync after resuming from failed `after-initial-sync`](https://gitlab.syncad.com/hive/hivemind/-/issues/162)” - Created a PEP517 workaround to fix [Hivemind install issues](https://gitlab.syncad.com/hive/hivemind/-/issues/161) Apparently, Python people were like: &amp;gt; &amp;gt; \- _We have a dozen of ways to install software and that might be confusing for sane people..._ &amp;gt; &amp;gt; \- _\*thinking\*_ &amp;gt; &amp;gt; \- _Let's make another one!_ &amp;gt; # Governance voting expiration Do you know that the last Hard Fork brought governance voting expiration? It means that if you are not active governance-wise (voting for proposals, voting for witnesses), your votes will expire after some (long) time. For example, for my account, expiration is currently set to `2022-07-06 12:06:24`. Vote for one or many HBD stabilizer proposal(s), approve me as a witness, unapprove inactive witnesses. Those are examples of many good ways to extend your expiration date and help Hive to grow :-) # Bonus ASCII version of the logo: ``` ___ ___ /\ \HI\ \VE\ /HI\ \VE\ \HI\ /HIVE\ \HI\ \VE\ /HIVEHI\ \VE\ \HI\ /HIVEHIVE\ \LO\ \HIVEHIVE/ ___ /VE/ \HIVEHI/ /VE/ /HI/ \HIVE/ /HI/ /VE/ \HI/ /VE/ /HI/ \/ /HI/ /VE/ ``` See: gtg’s abbrs. by @gtg</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmWkmhRAFcqarq7snYRGDBtxpeJGfHBQY1BfAYWHFWgYX9/HiveText.png" /><media:content medium="image" url="https://images.hive.blog/DQmWkmhRAFcqarq7snYRGDBtxpeJGfHBQY1BfAYWHFWgYX9/HiveText.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">21st update of 2021 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2021/08/27/21st-update-of-2021-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="21st update of 2021 on BlockTrades work on Hive software" /><published>2021-08-27T13:45:27-07:00</published><updated>2021-08-27T13:45:27-07:00</updated><id>https://hivedocs.info/news/core/development/2021/08/27/21st-update-of-2021-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/08/27/21st-update-of-2021-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-21st-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)

It’s been a while since I last posted a progress update, so I apologize for the delay.

In the early part of August, I devoted some time to looking for potential bottlenecks in our API node infrastructure, making improvements, then analyzing the results. The positive results of that work were discussed in  [Hive core developer meeting #27](https://www.youtube.com/watch?v=8DFXZPK9mQ0), so I won’t repeat the details today, but we did see very substantial gains in API response times. I may make a separate post in the future to show some of the gains across various API calls.

In this post I will focus mainly on some of the coding work done in the past two weeks by the BlockTrades team. As a side note, a lot of smaller tasks were completed during this period, including internal infrastructure improvements (i.e. adding and configuring new servers, gitlab is now hosted on a faster server), testing improvements, etc. that I won’t go into detail about here.

# Hived work (blockchain node software)

As mentioned in the Hive dev meeting, we reduced hived’s memory usage from 20GB to 16GB. As part of this work, we also did some code cleanup and minor efficiency improvements:
https://gitlab.syncad.com/hive/hive/-/merge_requests/248
https://gitlab.syncad.com/hive/hive/-/merge_requests/247

We also completed the refactor of the command-line-interface wallet to reduce the amount of work required when new API methods are added to hived:
https://gitlab.syncad.com/hive/hive/-/merge_requests/170


### Speeding up sql_serializer plugin that writes to HAF database
Most of the recent work in hived has focused on the sql_serializer plugin. This plugin fills a HAF database with the blockchain data used by Hive applications, so it is fundamental to our plan for a scalable 2nd layer app ecosystem.

Since the serializer provides data to HAF apps, the speed at which it can transfer blockchain data to a postgres database sets an upper limit on how fast a new HAF server can be initialized from scratch. Based on benchmarks ran last night, we’ve reduced this time down to where the serializer can write all 50M+ blocks of Hive to a postgres database and initialize the associated table indexes in 7 hours (on a fast machine). That’s more than 2x faster than our previous time for this task.

As impressive as those times are, we should still be able to dramatically reduce this time in many cases, by enabling an option for the serializer to filter out operations that aren’t interesting to the Hive apps hosted on a specific HAF server. For example, a standard social media application such as Hivemind doesn’t need to process most of the blockchain’s custom_json operations, so a lightweight hivemind node could configure its serializer to use a regular expression to capture only the operations it supports.


# Hivemind (2nd layer applications + social media middleware)

### bug fixes
Fix to community pagination with pinned posts: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/496

Only allow referencing permlinks that haven’t been deleted:
https://gitlab.syncad.com/hive/hivemind/-/merge_requests/494

Fixes related to improperly versioned package dependencies:
https://gitlab.syncad.com/hive/hivemind/-/merge_requests/498
https://gitlab.syncad.com/hive/hivemind/-/merge_requests/504
https://gitlab.syncad.com/hive/hivemind/-/merge_requests/506


Restore ctrl-c breaking and sync from block_log database with mock data (part of testing work for HAF):
https://gitlab.syncad.com/hive/hivemind/-/merge_requests/508

### Hivemind optimizations

Improve query planning under postgres 12 (postgres 12 is shipped with Ubuntu 20, so we’re planning to make 12 the recommended version for HAF and hivemind):
https://gitlab.syncad.com/hive/hivemind/-/merge_requests/505

Speedup of post-massive sync cleanup phase:
https://gitlab.syncad.com/hive/hivemind/-/merge_requests/507

Improved storage management during massive reputation data processing to speedup massive sync:
https://gitlab.syncad.com/hive/hivemind/-/merge_requests/509

After consulting with Hive dev apps to be sure no one used it, we are going to eliminate the “active” field from the response returned by some post-related Hive API calls.

# Hive Application Framework (HAF)

We’re currently examining alternatives for injecting “computed data” into a HAF database. As a specific example, currently hived computes “impacted accounts” which are a set of accounts affected by each blockchain operation. Many HAF apps will also need information about impacted accounts. In theory, there are three ways to handle this: 1) have the sql_serializer write the data computed by hived directly to the database, 2) re-use the code from hived in the form of a postgres c++ extension, and 3) have HAF apps recompute this data themselves. Option 3 seems pretty wasteful, so we’re mostly looking at options 1 and 2. Personally, I favor option 1, but we’re trying out option 2 now, to see how it works out.

### HAF-based Account history app

We’ve completed our first example app for HAF. This app replaces the functionality of a hived account history node. This means that future Hive API nodes will be able to operate with a consensus hived node instead of needed to run a heavier weight hived node configured with the account history plugin. The app is located here: https://gitlab.syncad.com/hive/HAfAH/-/commits/develop

Use of this app should also result in much more scalable and responsive account history API calls (currently these are some of the biggest bottlenecks in terms of API performance). I will probably have some benchmarks for this by next progress report.

### HAF-based Hivemind app
We had to make further updates to hivemind to get it to work from tables similar to those used by HAF (we’d done this previously, but later changes to hivemind had to be accounted for). Yesterday we completed a successful massive sync with this new quasi-HAF version of hivemind.

# What’s next?
In the coming week we’ll be focused on the following tasks:
* conversion of hivemind into a HAF app
* testing and benchmarking of HAF account history app
* continued tuning of HAF code based on above work
* begin planning for HF26&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/21st-update-of-2021-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-21st-update-of-2021-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-21st-update-of-2021-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-21st-update-of-2021-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-21st-update-of-2021-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-21st-update-of-2021-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-21st-update-of-2021-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-21st-update-of-2021-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-21st-update-of-2021-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-21st-update-of-2021-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-21st-update-of-2021-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/21st-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;21st update of 2021 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) It’s been a while since I last posted a progress update, so I apologize for the delay. In the early part of August, I devoted some time to looking for potential bottlenecks in our API node infrastructure, making improvements, then analyzing the results. The positive results of that work were discussed in [Hive core developer meeting #27](https://www.youtube.com/watch?v=8DFXZPK9mQ0), so I won’t repeat the details today, but we did see very substantial gains in API response times. I may make a separate post in the future to show some of the gains across various API calls. In this post I will focus mainly on some of the coding work done in the past two weeks by the BlockTrades team. As a side note, a lot of smaller tasks were completed during this period, including internal infrastructure improvements (i.e. adding and configuring new servers, gitlab is now hosted on a faster server), testing improvements, etc. that I won’t go into detail about here. # Hived work (blockchain node software) As mentioned in the Hive dev meeting, we reduced hived’s memory usage from 20GB to 16GB. As part of this work, we also did some code cleanup and minor efficiency improvements: https://gitlab.syncad.com/hive/hive/-/merge_requests/248 https://gitlab.syncad.com/hive/hive/-/merge_requests/247 We also completed the refactor of the command-line-interface wallet to reduce the amount of work required when new API methods are added to hived: https://gitlab.syncad.com/hive/hive/-/merge_requests/170 ### Speeding up sql_serializer plugin that writes to HAF database Most of the recent work in hived has focused on the sql_serializer plugin. This plugin fills a HAF database with the blockchain data used by Hive applications, so it is fundamental to our plan for a scalable 2nd layer app ecosystem. Since the serializer provides data to HAF apps, the speed at which it can transfer blockchain data to a postgres database sets an upper limit on how fast a new HAF server can be initialized from scratch. Based on benchmarks ran last night, we’ve reduced this time down to where the serializer can write all 50M+ blocks of Hive to a postgres database and initialize the associated table indexes in 7 hours (on a fast machine). That’s more than 2x faster than our previous time for this task. As impressive as those times are, we should still be able to dramatically reduce this time in many cases, by enabling an option for the serializer to filter out operations that aren’t interesting to the Hive apps hosted on a specific HAF server. For example, a standard social media application such as Hivemind doesn’t need to process most of the blockchain’s custom_json operations, so a lightweight hivemind node could configure its serializer to use a regular expression to capture only the operations it supports. # Hivemind (2nd layer applications + social media middleware) ### bug fixes Fix to community pagination with pinned posts: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/496 Only allow referencing permlinks that haven’t been deleted: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/494 Fixes related to improperly versioned package dependencies: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/498 https://gitlab.syncad.com/hive/hivemind/-/merge_requests/504 https://gitlab.syncad.com/hive/hivemind/-/merge_requests/506 Restore ctrl-c breaking and sync from block_log database with mock data (part of testing work for HAF): https://gitlab.syncad.com/hive/hivemind/-/merge_requests/508 ### Hivemind optimizations Improve query planning under postgres 12 (postgres 12 is shipped with Ubuntu 20, so we’re planning to make 12 the recommended version for HAF and hivemind): https://gitlab.syncad.com/hive/hivemind/-/merge_requests/505 Speedup of post-massive sync cleanup phase: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/507 Improved storage management during massive reputation data processing to speedup massive sync: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/509 After consulting with Hive dev apps to be sure no one used it, we are going to eliminate the “active” field from the response returned by some post-related Hive API calls. # Hive Application Framework (HAF) We’re currently examining alternatives for injecting “computed data” into a HAF database. As a specific example, currently hived computes “impacted accounts” which are a set of accounts affected by each blockchain operation. Many HAF apps will also need information about impacted accounts. In theory, there are three ways to handle this: 1) have the sql_serializer write the data computed by hived directly to the database, 2) re-use the code from hived in the form of a postgres c++ extension, and 3) have HAF apps recompute this data themselves. Option 3 seems pretty wasteful, so we’re mostly looking at options 1 and 2. Personally, I favor option 1, but we’re trying out option 2 now, to see how it works out. ### HAF-based Account history app We’ve completed our first example app for HAF. This app replaces the functionality of a hived account history node. This means that future Hive API nodes will be able to operate with a consensus hived node instead of needed to run a heavier weight hived node configured with the account history plugin. The app is located here: https://gitlab.syncad.com/hive/HAfAH/-/commits/develop Use of this app should also result in much more scalable and responsive account history API calls (currently these are some of the biggest bottlenecks in terms of API performance). I will probably have some benchmarks for this by next progress report. ### HAF-based Hivemind app We had to make further updates to hivemind to get it to work from tables similar to those used by HAF (we’d done this previously, but later changes to hivemind had to be accounted for). Yesterday we completed a successful massive sync with this new quasi-HAF version of hivemind. # What’s next? In the coming week we’ll be focused on the following tasks: * conversion of hivemind into a HAF app * testing and benchmarking of HAF account history app * continued tuning of HAF code based on above work * begin planning for HF26 See: 21st update of 2021 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Update on my work on Ledger App Hive</title><link href="https://hivedocs.info/news/nodejs/wallet/2021/08/16/update-on-my-work-on-ledger-app-hive.html" rel="alternate" type="text/html" title="Update on my work on Ledger App Hive" /><published>2021-08-16T16:12:51-07:00</published><updated>2021-08-16T16:12:51-07:00</updated><id>https://hivedocs.info/news/nodejs/wallet/2021/08/16/update-on-my-work-on-ledger-app-hive</id><content type="html" xml:base="https://hivedocs.info/news/nodejs/wallet/2021/08/16/update-on-my-work-on-ledger-app-hive.html">&lt;div id=&quot;content-engrave-update-on-my-work-on-ledger-app-hive&quot;&gt;![image.png](https://files.peakd.com/file/peakd-hive/engrave/Eo23B9HfFGSxgkzJckVtC8WSMeoEwfMcitDDJBZYJzaiVGqirgAwMZmEHpr2WcufBP6.png)
&lt;sup&gt;Image credit (@thepeakstudio)&lt;/sup&gt;

I'm continuing my work on Ledger App Hive and I am pretty confident it will be finished and submitted for official review soon.

Previous posts regarding Ledger app:
 * [Ledger App updated with HF25 operations](https://peakd.com/hive-139531/@engrave/ledger-app-updated-with-hf25-operations)
 * [JavaScript library for Ledger Nano S HIVE application
](https://peakd.com/hive-139531/@engrave/javascript-library-for-ledger-nano-s-hive-application)

# Source code

I decided to rewrite the app from scratch. The old codebase was a bit messy and it would be really hard to work on it. Sometimes it's just easier to start from scratch. I only adopted the ASN1 DER encoding, proposed by netuoso for communication between Ledger device and PC/mobile.

It's basically finished. The new codebase is built on top of the latest boilerplate (provided by Ledger) and utilizes their newest SDK. Thanks to this, I was able to write the code in a way that will work  on both Nano S and Nano X the same way (except for the fact that Nano X has a bigger display). 

**The new codebase compiles with 0 warnings and produces 0 bugs when tested with Clang Static Analyzer.** This is the requirement that needs to be met before submitting the app for official review.


```
make[1]: Leaving directory '/app'
scan-build: Removing directory '/app/output-scan-build/2021-08-16-224415-44-1' because it contains no reports.
scan-build: No bugs found.
```

# New user experience

New SDK allowed me to redesign the user experience. Now, you can loop through the entire transaction back and forward. The previous version allowed you to verify every transaction field only once and it was really easy to accept or reject the transaction by mistake. Ledger has also some functional/design requirements and the new version follows them.

And, as I already said, it's working on Nano X.
![Screenshot from 2021-08-17 00-39-42.png](https://files.peakd.com/file/peakd-hive/engrave/2424EvTL2WpTu7G3Fo1vV9Luv4gmvGNKXsSvoNZL4ChqCZQdfmHGUAWPLjEwNVPyZiSWw.png)

This is a view from the simulator because there is no way to sideload the unofficial app to Nano X.

# Unit tests

During the last two weeks, I put a lot of effort into unit testing the app (which is not only required by Ledger company but also very useful in the matter of stability and security). I covered most of my files with unit tests written in ANSI C, using `cmocka` framework. This will minimize potential problems, like buffer overflows/underflows etc which are really common on apps written in C.

![Screenshot from 2021-08-16 22-24-29.png](https://files.peakd.com/file/peakd-hive/engrave/23xLHzRhhbDYMswqNRr1ZJR4dT63qmLwWs6kMEnQ2YWkCnWQszYTbeBtNoq5JdyWYo3tV.png)

# Functional tests

Those need to be done yet, but I already prepared a Javascript library that is fully featured and can be used not only to communicate with the physical device but also with the simulator/debugger. The next step will be to create automated tests, to verify signatures for specified test transactions.

I'm already using it to broadcast transactions to our testnet. It shouldn't take long to finish this part.

![Screenshot from 2021-08-17 01-04-32.png](https://files.peakd.com/file/peakd-hive/engrave/Eo1vSpgX1pPc2PQ9xhLCuAa6rT1ZygYkijdBtEbyPnA6pjWm7ueHcR2mqdGezfTyKHn.png)

# Progress since the last update

 * ~~Make it compatible with a new SDK and 2.0 firmware version~~
 * ~~Make it working on both Nano S and Nano X devices~~
 * ~~Add Approve and Reject screens before signing the transaction~~
 * Prepare user guide and developer's documentation for Ledger Team
 * Develop a working version of the Hive companion app
 * Submit the app for the official review

We're close.

***

Follow me to be up to date with incoming development updates!

***

&lt;center&gt;

**Click on the image to vote for @engrave witness:**

[![banner_engrave 100.png](https://images.hive.blog/DQmUghvic5TCPPvVkB4iB7eXmgQ3RQ4L8jkVizvuCBW8RMT/banner_engrave%20100.png)](https://hivesigner.com/sign/account-witness-vote?witness=engrave&amp;amp;approve=1)


&lt;/center&gt;&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@engrave&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/update-on-my-work-on-ledger-app-hive&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-engrave-update-on-my-work-on-ledger-app-hive').html();
      const outputElem = $('#content-engrave-update-on-my-work-on-ledger-app-hive');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-engrave-update-on-my-work-on-ledger-app-hive {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-engrave-update-on-my-work-on-ledger-app-hive code {
    background: white;
  }
  #content-engrave-update-on-my-work-on-ledger-app-hive a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-engrave-update-on-my-work-on-ledger-app-hive a:hover {
    border-bottom: 0;
  }
  #content-engrave-update-on-my-work-on-ledger-app-hive h1 {
    font-size: 2.2em;
  }
  #content-engrave-update-on-my-work-on-ledger-app-hive h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-engrave-update-on-my-work-on-ledger-app-hive header small {
    color: #999;
    font-size: 50%;
  }
  #content-engrave-update-on-my-work-on-ledger-app-hive img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/hive-139531/@engrave/update-on-my-work-on-ledger-app-hive&quot;&gt;Update on my work on Ledger App Hive&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@engrave&quot;&gt;@engrave&lt;/a&gt;
&lt;/p&gt;</content><author><name>engrave</name></author><category term="news" /><category term="nodejs" /><category term="wallet" /><summary type="html">![image.png](https://files.peakd.com/file/peakd-hive/engrave/Eo23B9HfFGSxgkzJckVtC8WSMeoEwfMcitDDJBZYJzaiVGqirgAwMZmEHpr2WcufBP6.png) Image credit (@thepeakstudio) I'm continuing my work on Ledger App Hive and I am pretty confident it will be finished and submitted for official review soon. Previous posts regarding Ledger app: * [Ledger App updated with HF25 operations](https://peakd.com/hive-139531/@engrave/ledger-app-updated-with-hf25-operations) * [JavaScript library for Ledger Nano S HIVE application ](https://peakd.com/hive-139531/@engrave/javascript-library-for-ledger-nano-s-hive-application) # Source code I decided to rewrite the app from scratch. The old codebase was a bit messy and it would be really hard to work on it. Sometimes it's just easier to start from scratch. I only adopted the ASN1 DER encoding, proposed by netuoso for communication between Ledger device and PC/mobile. It's basically finished. The new codebase is built on top of the latest boilerplate (provided by Ledger) and utilizes their newest SDK. Thanks to this, I was able to write the code in a way that will work on both Nano S and Nano X the same way (except for the fact that Nano X has a bigger display). **The new codebase compiles with 0 warnings and produces 0 bugs when tested with Clang Static Analyzer.** This is the requirement that needs to be met before submitting the app for official review. ``` make[1]: Leaving directory '/app' scan-build: Removing directory '/app/output-scan-build/2021-08-16-224415-44-1' because it contains no reports. scan-build: No bugs found. ``` # New user experience New SDK allowed me to redesign the user experience. Now, you can loop through the entire transaction back and forward. The previous version allowed you to verify every transaction field only once and it was really easy to accept or reject the transaction by mistake. Ledger has also some functional/design requirements and the new version follows them. And, as I already said, it's working on Nano X. ![Screenshot from 2021-08-17 00-39-42.png](https://files.peakd.com/file/peakd-hive/engrave/2424EvTL2WpTu7G3Fo1vV9Luv4gmvGNKXsSvoNZL4ChqCZQdfmHGUAWPLjEwNVPyZiSWw.png) This is a view from the simulator because there is no way to sideload the unofficial app to Nano X. # Unit tests During the last two weeks, I put a lot of effort into unit testing the app (which is not only required by Ledger company but also very useful in the matter of stability and security). I covered most of my files with unit tests written in ANSI C, using `cmocka` framework. This will minimize potential problems, like buffer overflows/underflows etc which are really common on apps written in C. ![Screenshot from 2021-08-16 22-24-29.png](https://files.peakd.com/file/peakd-hive/engrave/23xLHzRhhbDYMswqNRr1ZJR4dT63qmLwWs6kMEnQ2YWkCnWQszYTbeBtNoq5JdyWYo3tV.png) # Functional tests Those need to be done yet, but I already prepared a Javascript library that is fully featured and can be used not only to communicate with the physical device but also with the simulator/debugger. The next step will be to create automated tests, to verify signatures for specified test transactions. I'm already using it to broadcast transactions to our testnet. It shouldn't take long to finish this part. ![Screenshot from 2021-08-17 01-04-32.png](https://files.peakd.com/file/peakd-hive/engrave/Eo1vSpgX1pPc2PQ9xhLCuAa6rT1ZygYkijdBtEbyPnA6pjWm7ueHcR2mqdGezfTyKHn.png) # Progress since the last update * ~~Make it compatible with a new SDK and 2.0 firmware version~~ * ~~Make it working on both Nano S and Nano X devices~~ * ~~Add Approve and Reject screens before signing the transaction~~ * Prepare user guide and developer's documentation for Ledger Team * Develop a working version of the Hive companion app * Submit the app for the official review We're close. *** Follow me to be up to date with incoming development updates! *** **Click on the image to vote for @engrave witness:** [![banner_engrave 100.png](https://images.hive.blog/DQmUghvic5TCPPvVkB4iB7eXmgQ3RQ4L8jkVizvuCBW8RMT/banner_engrave%20100.png)](https://hivesigner.com/sign/account-witness-vote?witness=engrave&amp;amp;approve=1) See: Update on my work on Ledger App Hive by @engrave</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://files.peakd.com/file/peakd-hive/engrave/Eo23B9HfFGSxgkzJckVtC8WSMeoEwfMcitDDJBZYJzaiVGqirgAwMZmEHpr2WcufBP6.png" /><media:content medium="image" url="https://files.peakd.com/file/peakd-hive/engrave/Eo23B9HfFGSxgkzJckVtC8WSMeoEwfMcitDDJBZYJzaiVGqirgAwMZmEHpr2WcufBP6.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Hive core developer meeting #27</title><link href="https://hivedocs.info/news/core/development/2021/08/15/hive-core-developer-meeting-27.html" rel="alternate" type="text/html" title="Hive core developer meeting #27" /><published>2021-08-15T08:47:39-07:00</published><updated>2021-08-15T08:47:39-07:00</updated><id>https://hivedocs.info/news/core/development/2021/08/15/hive-core-developer-meeting-27</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/08/15/hive-core-developer-meeting-27.html">&lt;div id=&quot;content-howo-hive-core-developer-meeting-27&quot;&gt;https://www.youtube.com/watch?v=8DFXZPK9mQ0
This meeting was pushed back a few times because most of the team was in the middle of developments or busy improving the api nodes/gitlab so there weren't much to sync on.

Meeting points:

# Dev sync 

It's better to listen to this one.

# Rc delegations and potential performance hog

RC delegations, is now pushed and ready to be reviewed by blocktrade's team. Should this review be positive, I'll probably spin up a small testnet for everyone to play with it.

There is one thing regarding rc delegations (that was also present in the RC pools implementation) that can be worrysome, we discuss some of the potential solutions to it. But we didn't go too much in depth as blocktrades team didn't see the code yet so it's hard for them to truly discuss the solutions. 

# HAF progress and communities migration to it
(some expected times + is it worth to wait for the migration to do some developments on it)

I working on communities after rc delegations, so since there was a plan to migrate hivemind to HAF, and as per this comment by @blocktrades: https://peakd.com/hive/@blocktrades/qxdi2b 

```
In my opinion, it would be a mistake to start this work [work on communities] before we complete HAF work. The next step in hivemind's evolution is to port it to HAF. After that, it should be no big deal for someone to enhance communities further.
```

I wanted to confirm the best way forward, ultimately after discussing we decided that porting hivemind onto HAF would not be a blocker to some development on communities right now.

# Gitlab issues

The gitlab instance where most of the code, issues etc suffered a failure and all the backups also failed, so we didn't lose code but some of the metadata (issues/merge requests/comments) from may to august were lost. If you submitted something in that timespan, make sure to resubmit it.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@howo&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/hive-core-developer-meeting-27&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-howo-hive-core-developer-meeting-27').html();
      const outputElem = $('#content-howo-hive-core-developer-meeting-27');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-howo-hive-core-developer-meeting-27 {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-howo-hive-core-developer-meeting-27 code {
    background: white;
  }
  #content-howo-hive-core-developer-meeting-27 a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-howo-hive-core-developer-meeting-27 a:hover {
    border-bottom: 0;
  }
  #content-howo-hive-core-developer-meeting-27 h1 {
    font-size: 2.2em;
  }
  #content-howo-hive-core-developer-meeting-27 h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-howo-hive-core-developer-meeting-27 header small {
    color: #999;
    font-size: 50%;
  }
  #content-howo-hive-core-developer-meeting-27 img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/core/@howo/hive-core-developer-meeting-27&quot;&gt;Hive core developer meeting #27&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@howo&quot;&gt;@howo&lt;/a&gt;
&lt;/p&gt;</content><author><name>howo</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">https://www.youtube.com/watch?v=8DFXZPK9mQ0 This meeting was pushed back a few times because most of the team was in the middle of developments or busy improving the api nodes/gitlab so there weren't much to sync on. Meeting points: # Dev sync It's better to listen to this one. # Rc delegations and potential performance hog RC delegations, is now pushed and ready to be reviewed by blocktrade's team. Should this review be positive, I'll probably spin up a small testnet for everyone to play with it. There is one thing regarding rc delegations (that was also present in the RC pools implementation) that can be worrysome, we discuss some of the potential solutions to it. But we didn't go too much in depth as blocktrades team didn't see the code yet so it's hard for them to truly discuss the solutions. # HAF progress and communities migration to it (some expected times + is it worth to wait for the migration to do some developments on it) I working on communities after rc delegations, so since there was a plan to migrate hivemind to HAF, and as per this comment by @blocktrades: https://peakd.com/hive/@blocktrades/qxdi2b ``` In my opinion, it would be a mistake to start this work [work on communities] before we complete HAF work. The next step in hivemind's evolution is to port it to HAF. After that, it should be no big deal for someone to enhance communities further. ``` I wanted to confirm the best way forward, ultimately after discussing we decided that porting hivemind onto HAF would not be a blocker to some development on communities right now. # Gitlab issues The gitlab instance where most of the code, issues etc suffered a failure and all the backups also failed, so we didn't lose code but some of the metadata (issues/merge requests/comments) from may to august were lost. If you submitted something in that timespan, make sure to resubmit it. See: Hive core developer meeting #27 by @howo</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hivedocs.info/assets/images/favicon.png" /><media:content medium="image" url="https://hivedocs.info/assets/images/favicon.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Reconfiguration notice for api.hive.blog API node infrastructure</title><link href="https://hivedocs.info/news/core/development/2021/08/02/reconfiguration-notice-for-api-hive-blog-api-node-infrastructure.html" rel="alternate" type="text/html" title="Reconfiguration notice for api.hive.blog API node infrastructure" /><published>2021-08-02T14:58:54-07:00</published><updated>2021-08-02T14:58:54-07:00</updated><id>https://hivedocs.info/news/core/development/2021/08/02/reconfiguration-notice-for-api-hive-blog-api-node-infrastructure</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/08/02/reconfiguration-notice-for-api-hive-blog-api-node-infrastructure.html">&lt;div id=&quot;content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure&quot;&gt;As I mentioned in my last post on development progress at BlockTrades, we’ve been asking Hive-based apps to change the way they broadcast transactions. Transactions are used to add user-generated operations such as voting, posting, transferring funds, delegating HP, etcetera  to the Hive blockchain. 

In this post I’m going to briefly describe why we’re requesting this change and also describe a change we’ve made to our API node infrastructure  to better handle the increased traffic from bots playing Splinterlands with “bad” API calls. This latter information will likely be interesting to other API node operators.

# Apps should  use broadcast_transaction call

Most Hive-based apps previously used the call `broadcast_transaction_synchronous` to broadcast transactions. This call, as the name implies, waits for a transaction to be included into the blockchain before it returns to the calling application. The problem with this call is that we’ve measured it takes about 3 seconds on average to complete on our API node infrastructure even under normal loading conditions. That’s a long time to a computer.

So if one of our hived nodes gets a lot of these calls, the calls keep all of that hived’s worker threads busy, effectively slowing down all API calls made to that hived server. 

By adding additional logging to one of our hived nodes, we were able to observe that even read-only API calls arriving at the loaded hived node could wait 2 seconds (or even more under very heavy loads) before they got assigned a worker thread to process them. A few days ago, this was noticeable as a slowdown on not only web sites that broadcast transactions such as hive.blog, ecency, and peakd, but even on read-only Hive-based sites (e.g. Hive block explorers such as https://hiveblocks.com).

The ideal solution to this problem is for apps to replace all these slow calls with the newer, faster `broadcast_transaction` call. This call doesn’t wait for the transaction to be included into the blockchain, and it completes on api.hive.blog within an average of 0.027 seconds (more than 100x faster than the synchronous version).

## Most hive apps have already migrated to the better API call

Most of the major Hive apps have moved to the faster broadcast call within the past few days, as app devs saw how much more responsive their app became with the new calls (i.e. they became faster than they ever were before, even when we had less loading on the Hive network). 

And I expect the few remaining big apps, such as hive.blog, to convert to using the new calls within the next couple of days (we have an engineer working now to fix one known issue preventing us from rolling out the latest version of condenser with the broadcast_transaction fix).

# Mitigating bad traffic from misbehaving bots

But despite movement of the major Hive apps to the faster call, we still see a lot of broadcast_transaction_synchronous traffic on our node being generated by presumably home-grown bots playing Splinterlands for their users. I suppose these bot devs will eventually fix their bots, but in the meantime, we have no easy way to contact them, so we’ve made a change to our API node infrastructure so that this “bad” traffic doesn’t impact the apps generating “good traffic”. 

We have redirected all incoming broadcast_transaction_synchronous traffic to a single hived node that only processes this type of API traffic, and all other API calls (including the “good” broadcast_transaction) are routed to our other hived nodes. This means that apps using the proper calls will not be slowed down by the bad traffic. And it will probably ultimately encourage the bad traffic generators to change their bots as well, although I’m not holding my breath for when that will happen.

# Add an extra consensus hived to manage broadcast_transaction_synchronous traffic
If other API nodes want to be capable of serving all the traffic from the Hive network right now, here’s the relatively easy way to do it:
* add one additional consensus (not account history) hived node to your server. Since consensus nodes don’t require much memory (around 4GB), the main additional resource cost is around 370GB of disk space for the additional block_log file. You only need a consensus node because it is only going to be processing broadcast calls and nothing else.
* Modify your jussi config file to redirect all below types of traffic to your consensus node:
** steemd.network_broadcast_api.broadcast_transaction_synchronous
** appbase.condenser_api.broadcast_transaction_synchronous
** appbase.network_broadcast_api.broadcast_transaction_synchronous

With the above steps, your regular account_history node(s) will process all the good traffic and the bad traffic will be offloaded to the light weight consensus node. Also, if you want to further improve quality of service for the bad traffic, you can increase the web-threads setting in your consensus node’s configuration file from the default value of 32 to 64 (or higher), at the cost of increased memory usage.

# Quick Update
We swapped over hive.blog to use the new calls earlier today and we swapped over wallet.hive.blog about an hour ago. The UX feels much more responsive now.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/reconfiguration-notice-for-api-hive-blog-api-node-infrastructure&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure').html();
      const outputElem = $('#content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure code {
    background: white;
  }
  #content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-reconfiguration-notice-for-api-hive-blog-api-node-infrastructure img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-102930/@blocktrades/reconfiguration-notice-for-api-hive-blog-api-node-infrastructure&quot;&gt;Reconfiguration notice for api.hive.blog API node infrastructure&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">As I mentioned in my last post on development progress at BlockTrades, we’ve been asking Hive-based apps to change the way they broadcast transactions. Transactions are used to add user-generated operations such as voting, posting, transferring funds, delegating HP, etcetera to the Hive blockchain. In this post I’m going to briefly describe why we’re requesting this change and also describe a change we’ve made to our API node infrastructure to better handle the increased traffic from bots playing Splinterlands with “bad” API calls. This latter information will likely be interesting to other API node operators. # Apps should use broadcast_transaction call Most Hive-based apps previously used the call `broadcast_transaction_synchronous` to broadcast transactions. This call, as the name implies, waits for a transaction to be included into the blockchain before it returns to the calling application. The problem with this call is that we’ve measured it takes about 3 seconds on average to complete on our API node infrastructure even under normal loading conditions. That’s a long time to a computer. So if one of our hived nodes gets a lot of these calls, the calls keep all of that hived’s worker threads busy, effectively slowing down all API calls made to that hived server. By adding additional logging to one of our hived nodes, we were able to observe that even read-only API calls arriving at the loaded hived node could wait 2 seconds (or even more under very heavy loads) before they got assigned a worker thread to process them. A few days ago, this was noticeable as a slowdown on not only web sites that broadcast transactions such as hive.blog, ecency, and peakd, but even on read-only Hive-based sites (e.g. Hive block explorers such as https://hiveblocks.com). The ideal solution to this problem is for apps to replace all these slow calls with the newer, faster `broadcast_transaction` call. This call doesn’t wait for the transaction to be included into the blockchain, and it completes on api.hive.blog within an average of 0.027 seconds (more than 100x faster than the synchronous version). ## Most hive apps have already migrated to the better API call Most of the major Hive apps have moved to the faster broadcast call within the past few days, as app devs saw how much more responsive their app became with the new calls (i.e. they became faster than they ever were before, even when we had less loading on the Hive network). And I expect the few remaining big apps, such as hive.blog, to convert to using the new calls within the next couple of days (we have an engineer working now to fix one known issue preventing us from rolling out the latest version of condenser with the broadcast_transaction fix). # Mitigating bad traffic from misbehaving bots But despite movement of the major Hive apps to the faster call, we still see a lot of broadcast_transaction_synchronous traffic on our node being generated by presumably home-grown bots playing Splinterlands for their users. I suppose these bot devs will eventually fix their bots, but in the meantime, we have no easy way to contact them, so we’ve made a change to our API node infrastructure so that this “bad” traffic doesn’t impact the apps generating “good traffic”. We have redirected all incoming broadcast_transaction_synchronous traffic to a single hived node that only processes this type of API traffic, and all other API calls (including the “good” broadcast_transaction) are routed to our other hived nodes. This means that apps using the proper calls will not be slowed down by the bad traffic. And it will probably ultimately encourage the bad traffic generators to change their bots as well, although I’m not holding my breath for when that will happen. # Add an extra consensus hived to manage broadcast_transaction_synchronous traffic If other API nodes want to be capable of serving all the traffic from the Hive network right now, here’s the relatively easy way to do it: * add one additional consensus (not account history) hived node to your server. Since consensus nodes don’t require much memory (around 4GB), the main additional resource cost is around 370GB of disk space for the additional block_log file. You only need a consensus node because it is only going to be processing broadcast calls and nothing else. * Modify your jussi config file to redirect all below types of traffic to your consensus node: ** steemd.network_broadcast_api.broadcast_transaction_synchronous ** appbase.condenser_api.broadcast_transaction_synchronous ** appbase.network_broadcast_api.broadcast_transaction_synchronous With the above steps, your regular account_history node(s) will process all the good traffic and the bad traffic will be offloaded to the light weight consensus node. Also, if you want to further improve quality of service for the bad traffic, you can increase the web-threads setting in your consensus node’s configuration file from the default value of 32 to 64 (or higher), at the cost of increased memory usage. # Quick Update We swapped over hive.blog to use the new calls earlier today and we swapped over wallet.hive.blog about an hour ago. The UX feels much more responsive now. See: Reconfiguration notice for api.hive.blog API node infrastructure by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hivedocs.info/assets/images/favicon.png" /><media:content medium="image" url="https://hivedocs.info/assets/images/favicon.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Everything can be faster on Hive - hive-js &amp;amp; dhive new changes</title><link href="https://hivedocs.info/howto/nodejs/2021/08/02/everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes.html" rel="alternate" type="text/html" title="Everything can be faster on Hive - hive-js &amp;amp; dhive new changes" /><published>2021-08-02T00:07:03-07:00</published><updated>2021-08-02T00:07:03-07:00</updated><id>https://hivedocs.info/howto/nodejs/2021/08/02/everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes</id><content type="html" xml:base="https://hivedocs.info/howto/nodejs/2021/08/02/everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes.html">&lt;div id=&quot;content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes&quot;&gt;&lt;center&gt;![alien-hive.jpg](https://files.peakd.com/file/peakd-hive/mahdiyari/23zS3SZnJwHE4qThdmvWRdRpLARpcFMm1Vq8LfDLqscHen53JFeNqq7EEGVfvhd2xcpTo.jpg)&lt;/center&gt;

Explaining why there were problems with RPC nodes and what was the problem and why you must update your application.

### Target audience
I explained everything as simple as I could. The main call is for the app owners to update their applications with the updated libraries.

After the recent timeout problems, it was necessary to take these steps and update the libraries.

I updated 2 major JS libraries, hive-js and dhive, to help resolve the timeout problems.

### The &quot;call&quot; method
The &quot;call&quot; method was a way of talking to hived (the hive blockchain's application). This method got deprecated and removed from hived with the appbase update (HF19?).

So what has happened since then? Well, we were still using the &quot;call&quot; method for most of our applications. But the &quot;call&quot; method is not known to hived anymore, so the requests have to go through a proxy called &quot;jussi&quot;. Jussi is installed in front of all the public RPC nodes.

Jussi takes the &quot;call&quot; method and translates it into the correct format then sends it to the hived. The problem is, you are adding an extra step for the API requests and it slows them down.

An example of the &quot;call&quot; method:
```
{&quot;id&quot;:0,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;method&quot;:&quot;call&quot;,&quot;params&quot;:[&quot;database_api&quot;,&quot;get_dynamic_global_properties&quot;,[]]}
```
Notice `&quot;method&quot;:&quot;call&quot;`. So jussi takes that and translates it into the new format:
```
{&quot;id&quot;:0,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;method&quot;:&quot;condenser_api.get_dynamic_global_properties&quot;,&quot;params&quot;: []}
```
See what happened? The user thinks the call was going to `database_api` but actually, jussi translates it into `condenser_api`.

To be honest, this is not the best way of doing things. The idea behind jussi was good but implementation was a mess.

***
### What is the &quot;new&quot; format?
The &quot;appbase&quot; style calls are the calls that are supported directly by hived and don't need a translation.

It's not new and has been there for years but we didn't bother updating our libraries to use the supported methods.

With switching to the appbase calls, every request to the RPC nodes becomes faster.


This is the first change to both hive-js and dhive libraries. They both were using the deprecated &quot;call&quot; method.
***
### You might not need jussi
Till now, hive-js wouldn't work with hived nodes until you added jussi in front of them. But with this update, hive-js will work with hived nodes. But remember, you will still need jussi for the hivemind calls. Also, there might be caching benefits in using jussi.
***
### Broadcasting transactions
This is the important part. There are two methods for broadcasting a transaction.
1. `broadcast_transaction_synchronous`
2. `broadcast_transaction`

The first method sends the transaction to the RPC node, the RPC node validates the transaction, then waits for the transaction to be included in a block, then sends back a response including the transaction id and block number.

The second method sends the transaction to the RPC node, the RPC node validates the transaction, then sends an empty success response back.

In the second method, the library has to generate the transaction id locally. Which is way too much faster. In this method, the only way for a transaction to fail is expiration or micro forking maybe. So in most cases, it's good enough.

Of course, hive-js and dhive both were using the first method. When many people connect to the RPC node and wait there, it becomes problematic. The best way for handling huge traffic is to release a connection as soon as possible so other connections can be made.

Both libraries now use the second method for broadcasting transactions. You may already notice its huge impact on the speed of your votes and comments.

Peakd is already updated. Keychain is waiting for google approval and the mobile version will be soon updated. Some apps are catching up like Hivesigner, and Ecency. There was a problem updating hive.blog and that will be also updated.

***
### Why bother updating
- Everything becomes faster for the end-user
- RPC nodes don't suffer overloading
- Apps become future proof
- It's easy as running a command

***
### How to update your apps
**hvie-js v2.0.1** - [Release notes](https://gitlab.syncad.com/hive/hive-js/-/releases)
Used in hive.blog, peakd, keychain, and many other apps.
```
npm install @hiveio/hive-js@latest --save
```
The minified JS for browsers:
```
https://cdn.jsdelivr.net/npm/@hiveio/hive-js@2/dist/hive.min.js
```
***
**dhive v1.0.0** - [Release notes](https://gitlab.syncad.com/hive/dhive/-/releases)
Used in ecency (mobile &amp;amp; web) and many other apps.
```
npm install @hiveio/dhive@latest --save
```
The minified JS for browsers:
```
https://cdn.jsdelivr.net/npm/@hiveio/dhive@1/dist/dhive.js
```
***
**hive-tx v4.0.0** - [Release notes](https://github.com/mahdiyari/hive-tx-js/releases)
Used in keychain mobile and other apps.

hive-tx already is using the appbase methods and there is a function called `.broadcastNoResult()` for the fast transaction broadcasting. But I did update hive-tx so now the `.broadcast()` function is also updated.
```
npm install hive-tx@latest --save
```
The minified JS for browsers:
```
https://cdn.jsdelivr.net/npm/hive-tx@4/dist/hive-tx.min.js
```


***
Note:
@foxon and @emrebeyler confirmed that `beem` doesn't need an update.
***

### Scaling problems?
The recent problem doesn't seem to be a scaling problem. It's more related to the libraries and apps using the deprecated methods.

The recent library changes seem to be solving most of the problems. Although there are investigations still going on by @blocktrades regarding the timeout problems.

Hive blockchain in theory can handle much more activity than this. Some say like 1,000 transactions per second. But there might be minor problems because those scenarios are not actually tested.
***

### Final words
I'm sure all the problems will be solved and we will see much better growth. I'm bullish on Hive and will continue my contributions as far as I can.

Thanks to the other devs who updated their applications on time to help with the problem.

***
&lt;sub&gt;Image source: pixabay.com&lt;/sub&gt;&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@mahdiyari&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes').html();
      const outputElem = $('#content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes code {
    background: white;
  }
  #content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes a:hover {
    border-bottom: 0;
  }
  #content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes h1 {
    font-size: 2.2em;
  }
  #content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes header small {
    color: #999;
    font-size: 50%;
  }
  #content-mahdiyari-everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/hive-139531/@mahdiyari/everything-can-be-faster-on-hive-hive-js-and-dhive-new-changes&quot;&gt;Everything can be faster on Hive - hive-js &amp;amp; dhive new changes&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@mahdiyari&quot;&gt;@mahdiyari&lt;/a&gt;
&lt;/p&gt;</content><author><name>mahdiyari</name></author><category term="howto" /><category term="nodejs" /><summary type="html">![alien-hive.jpg](https://files.peakd.com/file/peakd-hive/mahdiyari/23zS3SZnJwHE4qThdmvWRdRpLARpcFMm1Vq8LfDLqscHen53JFeNqq7EEGVfvhd2xcpTo.jpg) Explaining why there were problems with RPC nodes and what was the problem and why you must update your application. ### Target audience I explained everything as simple as I could. The main call is for the app owners to update their applications with the updated libraries. After the recent timeout problems, it was necessary to take these steps and update the libraries. I updated 2 major JS libraries, hive-js and dhive, to help resolve the timeout problems. ### The &quot;call&quot; method The &quot;call&quot; method was a way of talking to hived (the hive blockchain's application). This method got deprecated and removed from hived with the appbase update (HF19?). So what has happened since then? Well, we were still using the &quot;call&quot; method for most of our applications. But the &quot;call&quot; method is not known to hived anymore, so the requests have to go through a proxy called &quot;jussi&quot;. Jussi is installed in front of all the public RPC nodes. Jussi takes the &quot;call&quot; method and translates it into the correct format then sends it to the hived. The problem is, you are adding an extra step for the API requests and it slows them down. An example of the &quot;call&quot; method: ``` {&quot;id&quot;:0,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;method&quot;:&quot;call&quot;,&quot;params&quot;:[&quot;database_api&quot;,&quot;get_dynamic_global_properties&quot;,[]]} ``` Notice `&quot;method&quot;:&quot;call&quot;`. So jussi takes that and translates it into the new format: ``` {&quot;id&quot;:0,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;method&quot;:&quot;condenser_api.get_dynamic_global_properties&quot;,&quot;params&quot;: []} ``` See what happened? The user thinks the call was going to `database_api` but actually, jussi translates it into `condenser_api`. To be honest, this is not the best way of doing things. The idea behind jussi was good but implementation was a mess. *** ### What is the &quot;new&quot; format? The &quot;appbase&quot; style calls are the calls that are supported directly by hived and don't need a translation. It's not new and has been there for years but we didn't bother updating our libraries to use the supported methods. With switching to the appbase calls, every request to the RPC nodes becomes faster. This is the first change to both hive-js and dhive libraries. They both were using the deprecated &quot;call&quot; method. *** ### You might not need jussi Till now, hive-js wouldn't work with hived nodes until you added jussi in front of them. But with this update, hive-js will work with hived nodes. But remember, you will still need jussi for the hivemind calls. Also, there might be caching benefits in using jussi. *** ### Broadcasting transactions This is the important part. There are two methods for broadcasting a transaction. 1. `broadcast_transaction_synchronous` 2. `broadcast_transaction` The first method sends the transaction to the RPC node, the RPC node validates the transaction, then waits for the transaction to be included in a block, then sends back a response including the transaction id and block number. The second method sends the transaction to the RPC node, the RPC node validates the transaction, then sends an empty success response back. In the second method, the library has to generate the transaction id locally. Which is way too much faster. In this method, the only way for a transaction to fail is expiration or micro forking maybe. So in most cases, it's good enough. Of course, hive-js and dhive both were using the first method. When many people connect to the RPC node and wait there, it becomes problematic. The best way for handling huge traffic is to release a connection as soon as possible so other connections can be made. Both libraries now use the second method for broadcasting transactions. You may already notice its huge impact on the speed of your votes and comments. Peakd is already updated. Keychain is waiting for google approval and the mobile version will be soon updated. Some apps are catching up like Hivesigner, and Ecency. There was a problem updating hive.blog and that will be also updated. *** ### Why bother updating - Everything becomes faster for the end-user - RPC nodes don't suffer overloading - Apps become future proof - It's easy as running a command *** ### How to update your apps **hvie-js v2.0.1** - [Release notes](https://gitlab.syncad.com/hive/hive-js/-/releases) Used in hive.blog, peakd, keychain, and many other apps. ``` npm install @hiveio/hive-js@latest --save ``` The minified JS for browsers: ``` https://cdn.jsdelivr.net/npm/@hiveio/hive-js@2/dist/hive.min.js ``` *** **dhive v1.0.0** - [Release notes](https://gitlab.syncad.com/hive/dhive/-/releases) Used in ecency (mobile &amp;amp; web) and many other apps. ``` npm install @hiveio/dhive@latest --save ``` The minified JS for browsers: ``` https://cdn.jsdelivr.net/npm/@hiveio/dhive@1/dist/dhive.js ``` *** **hive-tx v4.0.0** - [Release notes](https://github.com/mahdiyari/hive-tx-js/releases) Used in keychain mobile and other apps. hive-tx already is using the appbase methods and there is a function called `.broadcastNoResult()` for the fast transaction broadcasting. But I did update hive-tx so now the `.broadcast()` function is also updated. ``` npm install hive-tx@latest --save ``` The minified JS for browsers: ``` https://cdn.jsdelivr.net/npm/hive-tx@4/dist/hive-tx.min.js ``` *** Note: @foxon and @emrebeyler confirmed that `beem` doesn't need an update. *** ### Scaling problems? The recent problem doesn't seem to be a scaling problem. It's more related to the libraries and apps using the deprecated methods. The recent library changes seem to be solving most of the problems. Although there are investigations still going on by @blocktrades regarding the timeout problems. Hive blockchain in theory can handle much more activity than this. Some say like 1,000 transactions per second. But there might be minor problems because those scenarios are not actually tested. *** ### Final words I'm sure all the problems will be solved and we will see much better growth. I'm bullish on Hive and will continue my contributions as far as I can. Thanks to the other devs who updated their applications on time to help with the problem. *** Image source: pixabay.com See: Everything can be faster on Hive - hive-js &amp;amp; dhive new changes by @mahdiyari</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://files.peakd.com/file/peakd-hive/mahdiyari/23zS3SZnJwHE4qThdmvWRdRpLARpcFMm1Vq8LfDLqscHen53JFeNqq7EEGVfvhd2xcpTo.jpg" /><media:content medium="image" url="https://files.peakd.com/file/peakd-hive/mahdiyari/23zS3SZnJwHE4qThdmvWRdRpLARpcFMm1Vq8LfDLqscHen53JFeNqq7EEGVfvhd2xcpTo.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>