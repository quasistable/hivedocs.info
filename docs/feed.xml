<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://hivedocs.info/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hivedocs.info/" rel="alternate" type="text/html" /><updated>2021-12-08T11:22:42-08:00</updated><id>https://hivedocs.info/feed.xml</id><title type="html">Hive Chain Documentation</title><subtitle>Your resource for various levels of Hive Documentation.</subtitle><author><name>site curated by: @inertia</name></author><entry><title type="html">Hive core dev meeting #30</title><link href="https://hivedocs.info/news/core/development/2021/12/06/hive-core-dev-meeting-30-r3q9qh.html" rel="alternate" type="text/html" title="Hive core dev meeting #30" /><published>2021-12-06T20:27:54-08:00</published><updated>2021-12-06T20:27:54-08:00</updated><id>https://hivedocs.info/news/core/development/2021/12/06/hive-core-dev-meeting-30-r3q9qh</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/12/06/hive-core-dev-meeting-30-r3q9qh.html">&lt;div id=&quot;content-howo-hive-core-dev-meeting-30-r3q9qh&quot;&gt;https://www.youtube.com/watch?v=9r1Sqhgd2_0

Quite a short meeting this time !

# Meeting tl;dr:

## Dev sync

It's better to listen to that one

## rosetta api (who, how)

If you're not familiar with it coinbase requires every blockchain to provide them with a specific api implementation defined here https://www.rosetta-api.org for them to list the token.
@blocktrades' team will take a look in the following days on what it would take to implement it as a hived plugin.

## Update the unit testing vest to TESTS ratio 

It makes it hard to build unit tests and be sure about the results when comparing such high numbers, and they are so far from reality that the testing doesn't really reflect the real world.

Overall we'll look into it but it's on a longer time frame. for now the focus is on hf26

## Restore witness automatic deactivation when missing blocks

Why not, but we want to improve on the existing logic and we don't have the bandwidth to do it for hf26. &lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@howo&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/hive-core-dev-meeting-30-r3q9qh&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-howo-hive-core-dev-meeting-30-r3q9qh').html();
      const outputElem = $('#content-howo-hive-core-dev-meeting-30-r3q9qh');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-howo-hive-core-dev-meeting-30-r3q9qh {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-howo-hive-core-dev-meeting-30-r3q9qh code {
    background: white;
  }
  #content-howo-hive-core-dev-meeting-30-r3q9qh a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-howo-hive-core-dev-meeting-30-r3q9qh a:hover {
    border-bottom: 0;
  }
  #content-howo-hive-core-dev-meeting-30-r3q9qh h1 {
    font-size: 2.2em;
  }
  #content-howo-hive-core-dev-meeting-30-r3q9qh h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-howo-hive-core-dev-meeting-30-r3q9qh header small {
    color: #999;
    font-size: 50%;
  }
  #content-howo-hive-core-dev-meeting-30-r3q9qh img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/core/@howo/hive-core-dev-meeting-30-r3q9qh&quot;&gt;Hive core dev meeting #30&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@howo&quot;&gt;@howo&lt;/a&gt;
&lt;/p&gt;</content><author><name>howo</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">https://www.youtube.com/watch?v=9r1Sqhgd2_0 Quite a short meeting this time ! # Meeting tl;dr: ## Dev sync It's better to listen to that one ## rosetta api (who, how) If you're not familiar with it coinbase requires every blockchain to provide them with a specific api implementation defined here https://www.rosetta-api.org for them to list the token. @blocktrades' team will take a look in the following days on what it would take to implement it as a hived plugin. ## Update the unit testing vest to TESTS ratio It makes it hard to build unit tests and be sure about the results when comparing such high numbers, and they are so far from reality that the testing doesn't really reflect the real world. Overall we'll look into it but it's on a longer time frame. for now the focus is on hf26 ## Restore witness automatic deactivation when missing blocks Why not, but we want to improve on the existing logic and we don't have the bandwidth to do it for hf26. See: Hive core dev meeting #30 by @howo</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hivedocs.info/assets/images/favicon.png" /><media:content medium="image" url="https://hivedocs.info/assets/images/favicon.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">31st update of 2021 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2021/12/01/31st-update-of-2021-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="31st update of 2021 on BlockTrades work on Hive software" /><published>2021-12-01T23:07:54-08:00</published><updated>2021-12-01T23:07:54-08:00</updated><id>https://hivedocs.info/news/core/development/2021/12/01/31st-update-of-2021-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/12/01/31st-update-of-2021-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-31st-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)
Below is a list of some of the Hive-related programming issues worked on by BlockTrades team during the past work period. I haven’t had time to check in with all our Hive developers this week, so this report will just focus on the devs I’ve been working with directly, and I’ll report on work done by other devs in my next report.

# Hive Application Framework: framework for building robust and scalable Hive apps

While running benchmarks of the HAF sql_serializer plugin over the past few weeks, I noticed that while the sql_serializer indexed most of the blocks very quickly, its performance was considerably slower once it dropped out of “massive sync” mode (where many blocks are processed simultaneously) into “single block” live sync mode. It also appeared that the sql_serializer was dropping out of massive sync mode earlier than it should, which could result in hundreds of thousands or even a million or more blocks being processed in this slower single block mode.

Benchmarks this week showed that the serializer can only process about 6.7 blocks/second on average in live sync mode, whereas in massive sync mode, it can process about 2300 blocks per second.


#### Live sync will always be slower than massive sync, but probably we can do better

Slower performance was expected in live sync mode, because this mode requires the sql tables to maintain several indexes and these indexes slow down the rate at which data can be added to the tables, but the magnitude of the slowdown was surprising (to me at least). 

So one of the first things we’re looking at to improve this situation is optimizing live sync performance. I believe there’s a good chance that we can improve the speed of live sync mode considerably still, because the live sync coding so far was only focused on proper functional behavior.

But even if we get a 4x speedup in live sync performance, it is still much slower than massive sync mode, so we also need to reduce the amount of old blocks that get processed in live sync mode.

### Performance impacts of swapping to live sync mode too early

As a real-world measurement of the impact of this, we replayed a hived with a block_log that was 6.5 days old, so it was missing about 189K blocks, and it only took 7.15 hours to process the 59M blocks in the block log. But it took another 10 hours to process the remaining 189K new blocks that it received from the peer-to-peer network that were missing from the block_log (plus it had to process new blocks that got created while processing the old blocks). 

We can envision an impractical “worst case” scenario where we sync a hived with an empty block_log with the sql_serializer enabled. This would currently take 59M blocks / 6.7 blocks/s / 3600 seconds/hr / 24 hrs/day = 101 days! I say this is an impractical scenario, because this can be worked around by first performing a normal sync of the hived node without the sql_serializer (which takes about 14 hours), then replaying with the sql_serializer with a fully filled block_log (so add another 7.5 hrs). But it does demonstrate one motivation for improving performance of the serializer with regard to live sync mode.

### Why does sql_serializer switch from massive sync to live sync so early?

Originally I believed that this transition from massive sync to live sync was accidentally happening too early because of an incorrect implementation of the `psql_index_threshold` setting. This setting helps hived decide whether it should start in massive sync mode or live sync mode, but I thought it might also be telling hived when to change from massive sync mode to live sync mode, so I expected we could fix the problem very easily and this issue was created to address the problem: https://gitlab.syncad.com/hive/haf/-/issues/9

But after further discussions this week with the devs working on the sql_serializer, it turns out that this flag wasn’t the problem. 

The real reason the sql_serializer was switching from massive_sync to live sync mode was because massive_sync mode can only be used for processing irreversible blocks (blocks that can’t be removed from the blockchain due to a fork), so as soon as the serializer was no longer sure that a block wasn’t irreversible, it swapped to live sync mode. 

The only way that the serializer knows a block is irreversible is if the block is in the block_log. So it first processes all the blocks in the block_log in massive sync mode, then switches to live sync mode to process all blocks it receives from the P2P network (this includes old blocks that were generated since hived was last running, plus new blocks that get generated while hived is processing old blocks).

So, ultimately the problem is that the serializer is processing new blocks as soon as it receives them from the P2P network, but these blocks only get marked as irreversible and added to the block_log after they are confirmed by later blocks received via the P2P network.

### How to stay in massive sync mode longer?

I’ve proposed a tentative solution to this problem that we’ll be trying to implement in the coming week: the serializer will continue to process blocks as they are received from the P2P network (this is important because the serializer makes use of information that must be computed at this time), but the resulting data will not be immediately sent to the SQL database, instead the data for the blocks will be stored in a queue in hived.

During massive sync mode, a task will be stepping through the blocks in the block_log and dispatching the associated SQL statements in the serializer queue. The serializer will stay in massive sync mode until it determines that the hived node has synced to within one minute of the head block of the chain AND all the serializer data for all blocks in the block_log has been sent to the SQL database, then it will switch to live sync mode. Switching to live sync mode currently takes about 30 minutes to build all the associated table indexes. Then the serializer will need to flush the queue of all blocks that have built up during the building of the table indexes. Finally, once the queue is flushed, new blocks from the P2P network can be sent immediately to the database, with no need to first store them in the local hived queue.


### Another possible improvement: make sql_serializer use non-blocking writes
Based on the benchmarks I ran, I believe that currently the sql_serializer writes data to the database as a blocking call (in other words, it waits for the SQL server to reply back that the block data has been written to the database before it allows hived to process another block). 

Using a blocking call ensures that hived’s state stays in sync with the state of the HAF database, but this comes at the cost of less parallelism, which means processing each block takes longer, and it also places additional strain on hived in a critical execution path (during the time that hived is processing a block, hived can’t safely do much else, so the block writing time should be kept as short as possible to lower the hardware requirements needed to operate a hived node).

To avoid this problem, we will take a look at converting the database write to a non-blocking call and only block if and when the queue of unprocessed non-blocking calls gets too large. This will make replays with the sql_serializer faster and will also reduce the amount of time that hived is blocked and unable to process new blocks from the P2P network.

The only potential scenario I can think at the moment where using using a non-blocking call could cause a problem would be the case where the postgres server failed to write a block to the database (for example, if the database storage device filled up). With a non-blocking call, hived would continue to process blocks for a while instead of immediately shutting down, and hived’s state would become out of sync with the state of the HAF database. 

Postgres servers tend to be very reliable, so this is an unlikely scenario, but even if it happens, in the worst case, it would only require the state of hived and the HAF database to be cleared and a replay from block 0. And more likely, a system admin would just re-initialize hived and HAF database using a relatively recent snapshot and a HAF database restore file, and then they could quickly be synced up to the head block again.


### Two new programmers working on balance_tracker app

We have  two new Hive devs (a python programmer and a web developer) working on creating an API and web interface for balance_tracker (an example HAF app) as an introduction to the Hive programming environment.


### Investigating defining HAF APIs with PostgREST web server

In hivemind and in our previous HAF apps, we’ve been using a Python-based jsonrpc server that translate the RPC calls to manually written python functions that then make SQL queries to the HAF database. This works pretty well and we recently optimized it to perform better when heavily loaded, but there is a certain amount of boilerplate Python code that must be written with this methodology, which requires someone familiar with Python (not a difficult requirement) and also increases the chance for errors during the intermediate translation of data back and forth between Python and SQL.

To make this task a little more interesting, Bartek suggested we investigate an alternative way to define the API interface to a HAF app using a web server called [PostgREST](https://postgrest.org) to replace the python-based server.

This approach could be very interesting for experienced SQL developers, I think. Instead of writing Python code, APIs are defined directly in SQL as table views and/or stored procedures. API clients can then make REST calls to the PostgREST server and it converts the REST calls to equivalent SQL queries. So far this approach looks like a promising alternative for SQL programmers, and it will also be interesting to compare the performance between the PostgREST web server and the Python-based one.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/31st-update-of-2021-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-31st-update-of-2021-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-31st-update-of-2021-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-31st-update-of-2021-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-31st-update-of-2021-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-31st-update-of-2021-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-31st-update-of-2021-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-31st-update-of-2021-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-31st-update-of-2021-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-31st-update-of-2021-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-31st-update-of-2021-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/31st-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;31st update of 2021 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below is a list of some of the Hive-related programming issues worked on by BlockTrades team during the past work period. I haven’t had time to check in with all our Hive developers this week, so this report will just focus on the devs I’ve been working with directly, and I’ll report on work done by other devs in my next report. # Hive Application Framework: framework for building robust and scalable Hive apps While running benchmarks of the HAF sql_serializer plugin over the past few weeks, I noticed that while the sql_serializer indexed most of the blocks very quickly, its performance was considerably slower once it dropped out of “massive sync” mode (where many blocks are processed simultaneously) into “single block” live sync mode. It also appeared that the sql_serializer was dropping out of massive sync mode earlier than it should, which could result in hundreds of thousands or even a million or more blocks being processed in this slower single block mode. Benchmarks this week showed that the serializer can only process about 6.7 blocks/second on average in live sync mode, whereas in massive sync mode, it can process about 2300 blocks per second. #### Live sync will always be slower than massive sync, but probably we can do better Slower performance was expected in live sync mode, because this mode requires the sql tables to maintain several indexes and these indexes slow down the rate at which data can be added to the tables, but the magnitude of the slowdown was surprising (to me at least). So one of the first things we’re looking at to improve this situation is optimizing live sync performance. I believe there’s a good chance that we can improve the speed of live sync mode considerably still, because the live sync coding so far was only focused on proper functional behavior. But even if we get a 4x speedup in live sync performance, it is still much slower than massive sync mode, so we also need to reduce the amount of old blocks that get processed in live sync mode. ### Performance impacts of swapping to live sync mode too early As a real-world measurement of the impact of this, we replayed a hived with a block_log that was 6.5 days old, so it was missing about 189K blocks, and it only took 7.15 hours to process the 59M blocks in the block log. But it took another 10 hours to process the remaining 189K new blocks that it received from the peer-to-peer network that were missing from the block_log (plus it had to process new blocks that got created while processing the old blocks). We can envision an impractical “worst case” scenario where we sync a hived with an empty block_log with the sql_serializer enabled. This would currently take 59M blocks / 6.7 blocks/s / 3600 seconds/hr / 24 hrs/day = 101 days! I say this is an impractical scenario, because this can be worked around by first performing a normal sync of the hived node without the sql_serializer (which takes about 14 hours), then replaying with the sql_serializer with a fully filled block_log (so add another 7.5 hrs). But it does demonstrate one motivation for improving performance of the serializer with regard to live sync mode. ### Why does sql_serializer switch from massive sync to live sync so early? Originally I believed that this transition from massive sync to live sync was accidentally happening too early because of an incorrect implementation of the `psql_index_threshold` setting. This setting helps hived decide whether it should start in massive sync mode or live sync mode, but I thought it might also be telling hived when to change from massive sync mode to live sync mode, so I expected we could fix the problem very easily and this issue was created to address the problem: https://gitlab.syncad.com/hive/haf/-/issues/9 But after further discussions this week with the devs working on the sql_serializer, it turns out that this flag wasn’t the problem. The real reason the sql_serializer was switching from massive_sync to live sync mode was because massive_sync mode can only be used for processing irreversible blocks (blocks that can’t be removed from the blockchain due to a fork), so as soon as the serializer was no longer sure that a block wasn’t irreversible, it swapped to live sync mode. The only way that the serializer knows a block is irreversible is if the block is in the block_log. So it first processes all the blocks in the block_log in massive sync mode, then switches to live sync mode to process all blocks it receives from the P2P network (this includes old blocks that were generated since hived was last running, plus new blocks that get generated while hived is processing old blocks). So, ultimately the problem is that the serializer is processing new blocks as soon as it receives them from the P2P network, but these blocks only get marked as irreversible and added to the block_log after they are confirmed by later blocks received via the P2P network. ### How to stay in massive sync mode longer? I’ve proposed a tentative solution to this problem that we’ll be trying to implement in the coming week: the serializer will continue to process blocks as they are received from the P2P network (this is important because the serializer makes use of information that must be computed at this time), but the resulting data will not be immediately sent to the SQL database, instead the data for the blocks will be stored in a queue in hived. During massive sync mode, a task will be stepping through the blocks in the block_log and dispatching the associated SQL statements in the serializer queue. The serializer will stay in massive sync mode until it determines that the hived node has synced to within one minute of the head block of the chain AND all the serializer data for all blocks in the block_log has been sent to the SQL database, then it will switch to live sync mode. Switching to live sync mode currently takes about 30 minutes to build all the associated table indexes. Then the serializer will need to flush the queue of all blocks that have built up during the building of the table indexes. Finally, once the queue is flushed, new blocks from the P2P network can be sent immediately to the database, with no need to first store them in the local hived queue. ### Another possible improvement: make sql_serializer use non-blocking writes Based on the benchmarks I ran, I believe that currently the sql_serializer writes data to the database as a blocking call (in other words, it waits for the SQL server to reply back that the block data has been written to the database before it allows hived to process another block). Using a blocking call ensures that hived’s state stays in sync with the state of the HAF database, but this comes at the cost of less parallelism, which means processing each block takes longer, and it also places additional strain on hived in a critical execution path (during the time that hived is processing a block, hived can’t safely do much else, so the block writing time should be kept as short as possible to lower the hardware requirements needed to operate a hived node). To avoid this problem, we will take a look at converting the database write to a non-blocking call and only block if and when the queue of unprocessed non-blocking calls gets too large. This will make replays with the sql_serializer faster and will also reduce the amount of time that hived is blocked and unable to process new blocks from the P2P network. The only potential scenario I can think at the moment where using using a non-blocking call could cause a problem would be the case where the postgres server failed to write a block to the database (for example, if the database storage device filled up). With a non-blocking call, hived would continue to process blocks for a while instead of immediately shutting down, and hived’s state would become out of sync with the state of the HAF database. Postgres servers tend to be very reliable, so this is an unlikely scenario, but even if it happens, in the worst case, it would only require the state of hived and the HAF database to be cleared and a replay from block 0. And more likely, a system admin would just re-initialize hived and HAF database using a relatively recent snapshot and a HAF database restore file, and then they could quickly be synced up to the head block again. ### Two new programmers working on balance_tracker app We have two new Hive devs (a python programmer and a web developer) working on creating an API and web interface for balance_tracker (an example HAF app) as an introduction to the Hive programming environment. ### Investigating defining HAF APIs with PostgREST web server In hivemind and in our previous HAF apps, we’ve been using a Python-based jsonrpc server that translate the RPC calls to manually written python functions that then make SQL queries to the HAF database. This works pretty well and we recently optimized it to perform better when heavily loaded, but there is a certain amount of boilerplate Python code that must be written with this methodology, which requires someone familiar with Python (not a difficult requirement) and also increases the chance for errors during the intermediate translation of data back and forth between Python and SQL. To make this task a little more interesting, Bartek suggested we investigate an alternative way to define the API interface to a HAF app using a web server called [PostgREST](https://postgrest.org) to replace the python-based server. This approach could be very interesting for experienced SQL developers, I think. Instead of writing Python code, APIs are defined directly in SQL as table views and/or stored procedures. API clients can then make REST calls to the PostgREST server and it converts the REST calls to equivalent SQL queries. So far this approach looks like a promising alternative for SQL programmers, and it will also be interesting to compare the performance between the PostgREST web server and the Python-based one. See: 31st update of 2021 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">30th update of 2021 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2021/11/23/30th-update-of-2021-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="30th update of 2021 on BlockTrades work on Hive software" /><published>2021-11-23T17:00:27-08:00</published><updated>2021-11-23T17:00:27-08:00</updated><id>https://hivedocs.info/news/core/development/2021/11/23/30th-update-of-2021-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/11/23/30th-update-of-2021-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-30th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)
Below is a list of some of the Hive-related programming issues worked on by BlockTrades team during the past week:

# Hived work (blockchain node software)

### Improving resource credit (RC) plugin

We’re continuing to experiment with tweaking the  RC credit plugin to more accurately account for real world costs. Because a lot of the work involves long-running reindexing tests whenever we make a change, I expect this task to continue for a couple of weeks at least.

### Miscellaneous work on hived

We upgraded the CI build/runtime/test images to Ubuntu 20.04 as part of our general move of hive development from U18 to U20.

We completed an analysis of some strange log messages during periods of high blockchain activity. These messages claimed some transactions were being included into a series of consecutive blocks when the transacting account didn’t have enough RC to pay for them. However, we could see on block explorers that these transactions were not actually included, so we decided to dig in to find out what was going on, in case there was some real problem. The ultimate issue was pretty complicated, but not problematic. For those want to see what kinds of problems programmers have to deal with, you can read the full result of this investigation here:
https://gitlab.syncad.com/hive/hive/-/issues/197

We also continued work on improving testtools and tests for hived. This change impacts anyone who is writing tests for hived:
https://gitlab.syncad.com/hive/hive/-/merge_requests/313


# Hive Application Framework: framework for building robust and scalable Hive apps

Most of our work this week continues to be HAF-related and we had a lot of good news in the past few days on the performance front.

### New tables generated by sql_serializer for HAF apps reduced hafah sync time by 45%

We finished optimizing the latest version of sql_serializer that writes two new tables to the database to indicate what accounts exist (hive.accounts table) and what accounts are affected by which blockchain operations (hive.account_operations). 

After optimization, writing these tables only added 10% to previous time required to generate a HAF database (in prototype form it took twice as long), which was really good news for us, because these two tables eliminate the need to have to a reindex/sync app in the HAF account history app (Hafah).

To put this in perspective, previously to reindex Hafah required two steps:
* Run sql_serializer to generate HAF database to the head block (24499 seconds)
* Run hafah reindex to generate tables needed by Hafah (23942 seconds)

So the total time to process 49M blocks was 48441 seconds = 13.5 hours

With the new version of sql_serializer, there is no need for the second step. The new version of sql_serializer runs a little longer (26738 seconds), but as there is noHafah sync step, the total time was reduced from 13.5 hours to7.3 hours (45% faster).

### Space consumed by HAF database tables at 59M blocks

hive.blocks                            | 7135 MB    |
hive.operations                    | 969 GB     |
hive.transactions                  | 408 GB     |
hive.transactions_multisig  | 438 MB     |
hive.accounts                         | 108 MB     |
hive.account_operations      | 190 GB     |

The HAF database above occupies 2180GB after adding in table indexes and other overhead.

The last two tables above are the new ones that were just added. They increased the HAF database size increased by about 14%. But it is safe to assume that these tables will be useful to most HAF apps. 

For apps that don’t require all this data, we will be adding a filtering option to what data is stored by sql_serializer, allowing standalone HAF apps to very dramatically lower their storage requirements (probably as low as 50GB or so).

Note that the above requirement does not account for the normal storage requirements for a hived node (450GB for block_log and block_log.index) but it does eliminate the need for 580GB database for rocksdb account history plugin, which is obsoleted by the Hafah app. So a hived node + HAF server with full blockchain data would require about 2.7TB of space. I suspect the sweet spot now for performance vs cost for most API servers will be to get two 2TB nvme drives and configure them as a RAID0 stripe.

### Confirmed that Hafah results match results from account history plugin

We completed the code that compares results of Hafah against the account history plugin that it replaces and verified that the output was the same. We also developed scripts to benchmark the various phases of computation performed by Hafah when processing an api call (SQL query time vs serialization of query output into json form).

### Hafah server benchmarks (more good news)

After more optimization work on the Hafah API server, we benchmarked it under various loads and compared the results against the hived account history plugin that it replaces. Benchmarks completed so far are here: https://gitlab.syncad.com/hive/HAfAH/-/issues/6

The results were inline with what I hoped for when we initiated this project, but still extremely gratifying: on average, Hafah serves up data 20 times faster than the account history plugin, and for worst case API calls, it is as much as 40 times faster!

As an example, the worst case time for a Hafah API call on our fast system was 3.7s, whereas the same call on the rocksdb plugin would take 108s (except on any public node this API call would just timeout). 

Another way to look at this is that a single server running the new code can handle the account history workload of 20 of our current servers. 

And even for light loads, the new servers will feel much more responsive to users. The average times for account history API calls to complete are sub 100ms now, so the dominant factor for most API calls will be the latency between the client app and the API server.

One final observation is that the new HAF-based solution seems to scale better as the size of the blockchain grows: the performance benefits were more substantial at 50M blocks than at 5M blocks, for example. Since Hive is already one of the most active blockchains and signs point to rapid growth, I believe this aspect will become increasingly critical to Hive’s scalability.

### Added 2 new programmers to finish up balance_tracker app

We’ve introduced two new programmers to our blockchain team that will focus on development of HAF-based apps. Having fresh eyes on the project will help us to identify deficiencies in the current documentation for HAF.

As a starter project they are implementing the API for the new HAF-based balance tracker app I described in my post last week. They will also be creating an associated web-based API that will allow users to graph balances of Hive and HBD over time (i.e.. like a portfolio tracker on a trading site to see how your account balances grow or shrink with time).

# Work in progress and upcoming work

* Cleanup documentation for HAF and HAF apps
* Fix slow down when sql_serializer swaps from massive sync to live sync mode because of sql_index_threshold being used improperly
* Add operation filtering option to sql_serializer
* Finish up balance_tracker application
* Repo and branch cleanup
* Finish conversion of hivemind to HAF-based app. Once we’re further along with HAF-based hivemind, we’ll test it using the fork-inducing tool.
* Continue RC related work.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/30th-update-of-2021-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-30th-update-of-2021-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-30th-update-of-2021-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-30th-update-of-2021-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-30th-update-of-2021-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-30th-update-of-2021-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-30th-update-of-2021-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-30th-update-of-2021-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-30th-update-of-2021-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-30th-update-of-2021-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-30th-update-of-2021-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/30th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;30th update of 2021 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below is a list of some of the Hive-related programming issues worked on by BlockTrades team during the past week: # Hived work (blockchain node software) ### Improving resource credit (RC) plugin We’re continuing to experiment with tweaking the RC credit plugin to more accurately account for real world costs. Because a lot of the work involves long-running reindexing tests whenever we make a change, I expect this task to continue for a couple of weeks at least. ### Miscellaneous work on hived We upgraded the CI build/runtime/test images to Ubuntu 20.04 as part of our general move of hive development from U18 to U20. We completed an analysis of some strange log messages during periods of high blockchain activity. These messages claimed some transactions were being included into a series of consecutive blocks when the transacting account didn’t have enough RC to pay for them. However, we could see on block explorers that these transactions were not actually included, so we decided to dig in to find out what was going on, in case there was some real problem. The ultimate issue was pretty complicated, but not problematic. For those want to see what kinds of problems programmers have to deal with, you can read the full result of this investigation here: https://gitlab.syncad.com/hive/hive/-/issues/197 We also continued work on improving testtools and tests for hived. This change impacts anyone who is writing tests for hived: https://gitlab.syncad.com/hive/hive/-/merge_requests/313 # Hive Application Framework: framework for building robust and scalable Hive apps Most of our work this week continues to be HAF-related and we had a lot of good news in the past few days on the performance front. ### New tables generated by sql_serializer for HAF apps reduced hafah sync time by 45% We finished optimizing the latest version of sql_serializer that writes two new tables to the database to indicate what accounts exist (hive.accounts table) and what accounts are affected by which blockchain operations (hive.account_operations). After optimization, writing these tables only added 10% to previous time required to generate a HAF database (in prototype form it took twice as long), which was really good news for us, because these two tables eliminate the need to have to a reindex/sync app in the HAF account history app (Hafah). To put this in perspective, previously to reindex Hafah required two steps: * Run sql_serializer to generate HAF database to the head block (24499 seconds) * Run hafah reindex to generate tables needed by Hafah (23942 seconds) So the total time to process 49M blocks was 48441 seconds = 13.5 hours With the new version of sql_serializer, there is no need for the second step. The new version of sql_serializer runs a little longer (26738 seconds), but as there is noHafah sync step, the total time was reduced from 13.5 hours to7.3 hours (45% faster). ### Space consumed by HAF database tables at 59M blocks hive.blocks | 7135 MB | hive.operations | 969 GB | hive.transactions | 408 GB | hive.transactions_multisig | 438 MB | hive.accounts | 108 MB | hive.account_operations | 190 GB | The HAF database above occupies 2180GB after adding in table indexes and other overhead. The last two tables above are the new ones that were just added. They increased the HAF database size increased by about 14%. But it is safe to assume that these tables will be useful to most HAF apps. For apps that don’t require all this data, we will be adding a filtering option to what data is stored by sql_serializer, allowing standalone HAF apps to very dramatically lower their storage requirements (probably as low as 50GB or so). Note that the above requirement does not account for the normal storage requirements for a hived node (450GB for block_log and block_log.index) but it does eliminate the need for 580GB database for rocksdb account history plugin, which is obsoleted by the Hafah app. So a hived node + HAF server with full blockchain data would require about 2.7TB of space. I suspect the sweet spot now for performance vs cost for most API servers will be to get two 2TB nvme drives and configure them as a RAID0 stripe. ### Confirmed that Hafah results match results from account history plugin We completed the code that compares results of Hafah against the account history plugin that it replaces and verified that the output was the same. We also developed scripts to benchmark the various phases of computation performed by Hafah when processing an api call (SQL query time vs serialization of query output into json form). ### Hafah server benchmarks (more good news) After more optimization work on the Hafah API server, we benchmarked it under various loads and compared the results against the hived account history plugin that it replaces. Benchmarks completed so far are here: https://gitlab.syncad.com/hive/HAfAH/-/issues/6 The results were inline with what I hoped for when we initiated this project, but still extremely gratifying: on average, Hafah serves up data 20 times faster than the account history plugin, and for worst case API calls, it is as much as 40 times faster! As an example, the worst case time for a Hafah API call on our fast system was 3.7s, whereas the same call on the rocksdb plugin would take 108s (except on any public node this API call would just timeout). Another way to look at this is that a single server running the new code can handle the account history workload of 20 of our current servers. And even for light loads, the new servers will feel much more responsive to users. The average times for account history API calls to complete are sub 100ms now, so the dominant factor for most API calls will be the latency between the client app and the API server. One final observation is that the new HAF-based solution seems to scale better as the size of the blockchain grows: the performance benefits were more substantial at 50M blocks than at 5M blocks, for example. Since Hive is already one of the most active blockchains and signs point to rapid growth, I believe this aspect will become increasingly critical to Hive’s scalability. ### Added 2 new programmers to finish up balance_tracker app We’ve introduced two new programmers to our blockchain team that will focus on development of HAF-based apps. Having fresh eyes on the project will help us to identify deficiencies in the current documentation for HAF. As a starter project they are implementing the API for the new HAF-based balance tracker app I described in my post last week. They will also be creating an associated web-based API that will allow users to graph balances of Hive and HBD over time (i.e.. like a portfolio tracker on a trading site to see how your account balances grow or shrink with time). # Work in progress and upcoming work * Cleanup documentation for HAF and HAF apps * Fix slow down when sql_serializer swaps from massive sync to live sync mode because of sql_index_threshold being used improperly * Add operation filtering option to sql_serializer * Finish up balance_tracker application * Repo and branch cleanup * Finish conversion of hivemind to HAF-based app. Once we’re further along with HAF-based hivemind, we’ll test it using the fork-inducing tool. * Continue RC related work. See: 30th update of 2021 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">29th update of 2021 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2021/11/17/29th-update-of-2021-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="29th update of 2021 on BlockTrades work on Hive software" /><published>2021-11-17T17:37:30-08:00</published><updated>2021-11-17T17:37:30-08:00</updated><id>https://hivedocs.info/news/core/development/2021/11/17/29th-update-of-2021-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/11/17/29th-update-of-2021-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-29th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)
Below is a list of some of the Hive-related programming issues worked on by BlockTrades team during the past work period:

# Hived work (blockchain node software)

### Updates to resource credit (RC) plugin

We’ve implemented the changes to include costs for verifying cryptographic signatures during resource credit (RC) cost calculations for transactions. 

Now we’ve begun analyzing the rationality of costs computed by the resource pools.

We also created more tests for RC delegations and reported some errors to @howo based on the test results. We’ll resume creating more tests after those issues are fixed.

### Miscellaneous work on hived

We fixed an erroneous error message that was sometimes displayed related to the –exit-before-sync command-line option.

Also, yesterday I started an analysis of how transactions are being treated when there wasn’t enough RC to get them included into the blockchain. This is because while analyzing a high traffic time period on the p2p network, I had seen evidence that such transactions are lingering until the account either regenerates enough RC or the transaction expires, which isn’t desirable, in my opinion, from a performance view. 

I’m currently testing an experimental version of hived that should reject such transactions and avoid propagating them to the rest of the p2p network.

# Hive Application Framework: framework for building robust and scalable Hive apps

Most of our work lately continues to be HAF-related:

### Experimenting with sql_serializer implementation and operation filtering

We’re modifying the sql_serializer to directly write some additional tables to the database to indicate what accounts are affected by which blockchain operations. 

The hope is that this will result in a speedup versus the current method, where each HAF app has to re-compute this data on-the-fly. 

We have a first pass at this done now, but the sql_serializer phase takes twice as long as it did before, so we’re going to see if there is anything that can be done to optimize that, since it doesn’t seem like it should take twice as long.

We’re also planning to add an option to sql_serializer to allow filtering of what operations get saved to the database. This should be useful for lowering the storage requirements and improving performance of HAF servers that are dedicated to supporting a specific HAF app.


### New example HAF app: balance_tracker

We created a new HAF application called balance_tracker that maintains a history of all the coin balances for an account changes over time (e.g. you can plot a graph of how your hive and hbd balance change over time).  It’s not completely finished yet, but it works well as a prototype already, and has excellent performance (processed 58M blocks in 2.5 hours), so it’s a great example of how fast HAF apps can be. 

It’s also not fully optimized for efficiency yet (account names are stored as strings in all the records), but even so its disk usage isn’t too bad (~22GB to store asset histories for every account on the blockchain).

The primary reason for creating this app was to serve as an example for new programmers starting out with HAF, but I think it is useful enough that we will further improve it and add a web UI for it (and we still need to add an API interface to the data before that).

Each time we create one of these small apps, we learn a little bit more about the most efficient way to build them, so we hope these examples will serve as guidelines for future app development.

### Optimizing HAF-based account history app (Hafah)

We completed benchmarking for Hafah and achieved a new performance record for syncing the data to headblock with the latest version (4.9 hours to sync 58M blocks, which was about a 40% speedup over 
pre-optimized version).
### Some progress on optimized multi-threaded json-rpc server for HAF apps

We finished an initial implementation of a the multi-threaded json-rpc server for HAF apps, but unfortunately benchmarking it as an API server for Hafah showed its performance was worse than the previous async-io based json-rpc server. 

We’ve identified the likely problems (one of which was continual creation of new connections to the SQL server), so we’ll be updating the server this week and re-running the benchmark after we’ve improved the implementation. I hope we can complete this task in the coming week, but if the upcoming optimizations aren’t sufficient, we may need to look at another jsonrpc server implementation such as Sanic (Sanic is used by the Jussi proxy server which is also implemented in Python).

We are also creating a CI test for hafah and it should be done in the next week.

# Hivemind (social media middleware app)

We’ve tagged the final version of hivemind that supports postgres 10 for production deployment. We’re currently creating a database dump file to ease upgrading by API server nodes. 

All new versions of hivemind after the one just released will require postgres 12, but we’re planning to convert hivemind to be HAF-based before we tag another production version, and that new version will need to undergo rigorous testing and benchmarking before we would tag it for production usage because of the magnitude of the change in the hivemind sync algorithm.

# Condenser (code for hive.blog)

We deployed a new version of hive.blog with @quochuy’s change to display the amount of resource credits (RC)  available as a circle around each account’s profile icon (it is also displayed at some other new places such as &quot;Account stats&quot; below a post being created).

# Work in progress and upcoming work

* Continue experimenting with generating the “impacted accounts” table directly from sql_serializer to see if it is faster than our current method where hafah generates this data on demand as it needs it.
* Above task will also require creating a simplified form of hafah.
* Fix the algorithm used by sql_serializer to determine when it should drop out of massive sync mode into normal live sync mode (currently it drops out of massive sync too early).
* Finish up work on multi-threaded jsonrpc server.
* Run tests to compare results between account history plugin and HAF-based account history apps.
* Finish conversion of hivemind to HAF-based app. Once we’re further along with HAF-based hivemind, we’ll test it using the fork-inducing tool.
* Continue RC related work.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/29th-update-of-2021-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-29th-update-of-2021-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-29th-update-of-2021-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-29th-update-of-2021-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-29th-update-of-2021-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-29th-update-of-2021-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-29th-update-of-2021-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-29th-update-of-2021-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-29th-update-of-2021-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-29th-update-of-2021-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-29th-update-of-2021-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/29th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;29th update of 2021 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below is a list of some of the Hive-related programming issues worked on by BlockTrades team during the past work period: # Hived work (blockchain node software) ### Updates to resource credit (RC) plugin We’ve implemented the changes to include costs for verifying cryptographic signatures during resource credit (RC) cost calculations for transactions. Now we’ve begun analyzing the rationality of costs computed by the resource pools. We also created more tests for RC delegations and reported some errors to @howo based on the test results. We’ll resume creating more tests after those issues are fixed. ### Miscellaneous work on hived We fixed an erroneous error message that was sometimes displayed related to the –exit-before-sync command-line option. Also, yesterday I started an analysis of how transactions are being treated when there wasn’t enough RC to get them included into the blockchain. This is because while analyzing a high traffic time period on the p2p network, I had seen evidence that such transactions are lingering until the account either regenerates enough RC or the transaction expires, which isn’t desirable, in my opinion, from a performance view. I’m currently testing an experimental version of hived that should reject such transactions and avoid propagating them to the rest of the p2p network. # Hive Application Framework: framework for building robust and scalable Hive apps Most of our work lately continues to be HAF-related: ### Experimenting with sql_serializer implementation and operation filtering We’re modifying the sql_serializer to directly write some additional tables to the database to indicate what accounts are affected by which blockchain operations. The hope is that this will result in a speedup versus the current method, where each HAF app has to re-compute this data on-the-fly. We have a first pass at this done now, but the sql_serializer phase takes twice as long as it did before, so we’re going to see if there is anything that can be done to optimize that, since it doesn’t seem like it should take twice as long. We’re also planning to add an option to sql_serializer to allow filtering of what operations get saved to the database. This should be useful for lowering the storage requirements and improving performance of HAF servers that are dedicated to supporting a specific HAF app. ### New example HAF app: balance_tracker We created a new HAF application called balance_tracker that maintains a history of all the coin balances for an account changes over time (e.g. you can plot a graph of how your hive and hbd balance change over time). It’s not completely finished yet, but it works well as a prototype already, and has excellent performance (processed 58M blocks in 2.5 hours), so it’s a great example of how fast HAF apps can be. It’s also not fully optimized for efficiency yet (account names are stored as strings in all the records), but even so its disk usage isn’t too bad (~22GB to store asset histories for every account on the blockchain). The primary reason for creating this app was to serve as an example for new programmers starting out with HAF, but I think it is useful enough that we will further improve it and add a web UI for it (and we still need to add an API interface to the data before that). Each time we create one of these small apps, we learn a little bit more about the most efficient way to build them, so we hope these examples will serve as guidelines for future app development. ### Optimizing HAF-based account history app (Hafah) We completed benchmarking for Hafah and achieved a new performance record for syncing the data to headblock with the latest version (4.9 hours to sync 58M blocks, which was about a 40% speedup over pre-optimized version). ### Some progress on optimized multi-threaded json-rpc server for HAF apps We finished an initial implementation of a the multi-threaded json-rpc server for HAF apps, but unfortunately benchmarking it as an API server for Hafah showed its performance was worse than the previous async-io based json-rpc server. We’ve identified the likely problems (one of which was continual creation of new connections to the SQL server), so we’ll be updating the server this week and re-running the benchmark after we’ve improved the implementation. I hope we can complete this task in the coming week, but if the upcoming optimizations aren’t sufficient, we may need to look at another jsonrpc server implementation such as Sanic (Sanic is used by the Jussi proxy server which is also implemented in Python). We are also creating a CI test for hafah and it should be done in the next week. # Hivemind (social media middleware app) We’ve tagged the final version of hivemind that supports postgres 10 for production deployment. We’re currently creating a database dump file to ease upgrading by API server nodes. All new versions of hivemind after the one just released will require postgres 12, but we’re planning to convert hivemind to be HAF-based before we tag another production version, and that new version will need to undergo rigorous testing and benchmarking before we would tag it for production usage because of the magnitude of the change in the hivemind sync algorithm. # Condenser (code for hive.blog) We deployed a new version of hive.blog with @quochuy’s change to display the amount of resource credits (RC) available as a circle around each account’s profile icon (it is also displayed at some other new places such as &quot;Account stats&quot; below a post being created). # Work in progress and upcoming work * Continue experimenting with generating the “impacted accounts” table directly from sql_serializer to see if it is faster than our current method where hafah generates this data on demand as it needs it. * Above task will also require creating a simplified form of hafah. * Fix the algorithm used by sql_serializer to determine when it should drop out of massive sync mode into normal live sync mode (currently it drops out of massive sync too early). * Finish up work on multi-threaded jsonrpc server. * Run tests to compare results between account history plugin and HAF-based account history apps. * Finish conversion of hivemind to HAF-based app. Once we’re further along with HAF-based hivemind, we’ll test it using the fork-inducing tool. * Continue RC related work. See: 29th update of 2021 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Hive Witness Essentials Update</title><link href="https://hivedocs.info/news/nodejs/witness/2021/11/12/p-19g65lkvw9fu0l191d09e3d22cd302.html" rel="alternate" type="text/html" title="Hive Witness Essentials Update" /><published>2021-11-12T02:51:48-08:00</published><updated>2021-11-12T02:51:48-08:00</updated><id>https://hivedocs.info/news/nodejs/witness/2021/11/12/p-19g65lkvw9fu0l191d09e3d22cd302</id><content type="html" xml:base="https://hivedocs.info/news/nodejs/witness/2021/11/12/p-19g65lkvw9fu0l191d09e3d22cd302.html">&lt;div id=&quot;content-therealwolf-p-19g65lkvw9fu0l191d09e3d22cd302&quot;&gt;I've updated #Hive Witness Essentials, most notably dhive/hive-js and also fixed a few small bugs.

In case you're using the `run.sh` docker script, you most likely still need to run `yarn` to update npm dependencies.

Repository: https://github.com/therealwolf42/hive-witness-essentials

&lt;sup&gt;&lt;sub&gt;Posted via [inji.com](https://inji.com/hive/@therealwolf/p-19g65lkvw9fu0l191d09e3d22cd302)&lt;/sub&gt;&lt;/sup&gt;&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@therealwolf&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/p-19g65lkvw9fu0l191d09e3d22cd302&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-therealwolf-p-19g65lkvw9fu0l191d09e3d22cd302').html();
      const outputElem = $('#content-therealwolf-p-19g65lkvw9fu0l191d09e3d22cd302');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-therealwolf-p-19g65lkvw9fu0l191d09e3d22cd302 {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-therealwolf-p-19g65lkvw9fu0l191d09e3d22cd302 code {
    background: white;
  }
  #content-therealwolf-p-19g65lkvw9fu0l191d09e3d22cd302 a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-therealwolf-p-19g65lkvw9fu0l191d09e3d22cd302 a:hover {
    border-bottom: 0;
  }
  #content-therealwolf-p-19g65lkvw9fu0l191d09e3d22cd302 h1 {
    font-size: 2.2em;
  }
  #content-therealwolf-p-19g65lkvw9fu0l191d09e3d22cd302 h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-therealwolf-p-19g65lkvw9fu0l191d09e3d22cd302 header small {
    color: #999;
    font-size: 50%;
  }
  #content-therealwolf-p-19g65lkvw9fu0l191d09e3d22cd302 img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-123456/@therealwolf/p-19g65lkvw9fu0l191d09e3d22cd302&quot;&gt;Hive Witness Essentials Update&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@therealwolf&quot;&gt;@therealwolf&lt;/a&gt;
&lt;/p&gt;</content><author><name>therealwolf</name></author><category term="news" /><category term="nodejs" /><category term="witness" /><summary type="html">I've updated #Hive Witness Essentials, most notably dhive/hive-js and also fixed a few small bugs. In case you're using the `run.sh` docker script, you most likely still need to run `yarn` to update npm dependencies. Repository: https://github.com/therealwolf42/hive-witness-essentials Posted via [inji.com](https://inji.com/hive/@therealwolf/p-19g65lkvw9fu0l191d09e3d22cd302) See: Hive Witness Essentials Update by @therealwolf</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hivedocs.info/assets/images/favicon.png" /><media:content medium="image" url="https://hivedocs.info/assets/images/favicon.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">28th update of 2021 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2021/11/08/28th-update-of-2021-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="28th update of 2021 on BlockTrades work on Hive software" /><published>2021-11-08T16:27:15-08:00</published><updated>2021-11-08T16:27:15-08:00</updated><id>https://hivedocs.info/news/core/development/2021/11/08/28th-update-of-2021-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/11/08/28th-update-of-2021-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-28th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)
Below is a list of some of the Hive-related programming issues worked on by BlockTrades team during the past work period:

# Hived work (blockchain node software)

### Updating RC cost calculation code

We continued to analyze the resource credits code. We’ve written some code to dump rc costs during replay of the blockchain and created some graphs to analyze how the rc resource pools change over the blockchain’s operational history. Our most significant finding so far is that the execution time costs are totally inaccurate right now, largely because signature verification time wasn’t accounted for at all.

To get better estimates of real world execution time costs, we’re probably going to create a tool to measure execution times for various operations during replay as a starting point for updating them to accurate values, with a separate cost calculation that accounts for signature costs based on number of signatures used by the transaction containing the operations.

### Testing via new “mirror net” technology identified a new bug (now fixed)

As mentioned a while back, we’ve been developing a tool for taking an existing block_log (e.g. a block_log from the mainnet) as a starting point for launching a testnet that more closely matches the configuration of the mainnet. This technology is conceptually similar to the idea behind the older tinman/gatlin code, but is designed for higher performance.

The new mirror net code is already proving it’s worth, as we found a bug in the hived code while testing the mirror net code, whereby the reward balance could go negative. For more on this bug, see the fix and associated issue: https://gitlab.syncad.com/hive/hive/-/merge_requests/306

In the longer term, we’ll be integrating this technology into our build-and-test system (continuous integration system) for various advanced test scenarios.

### Finished testing and merged in Command-line interface (CLI) wallet improvements to develop branch

Complete improvements for offline use of the CLI wallet:
https://gitlab.syncad.com/hive/hive/-/merge_requests/265
Add default value to the server-rpc-endpoint option:
https://gitlab.syncad.com/hive/hive/-/merge_requests/273

Other hived-related work:
* Finished testing and merged in fixes for sql_serializer and account history plugins: https://gitlab.syncad.com/hive/hive/-/merge_requests/289
https://gitlab.syncad.com/hive/hive/-/merge_requests/294
* Merged in changes for HBD limits for HF26: https://gitlab.syncad.com/hive/hive/-/merge_requests/297
* Removed obsolete option to `SKIP_BY_TX_ID` from compile options: https://gitlab.syncad.com/hive/hive/-/merge_requests/301

* Fix for problem with faketime library on some platforms: https://gitlab.syncad.com/hive/hive/-/merge_requests/303
* Updated some API pattern tests based on bug fixes: https://gitlab.syncad.com/hive/hive/-/merge_requests/299
* Improved testtools robustness when there is a temporary communication interruption to nodes being tested:
https://gitlab.syncad.com/hive/hive/-/merge_requests/302
* Updated to use newer clang-tidy linter (now uses the default one on Ubuntu 20):
https://gitlab.syncad.com/hive/hive/-/merge_requests/300
* Compile all targets with boost &amp;gt; 1.70 available on Ubuntu 20.04: https://gitlab.syncad.com/hive/hive/-/merge_requests/307

# Hive Application Framework: framework for building robust and scalable Hive apps

A lot of our work during the last period has continued to focus on app framework development and testing. 

We continued to work on code cleanup associated with the new HAF repo (this is the new repo mentioned last week that contains the components that are common to all HAF-based applications that was created to better manage version compatibility among HAF components and prerequisite applications such as hived). 

A lot of documentation was added and/or updated, more testing by 3rd party testers (like me) to ensure instructions are clear and accurate on “clean systems” (i.e. not the developer’s computer), fixes were made for build and test compatibility on both Ubuntu 18 and 20 (although Ubuntu 20 is still the recommended platform for any HAF-related development), etc.

A new API call, `hive.connect`, was added which handles database inconsistencies that can potentially arise if the connection between hived and the postgres server is broken, allowing serializing to be smoothly resumed. https://gitlab.syncad.com/hive/haf/-/merge_requests/13


We also added system tests for the sql_serializer to the new HAF repo: https://gitlab.syncad.com/hive/haf/-/merge_requests/17
With this addition, we have a full test suite for all the components contained in the HAF repo.



### Optimizing HAF-based account history app (Hafah)

This week we continued to do performance testing and optimization of hafah. 

We added a library that allows us to track memory usage, and the latest incarnation with memory optimizations was able to sync all the way to the headblock using only 4GB of virtual memory while configured to use 7 threads for sending/receiving data. Previously using this many threads required nearly 128GB of memory, so it was a useful improvement.

Further improvements were also made to the sync process, which at least in the 5M block scenario that was tested, reduced sync time from 280s down to 180s. This improvement still needs to be benchmarked with a full sync to headblock, but it is likely we'll see similar improvements in performance for a full sync.

### Benchmark multi-threaded jsonrpc server for HAF apps (using hafah as our “test” app)

Preliminary benchmarking using a multi-threaded jsonrpc server showed 2-3x speed improvement in api performance for an experimental version of hafah synced to 5M blocks (2-3x performance measured relative to the original single-threaded jsonrpc server), but we still need to repeat these benchmarks with a version of hafah synced to the current head block.

# Hivemind (social media middleware app used by social media frontends like hive.blog)

As mentioned last week, there was a bug detected during our production testing with some notifications showing as from 1970. 

We’ve fixed this bug and the new version with this fix is now being tested on production. Assuming no problems, we’ll tag an official release with the bug fix in the next couple of days.

# Work in progress and upcoming work

* In progress: experiment with generating the “impacted accounts” table directly from sql_serializer to see if it is faster than our current method where hafah generates this data on demand as it needs it. This task will also require creating a simplified form of hafah. In fact, the new hafah would be so simplified on the indexer side, that we’re considering adding additional functionality to it, to allow it to maintain a  per-block account balance history so that there is something that does real work in the indexing portion of the code. This would also be useful as a template for future work on 2nd-layer tokens.
* Release a final official version of hivemind with postgres 10 support, then update hivemind CI to start testing using postgres 12 instead of 10.
* Run tests to compare results between account history plugin and HAF-based account history apps.
* Finish setup of continuous integration testing for HAF account history app.
* Finish conversion of hivemind to HAF-based app. Once we’re further along with HAF-based hivemind, we’ll test it using the fork-inducing tool.
* Fix rc cost estimations for execution time as described in hived section of this post.
* Deploy new version of condenser that displays RC level for signed-in account.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/28th-update-of-2021-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-28th-update-of-2021-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-28th-update-of-2021-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-28th-update-of-2021-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-28th-update-of-2021-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-28th-update-of-2021-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-28th-update-of-2021-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-28th-update-of-2021-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-28th-update-of-2021-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-28th-update-of-2021-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-28th-update-of-2021-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/28th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;28th update of 2021 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below is a list of some of the Hive-related programming issues worked on by BlockTrades team during the past work period: # Hived work (blockchain node software) ### Updating RC cost calculation code We continued to analyze the resource credits code. We’ve written some code to dump rc costs during replay of the blockchain and created some graphs to analyze how the rc resource pools change over the blockchain’s operational history. Our most significant finding so far is that the execution time costs are totally inaccurate right now, largely because signature verification time wasn’t accounted for at all. To get better estimates of real world execution time costs, we’re probably going to create a tool to measure execution times for various operations during replay as a starting point for updating them to accurate values, with a separate cost calculation that accounts for signature costs based on number of signatures used by the transaction containing the operations. ### Testing via new “mirror net” technology identified a new bug (now fixed) As mentioned a while back, we’ve been developing a tool for taking an existing block_log (e.g. a block_log from the mainnet) as a starting point for launching a testnet that more closely matches the configuration of the mainnet. This technology is conceptually similar to the idea behind the older tinman/gatlin code, but is designed for higher performance. The new mirror net code is already proving it’s worth, as we found a bug in the hived code while testing the mirror net code, whereby the reward balance could go negative. For more on this bug, see the fix and associated issue: https://gitlab.syncad.com/hive/hive/-/merge_requests/306 In the longer term, we’ll be integrating this technology into our build-and-test system (continuous integration system) for various advanced test scenarios. ### Finished testing and merged in Command-line interface (CLI) wallet improvements to develop branch Complete improvements for offline use of the CLI wallet: https://gitlab.syncad.com/hive/hive/-/merge_requests/265 Add default value to the server-rpc-endpoint option: https://gitlab.syncad.com/hive/hive/-/merge_requests/273 Other hived-related work: * Finished testing and merged in fixes for sql_serializer and account history plugins: https://gitlab.syncad.com/hive/hive/-/merge_requests/289 https://gitlab.syncad.com/hive/hive/-/merge_requests/294 * Merged in changes for HBD limits for HF26: https://gitlab.syncad.com/hive/hive/-/merge_requests/297 * Removed obsolete option to `SKIP_BY_TX_ID` from compile options: https://gitlab.syncad.com/hive/hive/-/merge_requests/301 * Fix for problem with faketime library on some platforms: https://gitlab.syncad.com/hive/hive/-/merge_requests/303 * Updated some API pattern tests based on bug fixes: https://gitlab.syncad.com/hive/hive/-/merge_requests/299 * Improved testtools robustness when there is a temporary communication interruption to nodes being tested: https://gitlab.syncad.com/hive/hive/-/merge_requests/302 * Updated to use newer clang-tidy linter (now uses the default one on Ubuntu 20): https://gitlab.syncad.com/hive/hive/-/merge_requests/300 * Compile all targets with boost &amp;gt; 1.70 available on Ubuntu 20.04: https://gitlab.syncad.com/hive/hive/-/merge_requests/307 # Hive Application Framework: framework for building robust and scalable Hive apps A lot of our work during the last period has continued to focus on app framework development and testing. We continued to work on code cleanup associated with the new HAF repo (this is the new repo mentioned last week that contains the components that are common to all HAF-based applications that was created to better manage version compatibility among HAF components and prerequisite applications such as hived). A lot of documentation was added and/or updated, more testing by 3rd party testers (like me) to ensure instructions are clear and accurate on “clean systems” (i.e. not the developer’s computer), fixes were made for build and test compatibility on both Ubuntu 18 and 20 (although Ubuntu 20 is still the recommended platform for any HAF-related development), etc. A new API call, `hive.connect`, was added which handles database inconsistencies that can potentially arise if the connection between hived and the postgres server is broken, allowing serializing to be smoothly resumed. https://gitlab.syncad.com/hive/haf/-/merge_requests/13 We also added system tests for the sql_serializer to the new HAF repo: https://gitlab.syncad.com/hive/haf/-/merge_requests/17 With this addition, we have a full test suite for all the components contained in the HAF repo. ### Optimizing HAF-based account history app (Hafah) This week we continued to do performance testing and optimization of hafah. We added a library that allows us to track memory usage, and the latest incarnation with memory optimizations was able to sync all the way to the headblock using only 4GB of virtual memory while configured to use 7 threads for sending/receiving data. Previously using this many threads required nearly 128GB of memory, so it was a useful improvement. Further improvements were also made to the sync process, which at least in the 5M block scenario that was tested, reduced sync time from 280s down to 180s. This improvement still needs to be benchmarked with a full sync to headblock, but it is likely we'll see similar improvements in performance for a full sync. ### Benchmark multi-threaded jsonrpc server for HAF apps (using hafah as our “test” app) Preliminary benchmarking using a multi-threaded jsonrpc server showed 2-3x speed improvement in api performance for an experimental version of hafah synced to 5M blocks (2-3x performance measured relative to the original single-threaded jsonrpc server), but we still need to repeat these benchmarks with a version of hafah synced to the current head block. # Hivemind (social media middleware app used by social media frontends like hive.blog) As mentioned last week, there was a bug detected during our production testing with some notifications showing as from 1970. We’ve fixed this bug and the new version with this fix is now being tested on production. Assuming no problems, we’ll tag an official release with the bug fix in the next couple of days. # Work in progress and upcoming work * In progress: experiment with generating the “impacted accounts” table directly from sql_serializer to see if it is faster than our current method where hafah generates this data on demand as it needs it. This task will also require creating a simplified form of hafah. In fact, the new hafah would be so simplified on the indexer side, that we’re considering adding additional functionality to it, to allow it to maintain a per-block account balance history so that there is something that does real work in the indexing portion of the code. This would also be useful as a template for future work on 2nd-layer tokens. * Release a final official version of hivemind with postgres 10 support, then update hivemind CI to start testing using postgres 12 instead of 10. * Run tests to compare results between account history plugin and HAF-based account history apps. * Finish setup of continuous integration testing for HAF account history app. * Finish conversion of hivemind to HAF-based app. Once we’re further along with HAF-based hivemind, we’ll test it using the fork-inducing tool. * Fix rc cost estimations for execution time as described in hived section of this post. * Deploy new version of condenser that displays RC level for signed-in account. See: 28th update of 2021 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">How to use your ledger device with hive-ledger-cli</title><link href="https://hivedocs.info/howto/nodejs/wallet/2021/11/08/how-to-use-your-ledger-device-with-hive-ledger-cli.html" rel="alternate" type="text/html" title="How to use your ledger device with hive-ledger-cli" /><published>2021-11-08T15:28:09-08:00</published><updated>2021-11-08T15:28:09-08:00</updated><id>https://hivedocs.info/howto/nodejs/wallet/2021/11/08/how-to-use-your-ledger-device-with-hive-ledger-cli</id><content type="html" xml:base="https://hivedocs.info/howto/nodejs/wallet/2021/11/08/how-to-use-your-ledger-device-with-hive-ledger-cli.html">&lt;div id=&quot;content-engrave-how-to-use-your-ledger-device-with-hive-ledger-cli&quot;&gt;![](https://i.imgur.com/PRL7SLu.jpg)

The purpose of this post is to fulfill Ledger documentation requirements. It is a guide for advanced users that would like to use Hive application with a CLI (Command-line interface). In the near future I will provide an updated tutorial with user-friendly wallet supporting Ledger Hive application.

**Support me with your witness vote! Click on the image below:**

[![banner_engrave 100.png](https://images.hive.blog/DQmUghvic5TCPPvVkB4iB7eXmgQ3RQ4L8jkVizvuCBW8RMT/banner_engrave%20100.png)](https://hivesigner.com/sign/account-witness-vote?witness=engrave&amp;amp;approve=1)

# Introduction

Hardware wallets are considered to be the most secure way to keep your crypto assets. From now on, Ledger Nano S and X can be used to protect your Hive account. This guide will help you protecting your Hive account with keys derived on your ledger device and treat is as a cold wallet. This is the ultimate protection for your account.

## Quickly about Hive

Hive is different than most blockchains. It has two native assets: HIVE and HBD (Hive Backed Dollar) and usernames instead of public addresses. Those accounts have different keys associated with different roles (owner, active, posting and memo). Every role can contain different key. To ultimately protect your Hive account it is recommended to replace your Owner key with the one derived from your device - this will protect you from leaking your private key by mistake.

# Requirements

Before you start, make sure you have
 * Initialized Ledger device with newest firmware (2.0 for S and 1.3.0 for X)
 * Hive application installed on your device

# Install `hive-ledger-cli`

`hive-ledger-cli` is a npm package that can be installed globally on your system. In order to use it, you need to have NodeJS installed on your system. 

To install `hive-ledger-cli` simply paste

```bash=
sudo npm install -g hive-ledger-cli --unsafe-perm
```

Validate your installation with following command which should show you the CLI help

```bash=
hive-ledger-cli --help
```

# Get an account

**If you already have Hive account, you can skip this part.**

Hive accounts are not free - they need to be created by another user and it costs 3 HIVE. There are multiple providers of Hive accounts. Most of them requires some sort of validation (i.e phone or email) and some can utilize different crypto assets (like BTC) to pay the fee.

Visit and choose what fits you best:

https://signup.hive.io

# How to validate public key

In order to validate public key derived from a specified [SLIP-0048](https://github.com/satoshilabs/slips/blob/master/slip-0048.md) path, you can use `get-public-key` command:


```
USAGE
  $ hive-ledger-cli get-public-key PATH

ARGUMENTS
  PATH  BIP 32 path to derive key from

OPTIONS
  -c, --confirm  Force confirmation on Ledger device
  -h, --help     show CLI help

EXAMPLE
  $ hive-ledger-cli get-public-key &quot;m/48'/13'/0'/0'/0'&quot;
  Establishing transport with Hive application... done
  STM5m57x4BXEePAzVNrjUqYeh9C2a7eez1Ya2wPo7ngWLQUdEXjKn

```

1. Connect your device, unlock and open Hive application.

![](https://i.imgur.com/E5vdx3o.jpg)

3. Select [SLIP-0048](https://github.com/satoshilabs/slips/blob/master/slip-0048.md) path you want to generate and validate public key for. Example key valid for Hive is `m/48'/13'/0'/0'/0'`
4. Type `hive-ledger-cli get-public-key &quot;m/48'/13'/0'/0'/0'&quot; -c`

![](https://i.imgur.com/55oecOv.png)


4. Navigate using left and right buttons to inspect public key. Make sure it's the same that in your terminal

![](https://i.imgur.com/Gnggq0f.jpg)

![](https://i.imgur.com/M8HeJ7v.jpg)

![](https://i.imgur.com/yKceFr5.jpg)

![](https://i.imgur.com/sFxsNIh.jpg)

![](https://i.imgur.com/zYvqRBO.jpg)

6. Confirm public key by pressing both buttons on &quot;Approve&quot; screen.

![](https://i.imgur.com/YDvVEQV.jpg)

![](https://i.imgur.com/7Ied1zR.png)


# How to associate Hive account with your Ledger device

If you already have your Hive username, you can use your ledger device to ultimately protect it by changing all your keys to the ones derived from your device.

In the example below, we will associate existing account called `test.ledger` with the device. You will need current owner private key in Hive supported WIF format.

```
USAGE
  $ hive-ledger-cli associate-account USERNAME

ARGUMENTS
  USERNAME  Account to associate

OPTIONS
  -d, --dry              dry run will only print signed transaction instead broadcasting it
  -h, --help             show CLI help
  -k, --key-gap=key-gap  [default: 5] Gap limit for unused key indexes on which the software decides that SLIP-0048 account index is not used
  -t, --testnet          use testnet configuration

EXAMPLE
  $ hive-ledger-cli associate-account test.ledger
  Found existing account:  test.ledger

  This operation will replace ALL your keys with those from your device. First, we need to find new keys for your owner, active and posting authorities.

  Press any key to continue or q to exit:
  Establishing transport with Hive application... done
  Searching for unused keys on your device... done

  New owner key:  STM7ZEBoDotbYpnyNHdARYMDBMNnLWpV7fiiGa6pvHbXhfRo9ZrDf
  Derivation path:  m/48'/13'/0'/1'/0'
  Please confirm this public key on your device... done

  New active key:  STM5zpFRqa73yFULSwUYfPSftx4fE7kha9YfkAPR9yKvNLKU2QDFu
  Derivation path:  m/48'/13'/1'/1'/0'
  Please confirm this public key on your device... done

  New posting key:  STM6YGUesBwuotbZvcfqoXfjQUrfVXGYwJZ9DCHEiFLQfxUsLK9M1
  Derivation path:  m/48'/13'/4'/1'/0'
  Please confirm this public key on your device... done

  Now you need to enter your CURRENT OWNER PRIVATE KEY in WIF format. It will be used to update your account with a new keys. Please be aware that this operation will replace ALL of your current keys

  Enter your current PRIVATE OWNER KEY for test.ledger:
```

1. Connect your device, unlock and open Hive application.

![](https://i.imgur.com/E5vdx3o.jpg)

2. Type in your terminal (replace `test.ledger` with your own username) and follow the instruction

```bash
hive-ledger-cli associate-account test.ledger
```

3. Software will derive a new set of public keys and will make sure those are unused by searching in blockchain for those keys. 

4. Validate and approve three keys from on your device.

![](https://i.imgur.com/Gnggq0f.jpg)

![](https://i.imgur.com/M8HeJ7v.jpg)

![](https://i.imgur.com/yKceFr5.jpg)

![](https://i.imgur.com/sFxsNIh.jpg)

![](https://i.imgur.com/zYvqRBO.jpg)

6. Confirm public key by pressing both buttons on &quot;Approve&quot; screen.

![](https://i.imgur.com/YDvVEQV.jpg)

5. Repeat this step for next two keys.

6. Enter your current owner private key in WIF format

7. Your account is now associated with your device. You can validate it with [&quot;How to find accounts associated with your device&quot;](#how-to-find-accounts-associated-with-your-device) tutorial.

# How to find accounts associated with your device

You can quickly check which accounts are associated with your device (those will have owner authority derived from your device).


1. Connect your device, unlock and open Hive application.

![](https://i.imgur.com/E5vdx3o.jpg)

2. Type in your terminal

```bash=
hive-ledger-cli discover-accounts
```

3. Wait for the result with your accounts listed in a table

![](https://i.imgur.com/XRbhz5h.png)

# How to receive HIVE or HBD

If you know your account username, you can use it as a receiver address from any other wallet or exchange for HIVE or HBD. If you don't remember it, use `hive-ledger-cli` to determine account names associated with your device.

```
USAGE
  $ hive-ledger-cli discover-accounts [ROLE]

ARGUMENTS
  ROLE  (owner|active|memo|posting) [default: owner] Role to check for

OPTIONS
  -a, --account-gap=account-gap  [default: 5] Gap limit for unused account indexes after which the software decides that device is not used
  -d, --dry                      dry run will only print signed transaction instead broadcasting it
  -h, --help                     show CLI help
  -k, --key-gap=key-gap          [default: 5] Gap limit for unused key indexes on which the software decides that device is not used
  -t, --testnet                  use testnet configuration

EXAMPLE
  $ hive-ledger-cli discover-accounts

```
1. Connect your device, unlock and open Hive application.

![](https://i.imgur.com/E5vdx3o.jpg)

3. Type `hive-ledger-cli discover-accounts` and wait for process to finish

![](https://i.imgur.com/rjhOrXf.png)

3. You can see two accounts connected with this ledger: `acronyms` and `test.ledger`. Both can be used to receive funds.

4. You can validate path and public key with [&quot;How to validate public key&quot;](#how-to-validate-public-key) tutorial

# How to transfer HIVE or HBD to another account

You can send liquid tokens (HIVE or HBD) withing 3 seconds to any other existing Hive account. 

```
USAGE
  $ hive-ledger-cli transfer PATH FROM TO AMOUNT [MEMO]

ARGUMENTS
  PATH    BIP 32 (SLIP-0048) path to derive key from and use to sign the transaction
  FROM    sender
  TO      receiver
  AMOUNT  amount to send
  MEMO    optional transfer memo
```

To send `0.001 HIVE` from `test.ledger` to `engrave` account:

1. Connect your device, unlock and open Hive application.

![](https://i.imgur.com/E5vdx3o.jpg)

2. Type in your terminal

```bash=
hive-ledger-cli transfer &quot;m/48'/13'/0'/2'/0'&quot; test.ledger engrave &quot;0.001 HIVE&quot; &quot;Sent using ledger device!&quot;
```

![](https://i.imgur.com/u2NCGSr.png)

3. Use left and right buttons to verify transaction on your hardware wallet.

![](https://i.imgur.com/sh2zh9X.jpg)

![](https://i.imgur.com/nqvYQUi.jpg)

![](https://i.imgur.com/BKg8pW1.jpg)

![](https://i.imgur.com/v0eTCE0.jpg)

![](https://i.imgur.com/toFCSW5.jpg)

![](https://i.imgur.com/jzxeaAM.jpg)

![](https://i.imgur.com/y1UBY9m.jpg)

![](https://i.imgur.com/bxgL0oz.jpg)

4. Accept the transaction by pressing both buttons on &quot;Approve&quot; screen.

![](https://i.imgur.com/YDvVEQV.jpg)

![](https://i.imgur.com/Lus0eYw.png)

# How to stake HIVE


To gain more governance power over Hive blockchain, you can convert your HIVE into Hive Power. We call it &quot;powering up&quot; and other blockchains calls it &quot;staking&quot;. Underneath, your tokens are converted to VESTS. Withdrawal of your staked HIVE (converting it back to liquid HIVE) will take 13 weeks.

To convert HIVE into HP (Hive Power), use `hive-ledger-cli`.

```
USAGE
  $ hive-ledger-cli transfer-to-vesting PATH FROM TO AMOUNT

ARGUMENTS
  PATH    BIP 32 (SLIP-0048) path to derive key from and use to sign the transaction
  FROM    source account
  TO      target account
  AMOUNT  amount to stake

OPTIONS
  -d, --dry      dry run will only print signed transaction instead broadcasting it
  -h, --help     show CLI help
  -t, --testnet  use testnet configuration
```

1. Connect your device, unlock and open Hive application.

![](https://i.imgur.com/E5vdx3o.jpg)

2. Type in your terminal

```bash=
hive-ledger-cli transfer-to-vesting &quot;m/48'/13'/0'/2'/0'&quot; test.ledger test.ledger &quot;0.001 HIVE&quot;
```

3. Use left and right keys to verify transaction on your hardware wallet

![](https://i.imgur.com/sh2zh9X.jpg)

![](https://i.imgur.com/nqvYQUi.jpg)

![](https://i.imgur.com/OnL6Lgc.jpg)

![](https://i.imgur.com/v0eTCE0.jpg)

![](https://i.imgur.com/SWf4uAH.jpg)

![](https://i.imgur.com/jzxeaAM.jpg)

4. Accept the transaction by pressing both buttons on &quot;Approve&quot; screen.

![](https://i.imgur.com/YDvVEQV.jpg)

![](https://i.imgur.com/jDupcLj.png)

# Where to find account balance

You can use `hive-ledger-cli` package to check any account balance:

```
$ hive-ledger-cli balance test.ledger
```
Will produce
```
Balances for @test.ledger account
----------------
Liquid:  0.998 HIVE, 0.001 HBD
Savings: 0.000 HIVE, 0.000 HBD
Staked:  0.001 HP
----------------
Pending rewards: 0.000 HP, 0.000 HIVE, 0.000 HBD
```

You can view your account balance on multiple block explorer and Hive powered social media frontends. Most of them support `/@username` standard. For example:

https://hiveblocks.com/@test.ledger
https://hiveblockexplorer.com/@test.ledger
https://hive.ausbit.dev/@test.ledger
https://peakd.com/@test.ledger/wallet
https://wallet.hive.blog/@test.ledger

***
&lt;center&gt;&lt;sup&gt;Originally posted on [Hive Ledger Support](http://blog.hiveledger.io/how-to-use-your-ledger-device-with-hive-ledger-cli). Hive blog powered by [ENGRAVE](https://dblog.org).&lt;/sup&gt;&lt;/center&gt;&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@engrave&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/how-to-use-your-ledger-device-with-hive-ledger-cli&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-engrave-how-to-use-your-ledger-device-with-hive-ledger-cli').html();
      const outputElem = $('#content-engrave-how-to-use-your-ledger-device-with-hive-ledger-cli');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-engrave-how-to-use-your-ledger-device-with-hive-ledger-cli {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-engrave-how-to-use-your-ledger-device-with-hive-ledger-cli code {
    background: white;
  }
  #content-engrave-how-to-use-your-ledger-device-with-hive-ledger-cli a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-engrave-how-to-use-your-ledger-device-with-hive-ledger-cli a:hover {
    border-bottom: 0;
  }
  #content-engrave-how-to-use-your-ledger-device-with-hive-ledger-cli h1 {
    font-size: 2.2em;
  }
  #content-engrave-how-to-use-your-ledger-device-with-hive-ledger-cli h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-engrave-how-to-use-your-ledger-device-with-hive-ledger-cli header small {
    color: #999;
    font-size: 50%;
  }
  #content-engrave-how-to-use-your-ledger-device-with-hive-ledger-cli img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@engrave/how-to-use-your-ledger-device-with-hive-ledger-cli&quot;&gt;How to use your ledger device with hive-ledger-cli&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@engrave&quot;&gt;@engrave&lt;/a&gt;
&lt;/p&gt;</content><author><name>engrave</name></author><category term="howto" /><category term="nodejs" /><category term="wallet" /><summary type="html">![](https://i.imgur.com/PRL7SLu.jpg) The purpose of this post is to fulfill Ledger documentation requirements. It is a guide for advanced users that would like to use Hive application with a CLI (Command-line interface). In the near future I will provide an updated tutorial with user-friendly wallet supporting Ledger Hive application. **Support me with your witness vote! Click on the image below:** [![banner_engrave 100.png](https://images.hive.blog/DQmUghvic5TCPPvVkB4iB7eXmgQ3RQ4L8jkVizvuCBW8RMT/banner_engrave%20100.png)](https://hivesigner.com/sign/account-witness-vote?witness=engrave&amp;amp;approve=1) # Introduction Hardware wallets are considered to be the most secure way to keep your crypto assets. From now on, Ledger Nano S and X can be used to protect your Hive account. This guide will help you protecting your Hive account with keys derived on your ledger device and treat is as a cold wallet. This is the ultimate protection for your account. ## Quickly about Hive Hive is different than most blockchains. It has two native assets: HIVE and HBD (Hive Backed Dollar) and usernames instead of public addresses. Those accounts have different keys associated with different roles (owner, active, posting and memo). Every role can contain different key. To ultimately protect your Hive account it is recommended to replace your Owner key with the one derived from your device - this will protect you from leaking your private key by mistake. # Requirements Before you start, make sure you have * Initialized Ledger device with newest firmware (2.0 for S and 1.3.0 for X) * Hive application installed on your device # Install `hive-ledger-cli` `hive-ledger-cli` is a npm package that can be installed globally on your system. In order to use it, you need to have NodeJS installed on your system. To install `hive-ledger-cli` simply paste ```bash= sudo npm install -g hive-ledger-cli --unsafe-perm ``` Validate your installation with following command which should show you the CLI help ```bash= hive-ledger-cli --help ``` # Get an account **If you already have Hive account, you can skip this part.** Hive accounts are not free - they need to be created by another user and it costs 3 HIVE. There are multiple providers of Hive accounts. Most of them requires some sort of validation (i.e phone or email) and some can utilize different crypto assets (like BTC) to pay the fee. Visit and choose what fits you best: https://signup.hive.io # How to validate public key In order to validate public key derived from a specified [SLIP-0048](https://github.com/satoshilabs/slips/blob/master/slip-0048.md) path, you can use `get-public-key` command: ``` USAGE $ hive-ledger-cli get-public-key PATH ARGUMENTS PATH BIP 32 path to derive key from OPTIONS -c, --confirm Force confirmation on Ledger device -h, --help show CLI help EXAMPLE $ hive-ledger-cli get-public-key &quot;m/48'/13'/0'/0'/0'&quot; Establishing transport with Hive application... done STM5m57x4BXEePAzVNrjUqYeh9C2a7eez1Ya2wPo7ngWLQUdEXjKn ``` 1. Connect your device, unlock and open Hive application. ![](https://i.imgur.com/E5vdx3o.jpg) 3. Select [SLIP-0048](https://github.com/satoshilabs/slips/blob/master/slip-0048.md) path you want to generate and validate public key for. Example key valid for Hive is `m/48'/13'/0'/0'/0'` 4. Type `hive-ledger-cli get-public-key &quot;m/48'/13'/0'/0'/0'&quot; -c` ![](https://i.imgur.com/55oecOv.png) 4. Navigate using left and right buttons to inspect public key. Make sure it's the same that in your terminal ![](https://i.imgur.com/Gnggq0f.jpg) ![](https://i.imgur.com/M8HeJ7v.jpg) ![](https://i.imgur.com/yKceFr5.jpg) ![](https://i.imgur.com/sFxsNIh.jpg) ![](https://i.imgur.com/zYvqRBO.jpg) 6. Confirm public key by pressing both buttons on &quot;Approve&quot; screen. ![](https://i.imgur.com/YDvVEQV.jpg) ![](https://i.imgur.com/7Ied1zR.png) # How to associate Hive account with your Ledger device If you already have your Hive username, you can use your ledger device to ultimately protect it by changing all your keys to the ones derived from your device. In the example below, we will associate existing account called `test.ledger` with the device. You will need current owner private key in Hive supported WIF format. ``` USAGE $ hive-ledger-cli associate-account USERNAME ARGUMENTS USERNAME Account to associate OPTIONS -d, --dry dry run will only print signed transaction instead broadcasting it -h, --help show CLI help -k, --key-gap=key-gap [default: 5] Gap limit for unused key indexes on which the software decides that SLIP-0048 account index is not used -t, --testnet use testnet configuration EXAMPLE $ hive-ledger-cli associate-account test.ledger Found existing account: test.ledger This operation will replace ALL your keys with those from your device. First, we need to find new keys for your owner, active and posting authorities. Press any key to continue or q to exit: Establishing transport with Hive application... done Searching for unused keys on your device... done New owner key: STM7ZEBoDotbYpnyNHdARYMDBMNnLWpV7fiiGa6pvHbXhfRo9ZrDf Derivation path: m/48'/13'/0'/1'/0' Please confirm this public key on your device... done New active key: STM5zpFRqa73yFULSwUYfPSftx4fE7kha9YfkAPR9yKvNLKU2QDFu Derivation path: m/48'/13'/1'/1'/0' Please confirm this public key on your device... done New posting key: STM6YGUesBwuotbZvcfqoXfjQUrfVXGYwJZ9DCHEiFLQfxUsLK9M1 Derivation path: m/48'/13'/4'/1'/0' Please confirm this public key on your device... done Now you need to enter your CURRENT OWNER PRIVATE KEY in WIF format. It will be used to update your account with a new keys. Please be aware that this operation will replace ALL of your current keys Enter your current PRIVATE OWNER KEY for test.ledger: ``` 1. Connect your device, unlock and open Hive application. ![](https://i.imgur.com/E5vdx3o.jpg) 2. Type in your terminal (replace `test.ledger` with your own username) and follow the instruction ```bash hive-ledger-cli associate-account test.ledger ``` 3. Software will derive a new set of public keys and will make sure those are unused by searching in blockchain for those keys. 4. Validate and approve three keys from on your device. ![](https://i.imgur.com/Gnggq0f.jpg) ![](https://i.imgur.com/M8HeJ7v.jpg) ![](https://i.imgur.com/yKceFr5.jpg) ![](https://i.imgur.com/sFxsNIh.jpg) ![](https://i.imgur.com/zYvqRBO.jpg) 6. Confirm public key by pressing both buttons on &quot;Approve&quot; screen. ![](https://i.imgur.com/YDvVEQV.jpg) 5. Repeat this step for next two keys. 6. Enter your current owner private key in WIF format 7. Your account is now associated with your device. You can validate it with [&quot;How to find accounts associated with your device&quot;](#how-to-find-accounts-associated-with-your-device) tutorial. # How to find accounts associated with your device You can quickly check which accounts are associated with your device (those will have owner authority derived from your device). 1. Connect your device, unlock and open Hive application. ![](https://i.imgur.com/E5vdx3o.jpg) 2. Type in your terminal ```bash= hive-ledger-cli discover-accounts ``` 3. Wait for the result with your accounts listed in a table ![](https://i.imgur.com/XRbhz5h.png) # How to receive HIVE or HBD If you know your account username, you can use it as a receiver address from any other wallet or exchange for HIVE or HBD. If you don't remember it, use `hive-ledger-cli` to determine account names associated with your device. ``` USAGE $ hive-ledger-cli discover-accounts [ROLE] ARGUMENTS ROLE (owner|active|memo|posting) [default: owner] Role to check for OPTIONS -a, --account-gap=account-gap [default: 5] Gap limit for unused account indexes after which the software decides that device is not used -d, --dry dry run will only print signed transaction instead broadcasting it -h, --help show CLI help -k, --key-gap=key-gap [default: 5] Gap limit for unused key indexes on which the software decides that device is not used -t, --testnet use testnet configuration EXAMPLE $ hive-ledger-cli discover-accounts ``` 1. Connect your device, unlock and open Hive application. ![](https://i.imgur.com/E5vdx3o.jpg) 3. Type `hive-ledger-cli discover-accounts` and wait for process to finish ![](https://i.imgur.com/rjhOrXf.png) 3. You can see two accounts connected with this ledger: `acronyms` and `test.ledger`. Both can be used to receive funds. 4. You can validate path and public key with [&quot;How to validate public key&quot;](#how-to-validate-public-key) tutorial # How to transfer HIVE or HBD to another account You can send liquid tokens (HIVE or HBD) withing 3 seconds to any other existing Hive account. ``` USAGE $ hive-ledger-cli transfer PATH FROM TO AMOUNT [MEMO] ARGUMENTS PATH BIP 32 (SLIP-0048) path to derive key from and use to sign the transaction FROM sender TO receiver AMOUNT amount to send MEMO optional transfer memo ``` To send `0.001 HIVE` from `test.ledger` to `engrave` account: 1. Connect your device, unlock and open Hive application. ![](https://i.imgur.com/E5vdx3o.jpg) 2. Type in your terminal ```bash= hive-ledger-cli transfer &quot;m/48'/13'/0'/2'/0'&quot; test.ledger engrave &quot;0.001 HIVE&quot; &quot;Sent using ledger device!&quot; ``` ![](https://i.imgur.com/u2NCGSr.png) 3. Use left and right buttons to verify transaction on your hardware wallet. ![](https://i.imgur.com/sh2zh9X.jpg) ![](https://i.imgur.com/nqvYQUi.jpg) ![](https://i.imgur.com/BKg8pW1.jpg) ![](https://i.imgur.com/v0eTCE0.jpg) ![](https://i.imgur.com/toFCSW5.jpg) ![](https://i.imgur.com/jzxeaAM.jpg) ![](https://i.imgur.com/y1UBY9m.jpg) ![](https://i.imgur.com/bxgL0oz.jpg) 4. Accept the transaction by pressing both buttons on &quot;Approve&quot; screen. ![](https://i.imgur.com/YDvVEQV.jpg) ![](https://i.imgur.com/Lus0eYw.png) # How to stake HIVE To gain more governance power over Hive blockchain, you can convert your HIVE into Hive Power. We call it &quot;powering up&quot; and other blockchains calls it &quot;staking&quot;. Underneath, your tokens are converted to VESTS. Withdrawal of your staked HIVE (converting it back to liquid HIVE) will take 13 weeks. To convert HIVE into HP (Hive Power), use `hive-ledger-cli`. ``` USAGE $ hive-ledger-cli transfer-to-vesting PATH FROM TO AMOUNT ARGUMENTS PATH BIP 32 (SLIP-0048) path to derive key from and use to sign the transaction FROM source account TO target account AMOUNT amount to stake OPTIONS -d, --dry dry run will only print signed transaction instead broadcasting it -h, --help show CLI help -t, --testnet use testnet configuration ``` 1. Connect your device, unlock and open Hive application. ![](https://i.imgur.com/E5vdx3o.jpg) 2. Type in your terminal ```bash= hive-ledger-cli transfer-to-vesting &quot;m/48'/13'/0'/2'/0'&quot; test.ledger test.ledger &quot;0.001 HIVE&quot; ``` 3. Use left and right keys to verify transaction on your hardware wallet ![](https://i.imgur.com/sh2zh9X.jpg) ![](https://i.imgur.com/nqvYQUi.jpg) ![](https://i.imgur.com/OnL6Lgc.jpg) ![](https://i.imgur.com/v0eTCE0.jpg) ![](https://i.imgur.com/SWf4uAH.jpg) ![](https://i.imgur.com/jzxeaAM.jpg) 4. Accept the transaction by pressing both buttons on &quot;Approve&quot; screen. ![](https://i.imgur.com/YDvVEQV.jpg) ![](https://i.imgur.com/jDupcLj.png) # Where to find account balance You can use `hive-ledger-cli` package to check any account balance: ``` $ hive-ledger-cli balance test.ledger ``` Will produce ``` Balances for @test.ledger account ---------------- Liquid: 0.998 HIVE, 0.001 HBD Savings: 0.000 HIVE, 0.000 HBD Staked: 0.001 HP ---------------- Pending rewards: 0.000 HP, 0.000 HIVE, 0.000 HBD ``` You can view your account balance on multiple block explorer and Hive powered social media frontends. Most of them support `/@username` standard. For example: https://hiveblocks.com/@test.ledger https://hiveblockexplorer.com/@test.ledger https://hive.ausbit.dev/@test.ledger https://peakd.com/@test.ledger/wallet https://wallet.hive.blog/@test.ledger *** Originally posted on [Hive Ledger Support](http://blog.hiveledger.io/how-to-use-your-ledger-device-with-hive-ledger-cli). Hive blog powered by [ENGRAVE](https://dblog.org). See: How to use your ledger device with hive-ledger-cli by @engrave</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://i.imgur.com/PRL7SLu.jpg" /><media:content medium="image" url="https://i.imgur.com/PRL7SLu.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Simple, Fast &amp;amp; Secure Way To Create Hive Accounts</title><link href="https://hivedocs.info/howto/accounts/2021/11/08/p-19g65wkvqdkbqh39a927ce0d942e4c.html" rel="alternate" type="text/html" title="Simple, Fast &amp;amp; Secure Way To Create Hive Accounts" /><published>2021-11-08T00:00:39-08:00</published><updated>2021-11-08T00:00:39-08:00</updated><id>https://hivedocs.info/howto/accounts/2021/11/08/p-19g65wkvqdkbqh39a927ce0d942e4c</id><content type="html" xml:base="https://hivedocs.info/howto/accounts/2021/11/08/p-19g65wkvqdkbqh39a927ce0d942e4c.html">&lt;div id=&quot;content-therealwolf-p-19g65wkvqdkbqh39a927ce0d942e4c&quot;&gt;![](https://s3.amazonaws.com/inji-media/63402a8d-eeeb-4fc5-b6f5-a445ea208984)

---

If you're looking for a simple, fast &amp;amp; secure way to create Hive accounts, then make sure you check out this brand new service by inji: https://inji.com/create-hive-account

The interface provides you with the option to either claim account tokens and create Hive accounts with them (requires enough Hivepower =&amp;gt; Resource Credits).

Or, you can create Hive accounts instantly for a small payment of $2.99 via PayPal. (Crypto payments will be added)

&lt;sup&gt;&lt;sub&gt;Posted via [inji.com](https://inji.com/hive/@therealwolf/p-19g65wkvqdkbqh39a927ce0d942e4c)&lt;/sub&gt;&lt;/sup&gt;&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@therealwolf&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/p-19g65wkvqdkbqh39a927ce0d942e4c&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-therealwolf-p-19g65wkvqdkbqh39a927ce0d942e4c').html();
      const outputElem = $('#content-therealwolf-p-19g65wkvqdkbqh39a927ce0d942e4c');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-therealwolf-p-19g65wkvqdkbqh39a927ce0d942e4c {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-therealwolf-p-19g65wkvqdkbqh39a927ce0d942e4c code {
    background: white;
  }
  #content-therealwolf-p-19g65wkvqdkbqh39a927ce0d942e4c a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-therealwolf-p-19g65wkvqdkbqh39a927ce0d942e4c a:hover {
    border-bottom: 0;
  }
  #content-therealwolf-p-19g65wkvqdkbqh39a927ce0d942e4c h1 {
    font-size: 2.2em;
  }
  #content-therealwolf-p-19g65wkvqdkbqh39a927ce0d942e4c h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-therealwolf-p-19g65wkvqdkbqh39a927ce0d942e4c header small {
    color: #999;
    font-size: 50%;
  }
  #content-therealwolf-p-19g65wkvqdkbqh39a927ce0d942e4c img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-123456/@therealwolf/p-19g65wkvqdkbqh39a927ce0d942e4c&quot;&gt;Simple, Fast &amp;amp; Secure Way To Create Hive Accounts&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@therealwolf&quot;&gt;@therealwolf&lt;/a&gt;
&lt;/p&gt;</content><author><name>therealwolf</name></author><category term="howto" /><category term="accounts" /><summary type="html">![](https://s3.amazonaws.com/inji-media/63402a8d-eeeb-4fc5-b6f5-a445ea208984) --- If you're looking for a simple, fast &amp;amp; secure way to create Hive accounts, then make sure you check out this brand new service by inji: https://inji.com/create-hive-account The interface provides you with the option to either claim account tokens and create Hive accounts with them (requires enough Hivepower =&amp;gt; Resource Credits). Or, you can create Hive accounts instantly for a small payment of $2.99 via PayPal. (Crypto payments will be added) Posted via [inji.com](https://inji.com/hive/@therealwolf/p-19g65wkvqdkbqh39a927ce0d942e4c) See: Simple, Fast &amp;amp; Secure Way To Create Hive Accounts by @therealwolf</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hivedocs.info/assets/images/favicon.png" /><media:content medium="image" url="https://hivedocs.info/assets/images/favicon.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">27th update of 2021 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2021/10/29/27th-update-of-2021-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="27th update of 2021 on BlockTrades work on Hive software" /><published>2021-10-29T09:05:57-07:00</published><updated>2021-10-29T09:05:57-07:00</updated><id>https://hivedocs.info/news/core/development/2021/10/29/27th-update-of-2021-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/10/29/27th-update-of-2021-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-27th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)
Below is a list of some of the Hive-related programming issues worked on by BlockTrades team during the past week:

# Hived work (blockchain node software)

Most of our work in the past week has focused on HAF, and the hived portion of HAF (sql_serializer plugin) is mostly done now, so we didn’t work on hived much this week. 

We did more testing of the sql_serializer plugin on different servers, but we lost some time to hardware problems on the new server (finally resolved now, we had to replace a bad IO card used by the 4xnvme raid drive subsystem because one of the 4 channels was operating too slowly and destroying the performance of the overall raid0 drive).

We merged in a fix to ensure that the account_created_operation is generated in all cases where an account is created: https://gitlab.syncad.com/hive/hive/-/merge_requests/296

We fixed a problem with the way operations were stored inside account history: now normal operations contained in the transactions in a block are stored before the virtual operations generated by processing of the block (previously opposite order).

Also as part of HAF-related work, we modified the build process for hived to ensure that shared object targets are created of several libraries (fc, protocol, and schema): https://gitlab.syncad.com/hive/hive/-/merge_requests/298
This was done because these libraries are re-used by the sql_serializer plugin (which has been moved from the hived repo to the new haf repo discussed further down in this post).

As part of continuing work on the Hive Command-Line Interface (CLI) wallet, we added a ctrl-c handler for the non-daemon mode to allow for more graceful shutdown: 
https://gitlab.syncad.com/hive/hive/-/merge_requests/295

We also did a further review of @howo work on rc delegations for hived.

And finally, although no code was written yet, we began reviewing the resource credits code, to see if we can make some quick improvements. So far I can only say that this code is a little more complex mathematically than we originally anticipated.

# Condenser

We deployed a new version of hive.blog where @quochuy had reverted the cache buster, and confirmed that this resulted in a good speedup in the average render time of account avatars. We’re also testing a change he made to show RC levels for accounts, which we’ll probably deploy soon.

# Hive Application Framework: framework for building robust and scalable Hive apps

We continued to do a lot of testing and performance studies of HAF this week, mostly using the HAF account history app. We completed CI tests for HAF itself (build test, linting, plus unit tests for the postgres extension). 

The HAF framework is performing well in our tests so far, so in the past week we have been working to move out of “prototype” mode and into “production” mode with this tech:

### Created [haf repo](https://gitlab.syncad.com/hive/haf) to ensure compatibility of components used for HAF-based apps

One difficulty we noticed during testing was ensuring that our testers deployed compatible versions of multiple components of the framework (for example, changes in the sql_serializer or psql_tools could break a HAF application such as the HAF-based account history app). This didn’t pose serious problems for us, because we have relatively tight communication between our internal team members, but we could see this would be a problem once external developers started to create HAF-based apps.  To solve this problem, we’ve created a new repo called “haf”.

A HAF app developer will create a repo for his application, then add the haf repo as a submodule of his repo. In this way, anyone deploying the app can be sure they are also using the appropriate version of the HAF library (correct version of sql_serializer plugin and psql_tools). In a similar way, the HAF repo has a submodule to a version of hived that it is guaranteed to work with.

### Improving app initialization process (required for efficient app development and testing)

Another issue that we saw when repeatedly testing the account history app was the importance of being able to easily reset a HAF application after a test run. A HAF app walks through a stream of transaction data provided in HAF-generated tables and produces its own tables from this data. After making a change in the way a HAF app processes this stream of data, it is important to have an efficient way to throw away the results of the previous run and start a new one from scratch, retaining the blockchain table data and just discarding the application’s table data. For this process to work simple, and especially when working on a server with multiple HAF apps, it is expected that each HAF will maintain its data in one or more distinct schemas from other HAF apps.

We’ve streamlined and standardized this process now, so that it very simple to reset any HAF-based app’s processing state back to block 0:
* `select * from hive.app_remove_context(‘my_app_context_name’);` to clear the app’s context object which the HAF framework uses to track the last block processed by the app.
* `drop schema my_app_schema cascade;` to only clear the database tables generated by the app while processing the blockchain transactions.

## Considerations on combination of HAF-based apps

We also started thinking this week about how we can potentially combine the functionality of HAF apps. 

The simplest way that two HAF apps can communicate with each other is via their respective APIs (one app can simply make an API call to another app). This is the primary way most Hive-based apps communicate today. But this method does have some weaknesses.

### Problems can arise when communicating apps are out-of-sync

One potential problem is that the two apps can be out-of-sync when it comes to which block they are currently processing. Depending on the use being made of the data, this may or may not cause a problem for the app consuming the data from the source app. This is a well known issue. For example, as a health check, some Hive apps like hivemind can also be asked via API what is the block number of the last block they’ve processed, allowing the calling app can switch to another API data supplier if the hivemind app is too far behind in its block processing to supply reliable data.

### High performance communication via custom SQL queries

Another potential problem of inter-app communication is performance-related: collecting data from an app via its API can be much slower than directly reading and processing its internal data via SQL queries. Since all HAF apps will employ the same primary data representation method (SQL tables) and they will often run on the same HAF server, it will generally be feasible for such apps to have this highly performant access to each other’s tables, assuming the HAF server admin grants read privileges between to an app’s schemas. But with such internal access, it is likely even more important that the two apps are well synced in terms of their internal state by block number, so I’ve been considering ways of ensuring a 1-to-1 block sync between HAF apps.

### Maintaining lockstep sync via a “super app” (blame Bartek for the name of this one)

The simplest way to ensure that two or more HAF apps on a server stay in sync is to create a “super app” that has its own context and calls the block-processing routines for its sub-apps in lockstep. To make it easy to combine HAF apps in this fashion, it would be best to define a standard naming convention and parameter interface for the top-level block processing routine of any HAF app that might be usefully combined with others (probably just about every HAF app, in practice). 

In this methodology, the super app would fetch the next block number to be processed, then pass this block number to each sub-app in an order determined by their inter-dependencies. For example, if app B depends on data from app A, then the super app would first call the block processing routine of app A, then the block processing routine of app B. For obvious reasons, this method can’t work if the two apps are interdependent. 

One weakness of this approach is that by only passing a single block number, it is not possible to rapidly initialize the sub apps via any “massive sync” capability they have (“massive sync” is a mode where a HAF app processes multiple historical blocks at once to improve the speed at which the app can sync up to the current head block). 

It also means that a fast app can’t be run any faster than the apps than that depend on it while it is syncing up (for example, it would be nice to run this fast app in parallel with its dependent apps and let it get ahead of some of them).


### “State per block” and “single state” HAF apps

While thinking about the issue of ensuring block sync between HAF apps and the limitations incurred by lockstep operation, it occurred to me that there are two potentially distinct types of HAF apps: 1) a “state per block” app that keeps a history of internal state as it process each block and 2)  a “single state” app that only maintains its current state at the last block it has processed. 

Even for the app itself, there can be benefits to each approach: the first type of app can do rapid meta-analysis on its state (for example, an account that tracked the balance of an account could quickly provide the data to graph the historical change in the value of the account) whereas by contrast the “single state” app benefits from much smaller data storage requirements. 

In practice, we can see that only apps that maintain small amounts of state data can reasonably be operated as “state per block” apps due to the potential storage requirements. 

But despite this limitation, I think these types of apps may be a useful subset of HAF apps when they generate small amounts of data that is likely to be used by many other HAF apps. Normally, incorporating an app into a super app requires incorporating a copy of that app’s data into the super app. By contrast, the data for a state-per-block app can be shared across multiple super-apps operating asynchronously to each other. These state-per-block apps function similar to the way the sql-serializer generates the raw blockchain data tables, which are also shared across all HAF apps on the server.


### Optimizing HAF-based account history app (Hafah)

This week we continued to do performance testing of hafah. We resolved one issue where the python app was consuming more memory than the C++ app which was causing the python version to fail at somewhat random points in time. Along similar lines, we’ll probably do a little more work to manage how much memory the app uses, in order to allow it to safely run on systems with smaller amounts of memory (we’ve been testing on relatively powerful systems with 128GB + swap space).

We’re still looking into why the python version of hafah is somewhat slower than the C++ version. From observations so far, we can see the python process occasionally bottlenecking the app’s overall performance. One possibility is differences between the libraries used to access the database (pqxx for the C++ app vs sqlalchemy for the python app), so we need to look into the relative performance of these two libraries.

# Hivemind (social media middleware app used by social media frontends like hive.blog)

We made one improvement to hivemind this week: a speedup in the  time to compute account notifications inside the hivemind indexer process (aka hive sync process) . The average time required to process notifications in a block was decreased to 100ms from a previous average time of 170ms.


# Work in progress and upcoming work

* Release a final official version of hivemind with postgres 10 support, then update hivemind CI to start testing using postgres 12 instead of 10. This week we deployed the new version to production and we’ve observed one possibly new bug (an occasional notification missing a valid timestamp) that needs further investigation (the dev who is investigating suspects it was actually introduced earlier when changes were made to how hived processes reversible operations, but it is prudent to delay the new release until we’re sure about that). We’ll probably release an official update to hived soon as well (just for API node operators).
* Run tests to compare results between account history plugin and HAF-based account history apps.
* Cleanup and create more documentation for the HAF code as well as recommended guidelines for creating HAF-based apps.
* Finish setup of continuous integration testing for HAF account history app.
* Experiment with generating the “impacted accounts” table directly from sql_serializer to see if it is faster than our current method where hafah generates this data on demand as it needs it.
* Test and benchmark multi-threaded jsonrpc server for HAF apps.
* Finish conversion of hivemind to HAF-based app. Once we’re further along with HAF-based hivemind, we’ll test it using the fork-inducing tool.
* Continue work on speedup of TestTools-based tests.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/27th-update-of-2021-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-27th-update-of-2021-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-27th-update-of-2021-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-27th-update-of-2021-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-27th-update-of-2021-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-27th-update-of-2021-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-27th-update-of-2021-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-27th-update-of-2021-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-27th-update-of-2021-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-27th-update-of-2021-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-27th-update-of-2021-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/27th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;27th update of 2021 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below is a list of some of the Hive-related programming issues worked on by BlockTrades team during the past week: # Hived work (blockchain node software) Most of our work in the past week has focused on HAF, and the hived portion of HAF (sql_serializer plugin) is mostly done now, so we didn’t work on hived much this week. We did more testing of the sql_serializer plugin on different servers, but we lost some time to hardware problems on the new server (finally resolved now, we had to replace a bad IO card used by the 4xnvme raid drive subsystem because one of the 4 channels was operating too slowly and destroying the performance of the overall raid0 drive). We merged in a fix to ensure that the account_created_operation is generated in all cases where an account is created: https://gitlab.syncad.com/hive/hive/-/merge_requests/296 We fixed a problem with the way operations were stored inside account history: now normal operations contained in the transactions in a block are stored before the virtual operations generated by processing of the block (previously opposite order). Also as part of HAF-related work, we modified the build process for hived to ensure that shared object targets are created of several libraries (fc, protocol, and schema): https://gitlab.syncad.com/hive/hive/-/merge_requests/298 This was done because these libraries are re-used by the sql_serializer plugin (which has been moved from the hived repo to the new haf repo discussed further down in this post). As part of continuing work on the Hive Command-Line Interface (CLI) wallet, we added a ctrl-c handler for the non-daemon mode to allow for more graceful shutdown: https://gitlab.syncad.com/hive/hive/-/merge_requests/295 We also did a further review of @howo work on rc delegations for hived. And finally, although no code was written yet, we began reviewing the resource credits code, to see if we can make some quick improvements. So far I can only say that this code is a little more complex mathematically than we originally anticipated. # Condenser We deployed a new version of hive.blog where @quochuy had reverted the cache buster, and confirmed that this resulted in a good speedup in the average render time of account avatars. We’re also testing a change he made to show RC levels for accounts, which we’ll probably deploy soon. # Hive Application Framework: framework for building robust and scalable Hive apps We continued to do a lot of testing and performance studies of HAF this week, mostly using the HAF account history app. We completed CI tests for HAF itself (build test, linting, plus unit tests for the postgres extension). The HAF framework is performing well in our tests so far, so in the past week we have been working to move out of “prototype” mode and into “production” mode with this tech: ### Created [haf repo](https://gitlab.syncad.com/hive/haf) to ensure compatibility of components used for HAF-based apps One difficulty we noticed during testing was ensuring that our testers deployed compatible versions of multiple components of the framework (for example, changes in the sql_serializer or psql_tools could break a HAF application such as the HAF-based account history app). This didn’t pose serious problems for us, because we have relatively tight communication between our internal team members, but we could see this would be a problem once external developers started to create HAF-based apps. To solve this problem, we’ve created a new repo called “haf”. A HAF app developer will create a repo for his application, then add the haf repo as a submodule of his repo. In this way, anyone deploying the app can be sure they are also using the appropriate version of the HAF library (correct version of sql_serializer plugin and psql_tools). In a similar way, the HAF repo has a submodule to a version of hived that it is guaranteed to work with. ### Improving app initialization process (required for efficient app development and testing) Another issue that we saw when repeatedly testing the account history app was the importance of being able to easily reset a HAF application after a test run. A HAF app walks through a stream of transaction data provided in HAF-generated tables and produces its own tables from this data. After making a change in the way a HAF app processes this stream of data, it is important to have an efficient way to throw away the results of the previous run and start a new one from scratch, retaining the blockchain table data and just discarding the application’s table data. For this process to work simple, and especially when working on a server with multiple HAF apps, it is expected that each HAF will maintain its data in one or more distinct schemas from other HAF apps. We’ve streamlined and standardized this process now, so that it very simple to reset any HAF-based app’s processing state back to block 0: * `select * from hive.app_remove_context(‘my_app_context_name’);` to clear the app’s context object which the HAF framework uses to track the last block processed by the app. * `drop schema my_app_schema cascade;` to only clear the database tables generated by the app while processing the blockchain transactions. ## Considerations on combination of HAF-based apps We also started thinking this week about how we can potentially combine the functionality of HAF apps. The simplest way that two HAF apps can communicate with each other is via their respective APIs (one app can simply make an API call to another app). This is the primary way most Hive-based apps communicate today. But this method does have some weaknesses. ### Problems can arise when communicating apps are out-of-sync One potential problem is that the two apps can be out-of-sync when it comes to which block they are currently processing. Depending on the use being made of the data, this may or may not cause a problem for the app consuming the data from the source app. This is a well known issue. For example, as a health check, some Hive apps like hivemind can also be asked via API what is the block number of the last block they’ve processed, allowing the calling app can switch to another API data supplier if the hivemind app is too far behind in its block processing to supply reliable data. ### High performance communication via custom SQL queries Another potential problem of inter-app communication is performance-related: collecting data from an app via its API can be much slower than directly reading and processing its internal data via SQL queries. Since all HAF apps will employ the same primary data representation method (SQL tables) and they will often run on the same HAF server, it will generally be feasible for such apps to have this highly performant access to each other’s tables, assuming the HAF server admin grants read privileges between to an app’s schemas. But with such internal access, it is likely even more important that the two apps are well synced in terms of their internal state by block number, so I’ve been considering ways of ensuring a 1-to-1 block sync between HAF apps. ### Maintaining lockstep sync via a “super app” (blame Bartek for the name of this one) The simplest way to ensure that two or more HAF apps on a server stay in sync is to create a “super app” that has its own context and calls the block-processing routines for its sub-apps in lockstep. To make it easy to combine HAF apps in this fashion, it would be best to define a standard naming convention and parameter interface for the top-level block processing routine of any HAF app that might be usefully combined with others (probably just about every HAF app, in practice). In this methodology, the super app would fetch the next block number to be processed, then pass this block number to each sub-app in an order determined by their inter-dependencies. For example, if app B depends on data from app A, then the super app would first call the block processing routine of app A, then the block processing routine of app B. For obvious reasons, this method can’t work if the two apps are interdependent. One weakness of this approach is that by only passing a single block number, it is not possible to rapidly initialize the sub apps via any “massive sync” capability they have (“massive sync” is a mode where a HAF app processes multiple historical blocks at once to improve the speed at which the app can sync up to the current head block). It also means that a fast app can’t be run any faster than the apps than that depend on it while it is syncing up (for example, it would be nice to run this fast app in parallel with its dependent apps and let it get ahead of some of them). ### “State per block” and “single state” HAF apps While thinking about the issue of ensuring block sync between HAF apps and the limitations incurred by lockstep operation, it occurred to me that there are two potentially distinct types of HAF apps: 1) a “state per block” app that keeps a history of internal state as it process each block and 2) a “single state” app that only maintains its current state at the last block it has processed. Even for the app itself, there can be benefits to each approach: the first type of app can do rapid meta-analysis on its state (for example, an account that tracked the balance of an account could quickly provide the data to graph the historical change in the value of the account) whereas by contrast the “single state” app benefits from much smaller data storage requirements. In practice, we can see that only apps that maintain small amounts of state data can reasonably be operated as “state per block” apps due to the potential storage requirements. But despite this limitation, I think these types of apps may be a useful subset of HAF apps when they generate small amounts of data that is likely to be used by many other HAF apps. Normally, incorporating an app into a super app requires incorporating a copy of that app’s data into the super app. By contrast, the data for a state-per-block app can be shared across multiple super-apps operating asynchronously to each other. These state-per-block apps function similar to the way the sql-serializer generates the raw blockchain data tables, which are also shared across all HAF apps on the server. ### Optimizing HAF-based account history app (Hafah) This week we continued to do performance testing of hafah. We resolved one issue where the python app was consuming more memory than the C++ app which was causing the python version to fail at somewhat random points in time. Along similar lines, we’ll probably do a little more work to manage how much memory the app uses, in order to allow it to safely run on systems with smaller amounts of memory (we’ve been testing on relatively powerful systems with 128GB + swap space). We’re still looking into why the python version of hafah is somewhat slower than the C++ version. From observations so far, we can see the python process occasionally bottlenecking the app’s overall performance. One possibility is differences between the libraries used to access the database (pqxx for the C++ app vs sqlalchemy for the python app), so we need to look into the relative performance of these two libraries. # Hivemind (social media middleware app used by social media frontends like hive.blog) We made one improvement to hivemind this week: a speedup in the time to compute account notifications inside the hivemind indexer process (aka hive sync process) . The average time required to process notifications in a block was decreased to 100ms from a previous average time of 170ms. # Work in progress and upcoming work * Release a final official version of hivemind with postgres 10 support, then update hivemind CI to start testing using postgres 12 instead of 10. This week we deployed the new version to production and we’ve observed one possibly new bug (an occasional notification missing a valid timestamp) that needs further investigation (the dev who is investigating suspects it was actually introduced earlier when changes were made to how hived processes reversible operations, but it is prudent to delay the new release until we’re sure about that). We’ll probably release an official update to hived soon as well (just for API node operators). * Run tests to compare results between account history plugin and HAF-based account history apps. * Cleanup and create more documentation for the HAF code as well as recommended guidelines for creating HAF-based apps. * Finish setup of continuous integration testing for HAF account history app. * Experiment with generating the “impacted accounts” table directly from sql_serializer to see if it is faster than our current method where hafah generates this data on demand as it needs it. * Test and benchmark multi-threaded jsonrpc server for HAF apps. * Finish conversion of hivemind to HAF-based app. Once we’re further along with HAF-based hivemind, we’ll test it using the fork-inducing tool. * Continue work on speedup of TestTools-based tests. See: 27th update of 2021 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">26th update of 2021 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2021/10/18/26th-update-of-2021-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="26th update of 2021 on BlockTrades work on Hive software" /><published>2021-10-18T22:38:06-07:00</published><updated>2021-10-18T22:38:06-07:00</updated><id>https://hivedocs.info/news/core/development/2021/10/18/26th-update-of-2021-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/10/18/26th-update-of-2021-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-26th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)
Below is a list of some of the Hive-related programming issues worked on by BlockTrades team during the past two weeks:

# Hived work (blockchain node software)

### Hive nodes operating stably throughout past weeks

I recently heard reports that instability in the hive network had led to some failed hive-engine trading transactions, but we haven’t observed any problems in the operation of hived nodes. This can easily be seen in that normal hive transactions haven’t had any similar problems. 

Based on my previous review of hive-engine transactions, I believe the problem is that some of these transactions are just too big to fit into partially filled hive blocks because they contain large numbers of proposed trades.

If I’m correct, there’s only two ways I can see to address this problem: increase the size of Hive blocks or reduce the size of these transactions. In the near term, I believe the optimal solution would be for hive-engine servers to use encoding techniques when creating these transactions to reduce their size.

### Code to set new HBD haircut ratio and soft limit completed and tested

We completed the code and tests associated with changing the hard and soft limits for HBD supply, so I’ll start collecting final feedback soon on where we should set the new limits for HF26. 

We’ve tentatively set the new hard limit to when HBD supply reaches 30% of the virtual supply of Hive (in the current hardfork it is set at 10%). 

And we’ve set the both the soft range limit (the starting and ending point where post rewards begin to change from HBD to liquid Hive) at 20%. In other words, post rewards would immediately switch from paying HBD to paying only liquid Hive at 20% instead of gradually shifting between the two currencies as the debt ratio increases.

### Added C++ linter to hived build-and-test system (CI)

We modified the docker builder for hived to include clang-tidy linting and fixed resulting lint warning reported by non-optimal copy of a shared pointer: https://gitlab.syncad.com/hive/hive/-/merge_requests/291
Modify CmakeLists to enforce requirement for clang lint tools to be installed:
https://gitlab.syncad.com/hive/hive/-/merge_requests/286

### sql_serializer plugin (writes blockchain data to a HAF database)

Our primary hived work this week was focused on testing, benchmarking, and making improvements to the SQL serializer plugin and HAF-based account history app:

* [Invalid op_in_trx value in account history when multiple operations are included in the same transaction](https://gitlab.syncad.com/hive/hive/-/issues/184): https://gitlab.syncad.com/hive/hive/-/merge_requests/289/diffs
* [Fix SQL serializer plugin to match above change.](https://gitlab.syncad.com/hive/hive/-/merge_requests/290)
* [Fixed places where virtual operation account_created_operation wasn’t emitted (for example, for accounts defined in genesis block):](https://gitlab.syncad.com/hive/hive/-/merge_requests/296)
* [Removed block_vops counter from sql_serializer (now it is emitted as part of the virtual op notification itself).](https://gitlab.syncad.com/hive/hive/-/merge_requests/293/diffs)

### sql_serializer performance testing

We re-tested the sql_serializer syncing to head block performance after all the bug fixes were verified and there were no performance regressions. 

We’ve setup a bunch of new fast servers in our in-house datacenter to speed up our verification and benchmarking tests and we’ve just started experimenting with how fast we can get the IO systems for these systems on a reasonable budget using software RAID on mid- and high-end 2TB NVME drives (Force mp600, mp600 core, mp600 pro, Samsung 980, and possibly the rather expensive mp600 pro xt) with varying numbers of drives in the RAID array and with various distributions of the table space inside the HAF database between drives.

On our fastest system currently (an AMD5950X128GB with a 4xSamsung 980 RAID0 drive) it took 25594s to reach 58M blocks and restore database indexes. On a similar system with slower IO (an AMD5950X 128GBwith a 3x force mp600 RAID0 drive) it took 27351s. 

On both these systems, performance seems to be set by a mix of CPU speed and IO speed, but on systems with more typical drives, disk IO speed will likely be the dominant factor as a full sync to 58M blocks creates a 1.6TB database.

Eventually we’ll also test on some slower systems, but for now we’re testing on our fastest systems when we do full syncs (we only sync to 5M blocks on our slower systems) as our primary testing goal right now is to check for bugs in the code and the tests are time-consuming even on our fastest machines.

# Moved image server to much bigger (and much more expensive) server

We were almost out of disk space on our existing image server (only has 36TB of storage with RAID setup) so we’ve been migrating the images to a new server with a 168TB RAID drive. We completed the handoff to the new server this weekend and worked thru some minor issues that resulted (tuning the caches appropriately, fix a rate-limiting issue between the new server and api.hive.blog, etc). If you noticed any issues rendering or uploading an image this weekend, you were likely observing us at work.

During this process we noticed that the cache-busting code added to condenser was negatively impacting Cloudflare’s CDN-based caching (this became more obvious during our performance testing with the new image server because it is located further away from our US office and cache misses were more painful due to network latency), so we asked @quochuy to revert that change (which he has already done and will be deployed to production tomorrow). Once that change is deployed, I expect that avatars on hive.blog will render about 2x faster on average.

We also noticed in this testing that we could potentially reduce the delays incurred by cache misses in the future by creating a simple HAF-based app to locally maintain the link between hive accounts and hive avatars and avoid the current call to database.get_accounts that the image server makes to a remote hived node. In this scenario, a remote HAF server would keep a “hive account” to “hive avatar” mapping table and push the occasional table updates to a local replication of the mapping table on the image server. I think this will make a nice and simple “starter” task for someone looking to create their first HAF app

# Hive Application Framework: framework for building robust and scalable Hive apps

### Fixing/Optimizing HAF-based account history app (Hafah)

We found and fixed several bugs in the HAF software ecosystem (sql_serializer, hive_fork_manager, account history app) this week. We completed a full sync to head block using 7 sending threads and 7 receiving threads on both the C++-based account history app (took 19730s) and the newer, python-based account history app (took 22021s). 

So the C++ version is currently a little over 11% faster at syncing than the python version. Ideally we’ll be able to tune the python version to achieve the same speed as the C++ version, in which case we’ll be able to drop the C++ version and just maintain the python version in the future. And it’s likely that whatever knowledge we gain during that analysis will be useful for future python-based HAF apps as well.

# Upcoming work

* Release a final official version of hivemind with postgres 10 support, then update hivemind CI to start testing using postgres 12 instead of 10. We finished a full sync to headblock of the new version and next @gandalf will deploy it for production testing tomorrow. I don’t expect any problems, so we’ll probably officially recommend API servers to upgrade to the new version this week.
* Run new tests to compare results between account history plugin and HAF-based account history apps.
* Simplify build and installation of HAF-based apps and create a repo with HAF components as submodules to track version requirements between HAF components.
* Finish setup of continuous integration testing for HAF account history app.
* Test and benchmark multi-threaded jsonrpc server for HAF apps.
* Finish conversion of hivemind to HAF-based app (didn’t get back to this task last week). Once we’re further along with HAF-based hivemind, we’ll test it using the fork-inducing tool.
* Continue work on speedup of TestTools-based tests.

# Schedule predictions (always a bit dangerous)

At this point I’m fairly confident we’ll be able to release HAF for production use by the end of this month. Since HAF doesn’t impact hived consensus, it can be released whenever it is ready, without requiring a hardfork.

As for hardfork 26 itself,  it is still scheduled for December/January time frame (we’ll set an official date early next month). We’ve got two HF-related tasks we still haven’t started on, but I don’t think they will be too difficult: 1) make some simple “low-hanging fruit” improvements to RC calculations (for example, we’ve seen that some operations of varying size don’t get charged based on the byte size of the operation) and 2) allow asset-related transactions to use numeric asset identifiers (aka NAIs) instead of strings as part of the process of deprecating string-based asset identifiers. I’m confident we can complete the first task in time for the hardfork, and I’m reasonably confident we can complete the second task as well.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/26th-update-of-2021-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-26th-update-of-2021-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-26th-update-of-2021-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-26th-update-of-2021-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-26th-update-of-2021-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-26th-update-of-2021-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-26th-update-of-2021-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-26th-update-of-2021-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-26th-update-of-2021-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-26th-update-of-2021-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-26th-update-of-2021-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/26th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;26th update of 2021 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below is a list of some of the Hive-related programming issues worked on by BlockTrades team during the past two weeks: # Hived work (blockchain node software) ### Hive nodes operating stably throughout past weeks I recently heard reports that instability in the hive network had led to some failed hive-engine trading transactions, but we haven’t observed any problems in the operation of hived nodes. This can easily be seen in that normal hive transactions haven’t had any similar problems. Based on my previous review of hive-engine transactions, I believe the problem is that some of these transactions are just too big to fit into partially filled hive blocks because they contain large numbers of proposed trades. If I’m correct, there’s only two ways I can see to address this problem: increase the size of Hive blocks or reduce the size of these transactions. In the near term, I believe the optimal solution would be for hive-engine servers to use encoding techniques when creating these transactions to reduce their size. ### Code to set new HBD haircut ratio and soft limit completed and tested We completed the code and tests associated with changing the hard and soft limits for HBD supply, so I’ll start collecting final feedback soon on where we should set the new limits for HF26. We’ve tentatively set the new hard limit to when HBD supply reaches 30% of the virtual supply of Hive (in the current hardfork it is set at 10%). And we’ve set the both the soft range limit (the starting and ending point where post rewards begin to change from HBD to liquid Hive) at 20%. In other words, post rewards would immediately switch from paying HBD to paying only liquid Hive at 20% instead of gradually shifting between the two currencies as the debt ratio increases. ### Added C++ linter to hived build-and-test system (CI) We modified the docker builder for hived to include clang-tidy linting and fixed resulting lint warning reported by non-optimal copy of a shared pointer: https://gitlab.syncad.com/hive/hive/-/merge_requests/291 Modify CmakeLists to enforce requirement for clang lint tools to be installed: https://gitlab.syncad.com/hive/hive/-/merge_requests/286 ### sql_serializer plugin (writes blockchain data to a HAF database) Our primary hived work this week was focused on testing, benchmarking, and making improvements to the SQL serializer plugin and HAF-based account history app: * [Invalid op_in_trx value in account history when multiple operations are included in the same transaction](https://gitlab.syncad.com/hive/hive/-/issues/184): https://gitlab.syncad.com/hive/hive/-/merge_requests/289/diffs * [Fix SQL serializer plugin to match above change.](https://gitlab.syncad.com/hive/hive/-/merge_requests/290) * [Fixed places where virtual operation account_created_operation wasn’t emitted (for example, for accounts defined in genesis block):](https://gitlab.syncad.com/hive/hive/-/merge_requests/296) * [Removed block_vops counter from sql_serializer (now it is emitted as part of the virtual op notification itself).](https://gitlab.syncad.com/hive/hive/-/merge_requests/293/diffs) ### sql_serializer performance testing We re-tested the sql_serializer syncing to head block performance after all the bug fixes were verified and there were no performance regressions. We’ve setup a bunch of new fast servers in our in-house datacenter to speed up our verification and benchmarking tests and we’ve just started experimenting with how fast we can get the IO systems for these systems on a reasonable budget using software RAID on mid- and high-end 2TB NVME drives (Force mp600, mp600 core, mp600 pro, Samsung 980, and possibly the rather expensive mp600 pro xt) with varying numbers of drives in the RAID array and with various distributions of the table space inside the HAF database between drives. On our fastest system currently (an AMD5950X128GB with a 4xSamsung 980 RAID0 drive) it took 25594s to reach 58M blocks and restore database indexes. On a similar system with slower IO (an AMD5950X 128GBwith a 3x force mp600 RAID0 drive) it took 27351s. On both these systems, performance seems to be set by a mix of CPU speed and IO speed, but on systems with more typical drives, disk IO speed will likely be the dominant factor as a full sync to 58M blocks creates a 1.6TB database. Eventually we’ll also test on some slower systems, but for now we’re testing on our fastest systems when we do full syncs (we only sync to 5M blocks on our slower systems) as our primary testing goal right now is to check for bugs in the code and the tests are time-consuming even on our fastest machines. # Moved image server to much bigger (and much more expensive) server We were almost out of disk space on our existing image server (only has 36TB of storage with RAID setup) so we’ve been migrating the images to a new server with a 168TB RAID drive. We completed the handoff to the new server this weekend and worked thru some minor issues that resulted (tuning the caches appropriately, fix a rate-limiting issue between the new server and api.hive.blog, etc). If you noticed any issues rendering or uploading an image this weekend, you were likely observing us at work. During this process we noticed that the cache-busting code added to condenser was negatively impacting Cloudflare’s CDN-based caching (this became more obvious during our performance testing with the new image server because it is located further away from our US office and cache misses were more painful due to network latency), so we asked @quochuy to revert that change (which he has already done and will be deployed to production tomorrow). Once that change is deployed, I expect that avatars on hive.blog will render about 2x faster on average. We also noticed in this testing that we could potentially reduce the delays incurred by cache misses in the future by creating a simple HAF-based app to locally maintain the link between hive accounts and hive avatars and avoid the current call to database.get_accounts that the image server makes to a remote hived node. In this scenario, a remote HAF server would keep a “hive account” to “hive avatar” mapping table and push the occasional table updates to a local replication of the mapping table on the image server. I think this will make a nice and simple “starter” task for someone looking to create their first HAF app # Hive Application Framework: framework for building robust and scalable Hive apps ### Fixing/Optimizing HAF-based account history app (Hafah) We found and fixed several bugs in the HAF software ecosystem (sql_serializer, hive_fork_manager, account history app) this week. We completed a full sync to head block using 7 sending threads and 7 receiving threads on both the C++-based account history app (took 19730s) and the newer, python-based account history app (took 22021s). So the C++ version is currently a little over 11% faster at syncing than the python version. Ideally we’ll be able to tune the python version to achieve the same speed as the C++ version, in which case we’ll be able to drop the C++ version and just maintain the python version in the future. And it’s likely that whatever knowledge we gain during that analysis will be useful for future python-based HAF apps as well. # Upcoming work * Release a final official version of hivemind with postgres 10 support, then update hivemind CI to start testing using postgres 12 instead of 10. We finished a full sync to headblock of the new version and next @gandalf will deploy it for production testing tomorrow. I don’t expect any problems, so we’ll probably officially recommend API servers to upgrade to the new version this week. * Run new tests to compare results between account history plugin and HAF-based account history apps. * Simplify build and installation of HAF-based apps and create a repo with HAF components as submodules to track version requirements between HAF components. * Finish setup of continuous integration testing for HAF account history app. * Test and benchmark multi-threaded jsonrpc server for HAF apps. * Finish conversion of hivemind to HAF-based app (didn’t get back to this task last week). Once we’re further along with HAF-based hivemind, we’ll test it using the fork-inducing tool. * Continue work on speedup of TestTools-based tests. # Schedule predictions (always a bit dangerous) At this point I’m fairly confident we’ll be able to release HAF for production use by the end of this month. Since HAF doesn’t impact hived consensus, it can be released whenever it is ready, without requiring a hardfork. As for hardfork 26 itself, it is still scheduled for December/January time frame (we’ll set an official date early next month). We’ve got two HF-related tasks we still haven’t started on, but I don’t think they will be too difficult: 1) make some simple “low-hanging fruit” improvements to RC calculations (for example, we’ve seen that some operations of varying size don’t get charged based on the byte size of the operation) and 2) allow asset-related transactions to use numeric asset identifiers (aka NAIs) instead of strings as part of the process of deprecating string-based asset identifiers. I’m confident we can complete the first task in time for the hardfork, and I’m reasonably confident we can complete the second task as well. See: 26th update of 2021 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>