<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://hivedocs.info/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hivedocs.info/" rel="alternate" type="text/html" /><updated>2021-07-31T12:23:43-07:00</updated><id>https://hivedocs.info/feed.xml</id><title type="html">Hive Chain Documentation</title><subtitle>Your resource for various levels of Hive Documentation.</subtitle><author><name>site curated by: @inertia</name></author><entry><title type="html">20th update of 2021 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2021/07/29/20th-update-of-2021-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="20th update of 2021 on BlockTrades work on Hive software" /><published>2021-07-29T14:42:09-07:00</published><updated>2021-07-29T14:42:09-07:00</updated><id>https://hivedocs.info/news/core/development/2021/07/29/20th-update-of-2021-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/07/29/20th-update-of-2021-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)

Below is a list of Hive-related programming issues worked on by BlockTrades team during last week or so:

# Hived work (blockchain node software)

### Continued development of TestTools framework
We made a substantial number of changes to the new TestTools framework that we are now using to do blackbox testing on hived: https://gitlab.syncad.com/hive/hive/-/merge_requests/278

There are too many changes to discuss individually (see the merge request for full details), but at a high-level, the new testing framework allows us to perform more complex tests faster and more efficiently than the previous framework.


### Command-line interface (CLI) wallet enhancements
We continued work on improvements to CLI wallet (refactoring to remove need for duplicate code to support legacy operations, upgrading cli wallet tests to use TestTools, support for signing of transactions based on authority) and I expect we’ll merge in these changes in the coming week.

### Continuing work on blockhain converter tool
We’re also continuing work on the blockchain converter that generates a testnet blockchain configuration form an existing blocklog. Most recently, we added multithreading support to speed it up and those changes are being tested now. You can follow the work on this task here: https://gitlab.syncad.com/hive/hive/-/commits/tm-blockchain-converter/

# Hivemind (2nd layer applications + social media middleware)


### Reduced peak memory consumption
We reviewed and merged in the changes to reduce memory usage of hive sync (just over 4GB footprint now, whereas previous peak usage was approaching 14GB). Finalized changes that were merged in are here: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/527/diffs


### Optimized update_post_rshares

We completed and merged in the code that I mentioned last week that creates a temporary index for faster execution of the update_post_rshares function and we were finally able to achieve the same speedup in a full sync of hivemind on one of our production systems after some tweaks to the code (function with temporary index processed 52 million blocks in 19 minutes). 

To achieve the same results that we saw in our development testing, we found we had to perform a `vacuum analyze` on the `hive_votes` table prior to running the update_post_rshares function to ensure the query planner had the proper statistics to generate an efficient query plan. The merge request for the optimized code is here:
https://gitlab.syncad.com/hive/hivemind/-/merge_requests/521

### Optimized process of effective_comment_vote operation during massive sync

When we know that a particular post will be paid out before end of massive sync block processing, we can skip processing of effective_comment_vote_operations for such posts. This optimization reduced the amount of post records that we need to flush to the database during massive sync by more than 50%. The merge request for this optimization is here: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/525


### Merged in new hive sync option --max-retries
We also finished review and testing of the new –max-retries option that allows configuring how many retries (or an indefinite number of retries) before the hive sync process will shutdown if it loses contact with the hived serving blockchain data to it: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/526

# Optimized api.hive.blog servers (BlockTrades-supported API node)

While there are number of API nodes available nowadays, many Hive apps default to using our node (which is often useful, since it allows us to quickly spot any scaling issue that might be arising as Hive API traffic increases with time). Here is a graph of how that traffic has increased over the last year:

![image.png](https://images.hive.blog/DQmWoWKQctKxSMTj5YXm7JfmVdZN42c2NCJt28StVKqwLvd/image.png)

As you can see, our incoming API traffic (the top part of the graph) has roughly tripled over the year.

### Solved issue with transaction timeouts

Yesterday we started getting some reports of timeouts on transactions processed via api.hive.blog. The immediate suspicion, of course, was that this was due to the increased traffic coming from splinterlands servers, which turned out to be the case. 

But we weren’t seeing much CPU or IO loading on our servers, despite the increased traffic (in fact, we had substantial headroom there and we could likely easily handle 12x or more than the traffic we were receiving based on CPU bottlenecking just with our existing servers), so this lead us to suspect a network-related problem.

As a quick fix, we tried making some changes to the network configuration parameters of our servers based on recommendations from @mahdiyari, and we had a report from him that this helped some, but we were still getting reports of a fair number of timeouts, so we decided to do a more thorough analysis of the issue.

To properly analyze the network traffic, we first had to make some improvements to the jussi traffic analyzer that we use to analyze loading, because most of the increased traffic was encoding the nature of the request in the post bodies, which means the details of the request weren’t seen by the analyzer tool (e.g. we couldn’t see exactly what types of requests were creating the network problems). With these changes, the jussi analyzer can now distinguish what type of request is being made, allowing us to see which type of requests were slow and/or timing out.

After making this change, we found that the requests that were timing out were mostly transaction broadcasts using the old style “pre-appbase” format. Requests of this type can’t be directly processed by a hived node, but we run a jussi gateway on api.hive.blog that converts these legacy-formatted requests into appbase-formatted requests (effectively making our node backwards-compatible with these old style requests). 

So this was our first clue as to what the real problem was. After investigating how our jussi gateway was configured, we found that the converted requests were being sent to our hived nodes using the websocket protocol, unlike most other requests, which were sent via http. So, on a hunch, we modified the configuration of our jussi process to send translated requests as http requests instead of web socket requests and this eliminated the timeouts (as confirmed by both the jussi analyzer, beacon.peakd.com, and individual script testing by devs).

At this point, our node is operating very smoothly, despite signs that traffic has further increased beyond even yesterday’s traffic, so I’m confident we’ve solved all immediate issues.

### Replacement of calls to broadcast_transaction_synchronous with broadcast_transaction

Despite handling traffic fine now, I’m also a bit concerned about the use of broadcast_transaction_synchronous calls. These are blocking calls that tend to hold a connection open for 1.5s or more on average (half a block interval) because they wait for the transaction be included in the blockchain before returning. 

I’ve asked the library devs to look into changing their libraries to begin relying on the newer broadcast_transaction operation (a non-blocking call) and use the transaction_status API call to determine when their transaction has been included into the block, which should eliminate the large number of open connections that can occur when many transactions are being broadcast at once.

@mahdiyari has already made changes to hive-js (the Javascript library for hive apps) along these lines (as well as replacing the use of legacy-formatted API calls with appbase formatted API calls) and apps developers are now beginning to test their apps with this beta version of hive-js. Assuming this process goes smoothly, I anticipate that other library devs will swiftly follow suit.

# Hive Application Framework (HAF)

Some enhancements to HAF have been made to support synchronizing of multiple Hive apps operating on a HAF server. This allows an app that relies on the data of other apps to be sure that those apps have   processed all blocks up to the point where the dependent app is currently working at. This enhancement also involved support for secure sharing of data between HAF apps on a HAF server (for example, a dependent app can read, but not write, data in the tables of the app it depends on).

As I understand it, we now have a sample application built using HAF, but I haven’t had a chance to review it yet, as I was busy yesterday analyzing and optimizing our web infrastructure with our infrastructure team as discussed above. But this is high on my personal priority list, so I plan to review the work done here soon.

# What’s next?

We’ve resumed work on the sql_serializer plugin, which is one of the last key pieces needed before we can release HAF. Once those changes are completed, we’ll be able to do an end-to-end test with hived→sql_serializer→hivemind with a full sync of the blockchain. Perhaps I’m optimistic about the resulting speedup, but it’s possible we could have results as early as next Monday. I’m hoping we have some form of HAF ready for early beta testers within a week or two, but bear in mind that is a best case scenario.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/20th-update-of-2021-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-20th-update-of-2021-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/20th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;20th update of 2021 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below is a list of Hive-related programming issues worked on by BlockTrades team during last week or so: # Hived work (blockchain node software) ### Continued development of TestTools framework We made a substantial number of changes to the new TestTools framework that we are now using to do blackbox testing on hived: https://gitlab.syncad.com/hive/hive/-/merge_requests/278 There are too many changes to discuss individually (see the merge request for full details), but at a high-level, the new testing framework allows us to perform more complex tests faster and more efficiently than the previous framework. ### Command-line interface (CLI) wallet enhancements We continued work on improvements to CLI wallet (refactoring to remove need for duplicate code to support legacy operations, upgrading cli wallet tests to use TestTools, support for signing of transactions based on authority) and I expect we’ll merge in these changes in the coming week. ### Continuing work on blockhain converter tool We’re also continuing work on the blockchain converter that generates a testnet blockchain configuration form an existing blocklog. Most recently, we added multithreading support to speed it up and those changes are being tested now. You can follow the work on this task here: https://gitlab.syncad.com/hive/hive/-/commits/tm-blockchain-converter/ # Hivemind (2nd layer applications + social media middleware) ### Reduced peak memory consumption We reviewed and merged in the changes to reduce memory usage of hive sync (just over 4GB footprint now, whereas previous peak usage was approaching 14GB). Finalized changes that were merged in are here: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/527/diffs ### Optimized update_post_rshares We completed and merged in the code that I mentioned last week that creates a temporary index for faster execution of the update_post_rshares function and we were finally able to achieve the same speedup in a full sync of hivemind on one of our production systems after some tweaks to the code (function with temporary index processed 52 million blocks in 19 minutes). To achieve the same results that we saw in our development testing, we found we had to perform a `vacuum analyze` on the `hive_votes` table prior to running the update_post_rshares function to ensure the query planner had the proper statistics to generate an efficient query plan. The merge request for the optimized code is here: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/521 ### Optimized process of effective_comment_vote operation during massive sync When we know that a particular post will be paid out before end of massive sync block processing, we can skip processing of effective_comment_vote_operations for such posts. This optimization reduced the amount of post records that we need to flush to the database during massive sync by more than 50%. The merge request for this optimization is here: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/525 ### Merged in new hive sync option --max-retries We also finished review and testing of the new –max-retries option that allows configuring how many retries (or an indefinite number of retries) before the hive sync process will shutdown if it loses contact with the hived serving blockchain data to it: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/526 # Optimized api.hive.blog servers (BlockTrades-supported API node) While there are number of API nodes available nowadays, many Hive apps default to using our node (which is often useful, since it allows us to quickly spot any scaling issue that might be arising as Hive API traffic increases with time). Here is a graph of how that traffic has increased over the last year: ![image.png](https://images.hive.blog/DQmWoWKQctKxSMTj5YXm7JfmVdZN42c2NCJt28StVKqwLvd/image.png) As you can see, our incoming API traffic (the top part of the graph) has roughly tripled over the year. ### Solved issue with transaction timeouts Yesterday we started getting some reports of timeouts on transactions processed via api.hive.blog. The immediate suspicion, of course, was that this was due to the increased traffic coming from splinterlands servers, which turned out to be the case. But we weren’t seeing much CPU or IO loading on our servers, despite the increased traffic (in fact, we had substantial headroom there and we could likely easily handle 12x or more than the traffic we were receiving based on CPU bottlenecking just with our existing servers), so this lead us to suspect a network-related problem. As a quick fix, we tried making some changes to the network configuration parameters of our servers based on recommendations from @mahdiyari, and we had a report from him that this helped some, but we were still getting reports of a fair number of timeouts, so we decided to do a more thorough analysis of the issue. To properly analyze the network traffic, we first had to make some improvements to the jussi traffic analyzer that we use to analyze loading, because most of the increased traffic was encoding the nature of the request in the post bodies, which means the details of the request weren’t seen by the analyzer tool (e.g. we couldn’t see exactly what types of requests were creating the network problems). With these changes, the jussi analyzer can now distinguish what type of request is being made, allowing us to see which type of requests were slow and/or timing out. After making this change, we found that the requests that were timing out were mostly transaction broadcasts using the old style “pre-appbase” format. Requests of this type can’t be directly processed by a hived node, but we run a jussi gateway on api.hive.blog that converts these legacy-formatted requests into appbase-formatted requests (effectively making our node backwards-compatible with these old style requests). So this was our first clue as to what the real problem was. After investigating how our jussi gateway was configured, we found that the converted requests were being sent to our hived nodes using the websocket protocol, unlike most other requests, which were sent via http. So, on a hunch, we modified the configuration of our jussi process to send translated requests as http requests instead of web socket requests and this eliminated the timeouts (as confirmed by both the jussi analyzer, beacon.peakd.com, and individual script testing by devs). At this point, our node is operating very smoothly, despite signs that traffic has further increased beyond even yesterday’s traffic, so I’m confident we’ve solved all immediate issues. ### Replacement of calls to broadcast_transaction_synchronous with broadcast_transaction Despite handling traffic fine now, I’m also a bit concerned about the use of broadcast_transaction_synchronous calls. These are blocking calls that tend to hold a connection open for 1.5s or more on average (half a block interval) because they wait for the transaction be included in the blockchain before returning. I’ve asked the library devs to look into changing their libraries to begin relying on the newer broadcast_transaction operation (a non-blocking call) and use the transaction_status API call to determine when their transaction has been included into the block, which should eliminate the large number of open connections that can occur when many transactions are being broadcast at once. @mahdiyari has already made changes to hive-js (the Javascript library for hive apps) along these lines (as well as replacing the use of legacy-formatted API calls with appbase formatted API calls) and apps developers are now beginning to test their apps with this beta version of hive-js. Assuming this process goes smoothly, I anticipate that other library devs will swiftly follow suit. # Hive Application Framework (HAF) Some enhancements to HAF have been made to support synchronizing of multiple Hive apps operating on a HAF server. This allows an app that relies on the data of other apps to be sure that those apps have processed all blocks up to the point where the dependent app is currently working at. This enhancement also involved support for secure sharing of data between HAF apps on a HAF server (for example, a dependent app can read, but not write, data in the tables of the app it depends on). As I understand it, we now have a sample application built using HAF, but I haven’t had a chance to review it yet, as I was busy yesterday analyzing and optimizing our web infrastructure with our infrastructure team as discussed above. But this is high on my personal priority list, so I plan to review the work done here soon. # What’s next? We’ve resumed work on the sql_serializer plugin, which is one of the last key pieces needed before we can release HAF. Once those changes are completed, we’ll be able to do an end-to-end test with hived→sql_serializer→hivemind with a full sync of the blockchain. Perhaps I’m optimistic about the resulting speedup, but it’s possible we could have results as early as next Monday. I’m hoping we have some form of HAF ready for early beta testers within a week or two, but bear in mind that is a best case scenario. See: 20th update of 2021 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Hive libraries for Microsoft .NET now fully support broadcasting transactions</title><link href="https://hivedocs.info/news/dotnet/2021/07/26/msnet-libraries-transactions.html" rel="alternate" type="text/html" title="Hive libraries for Microsoft .NET now fully support broadcasting transactions" /><published>2021-07-26T05:56:27-07:00</published><updated>2021-07-26T05:56:27-07:00</updated><id>https://hivedocs.info/news/dotnet/2021/07/26/msnet-libraries-transactions</id><content type="html" xml:base="https://hivedocs.info/news/dotnet/2021/07/26/msnet-libraries-transactions.html">&lt;div id=&quot;content-arcange-msnet-libraries-transactions&quot;&gt;![](https://i.imgur.com/uBDjiIT.png)

Months ago, I introduced the [Hive library for Microsoft .NET  developers](/hive-139531/@arcange/msnet-libraries). This library provides developers used to VB.NET and C# languages with easy-to-use .NET classes, exposing almost all the APIs of the `hived` and `cli_wallet` daemons.

It's also available as a &quot;COM library&quot;  (aka .dll) so people can use it directly in their favorites COM-enabled Windows applications like Word, Excel, ...

### A missing feature

Although the existing library was versatile enough to read information from the blockchain and even record transactions in it, it was however limited to operations that can be sent to the blockchain through the interfaces exposed by the cli_wallet.

Although most of the usual operations are available through that interface, many new operations have been supported by the blockchain but not added to the cli_wallet API (yet).

On the other hand, this way of sending transactions to the blockchain requires that you have a running daemon available at hand, and in case of failure you cannot easily switch to another.

I had wanted for years to have the ability to connect my library directly to API servers that several Witnesses operate (including myself), but so far I have managed to postpone this work. Time to cut it out!

### Taking matters into my own hands

Broadcasting transactions directly to the blockchain means serializing the transaction and signing it before sending it to an RPC node.

Easier said than done because it is an area that was new to me and I know several who have broken their teeth there. Never mind, I like to take up challenges and I often tell myself that nothing is impossible when approached with calm and patience.

Thanks to the Hive developers portal [tutorial about transactions](https://developers.hive.io/tutorials-recipes/how-to-serialize-and-sign-using-js.html), I gradually integrated the understanding of the serialization and cryptography processes required to sign transactions.

As a result, it is no longer necessary to go through cli_wallet to write operations into the blockchain, and **all** available operations can now be broadcasted using the hive.net library.

### Code example

The library is really easy to use. Here is a simple C# code example that broadcast a vote operation to the blockchain:

```
string strPosting = &quot;5...&quot;;   // Posting private key
string strActive = &quot;5...&quot;;    // Active private key

HttpClient oHTTP = new HttpClient();
CHived oHived = new CHived(oHTTP, &quot;https://api.hive.blog&quot;);

COperations.vote oVote = new COperations.vote {
    voter = &quot;arcange&quot;, 
    author = &quot;author&quot;, 
    permlink = &quot;permlink&quot;,
    weight = 100 
    };

try 
{

    string txid = oHived.broadcast_transaction(
        new object[] { oVote }, 
        new string[] { strPosting }
        );

    Console.Write(txid);
}
catch (Exception e)
{
    Console.Write(e.Message);
}
```

Broadcasting multiple operations at once with a single transaction is also very easy:

```
COperations.transfer oTransfer1 = new COperations.transfer { 
    from = &quot;arcange&quot;, 
    to = &quot;account1&quot;, 
    amount = new Asset(&quot;0.001 HIVE&quot;), 
    memo = &quot;A first transfer&quot; 
    };
    
COperations.transfer oTransfer2 = new COperations.transfer { 
    from = &quot;arcange&quot;, 
    to = &quot;account2&quot;, 
    amount = new Asset(&quot;1.000 HBD&quot;), 
    memo = &quot;A second transfer&quot; 
    };
    
try
{
    string txid = oHived.broadcast_transaction(
        new object[] { oTransfer1, oTransfer2 }, 
        new string[] { strActive }
        );
    Console.Write(txid);
}
catch (Exception e)
{
    Console.Write(e.Message);
}
```

Here is another example in VB.NET:

```
Dim strActive = &quot;5....&quot;    ' Posting private key
Dim strPosting = &quot;5....&quot;   ' Active Private key

Dim oHttp As HttpClient = New HttpClient()
Dim oHiveAPI = New CHived(oHttp, &quot;https://api.hive.blog&quot;)

Dim oWitnessUpdate As New COperations.witness_update With {
    .owner = &quot;arcange&quot;,
    .url = &quot;https://....&quot;,
    .block_signing_key = New PublicKey(&quot;STM....&quot;),
    .props = New ChainProperties With {
        .account_creation_fee = New Asset(3, &quot;HIVE&quot;),
        .maximum_block_size = 65536,
        .hbd_interest_rate = 1000
        },
    .fee = New Asset(3, &quot;HIVE&quot;)
    }

Try
    Dim txid = oHiveAPI.broadcast_transaction({oWitnessUpdate}, {strPosting})
    Console.Write(txid)
Catch ex As Exception
    Console.Write(ex.Message)
End Try
```

### What's next?

There is still some work to do:

- add a set of functions to simplify the generation of the most used operations.
- create documentation
- create some tutorials

I plan to tackle these tasks in the next weeks.

### Open source

The libraries are open source and can be found on [GitLab](https://gitlab.syncad.com/hive/hive-net).

If you have any comments or requests, please create an issue on GitLab too. You can also contact me on [Discord](https://discordapp.com/channels/@me/237480770933882881) or [Telegram](https://t.me/the_arcange)

---
&lt;center&gt;

### Check out my apps and services
&lt;a href=&quot;/hive/@hive.engage/stay-connected-with-your-hive-audience-and-catch-attention&quot;&gt;&lt;img src=&quot;https://i.imgur.com/GiNJqlm.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/@hivebuzz&quot;&gt;&lt;img src=&quot;https://i.imgur.com/B4UTun2.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/@hivesql&quot;&gt;&lt;img src=&quot;https://i.imgur.com/EPN8RW6.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/hive/@arcange/introducing-hive-account-recovery&quot;&gt;&lt;img src=&quot;https://i.imgur.com/6TWeW7V.png&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;/@hive.autoclaim&quot;&gt;&lt;img src=&quot;https://i.imgur.com/ih2pEOw.png&quot; /&gt;&lt;/a&gt;&lt;/center&gt;
&lt;center&gt;

### [Vote for me as a witness ![](https://i.imgur.com/2bi4SnT.png)](https://hivesigner.com/sign/account-witness-vote?witness=arcange&amp;amp;approve=1)&amp;lt;/div&amp;gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;
&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@arcange&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/msnet-libraries-transactions&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-arcange-msnet-libraries-transactions').html();
      const outputElem = $('#content-arcange-msnet-libraries-transactions');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;
&lt;style&gt;
  #content-arcange-msnet-libraries-transactions {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-arcange-msnet-libraries-transactions code {
    background: white;
  }
  #content-arcange-msnet-libraries-transactions a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-arcange-msnet-libraries-transactions a:hover {
    border-bottom: 0;
  }
  #content-arcange-msnet-libraries-transactions h1 {
    font-size: 2.2em;
  }
  #content-arcange-msnet-libraries-transactions h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-arcange-msnet-libraries-transactions header small {
    color: #999;
    font-size: 50%;
  }
  #content-arcange-msnet-libraries-transactions img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;
&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/hive-139531/@arcange/msnet-libraries-transactions&quot;&gt;Hive libraries for Microsoft .NET now fully support broadcasting transactions&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@arcange&quot;&gt;@arcange&lt;/a&gt;
&lt;/p&gt;

&lt;/center&gt;&lt;/div&gt;</content><author><name>arcange</name></author><category term="news" /><category term="dotnet" /><summary type="html">![](https://i.imgur.com/uBDjiIT.png) Months ago, I introduced the [Hive library for Microsoft .NET developers](/hive-139531/@arcange/msnet-libraries). This library provides developers used to VB.NET and C# languages with easy-to-use .NET classes, exposing almost all the APIs of the `hived` and `cli_wallet` daemons. It's also available as a &quot;COM library&quot; (aka .dll) so people can use it directly in their favorites COM-enabled Windows applications like Word, Excel, ... ### A missing feature Although the existing library was versatile enough to read information from the blockchain and even record transactions in it, it was however limited to operations that can be sent to the blockchain through the interfaces exposed by the cli_wallet. Although most of the usual operations are available through that interface, many new operations have been supported by the blockchain but not added to the cli_wallet API (yet). On the other hand, this way of sending transactions to the blockchain requires that you have a running daemon available at hand, and in case of failure you cannot easily switch to another. I had wanted for years to have the ability to connect my library directly to API servers that several Witnesses operate (including myself), but so far I have managed to postpone this work. Time to cut it out! ### Taking matters into my own hands Broadcasting transactions directly to the blockchain means serializing the transaction and signing it before sending it to an RPC node. Easier said than done because it is an area that was new to me and I know several who have broken their teeth there. Never mind, I like to take up challenges and I often tell myself that nothing is impossible when approached with calm and patience. Thanks to the Hive developers portal [tutorial about transactions](https://developers.hive.io/tutorials-recipes/how-to-serialize-and-sign-using-js.html), I gradually integrated the understanding of the serialization and cryptography processes required to sign transactions. As a result, it is no longer necessary to go through cli_wallet to write operations into the blockchain, and **all** available operations can now be broadcasted using the hive.net library. ### Code example The library is really easy to use. Here is a simple C# code example that broadcast a vote operation to the blockchain: ``` string strPosting = &quot;5...&quot;; // Posting private key string strActive = &quot;5...&quot;; // Active private key HttpClient oHTTP = new HttpClient(); CHived oHived = new CHived(oHTTP, &quot;https://api.hive.blog&quot;); COperations.vote oVote = new COperations.vote { voter = &quot;arcange&quot;, author = &quot;author&quot;, permlink = &quot;permlink&quot;, weight = 100 }; try { string txid = oHived.broadcast_transaction( new object[] { oVote }, new string[] { strPosting } ); Console.Write(txid); } catch (Exception e) { Console.Write(e.Message); } ``` Broadcasting multiple operations at once with a single transaction is also very easy: ``` COperations.transfer oTransfer1 = new COperations.transfer { from = &quot;arcange&quot;, to = &quot;account1&quot;, amount = new Asset(&quot;0.001 HIVE&quot;), memo = &quot;A first transfer&quot; }; COperations.transfer oTransfer2 = new COperations.transfer { from = &quot;arcange&quot;, to = &quot;account2&quot;, amount = new Asset(&quot;1.000 HBD&quot;), memo = &quot;A second transfer&quot; }; try { string txid = oHived.broadcast_transaction( new object[] { oTransfer1, oTransfer2 }, new string[] { strActive } ); Console.Write(txid); } catch (Exception e) { Console.Write(e.Message); } ``` Here is another example in VB.NET: ``` Dim strActive = &quot;5....&quot; ' Posting private key Dim strPosting = &quot;5....&quot; ' Active Private key Dim oHttp As HttpClient = New HttpClient() Dim oHiveAPI = New CHived(oHttp, &quot;https://api.hive.blog&quot;) Dim oWitnessUpdate As New COperations.witness_update With { .owner = &quot;arcange&quot;, .url = &quot;https://....&quot;, .block_signing_key = New PublicKey(&quot;STM....&quot;), .props = New ChainProperties With { .account_creation_fee = New Asset(3, &quot;HIVE&quot;), .maximum_block_size = 65536, .hbd_interest_rate = 1000 }, .fee = New Asset(3, &quot;HIVE&quot;) } Try Dim txid = oHiveAPI.broadcast_transaction({oWitnessUpdate}, {strPosting}) Console.Write(txid) Catch ex As Exception Console.Write(ex.Message) End Try ``` ### What's next? There is still some work to do: - add a set of functions to simplify the generation of the most used operations. - create documentation - create some tutorials I plan to tackle these tasks in the next weeks. ### Open source The libraries are open source and can be found on [GitLab](https://gitlab.syncad.com/hive/hive-net). If you have any comments or requests, please create an issue on GitLab too. You can also contact me on [Discord](https://discordapp.com/channels/@me/237480770933882881) or [Telegram](https://t.me/the_arcange) --- ### Check out my apps and services ### [Vote for me as a witness ![](https://i.imgur.com/2bi4SnT.png)](https://hivesigner.com/sign/account-witness-vote?witness=arcange&amp;amp;approve=1)&amp;lt;/div&amp;gt; See: Hive libraries for Microsoft .NET now fully support broadcasting transactions by @arcange</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://i.imgur.com/uBDjiIT.png" /><media:content medium="image" url="https://i.imgur.com/uBDjiIT.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">19th update of 2021 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2021/07/20/19th-update-of-2021-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="19th update of 2021 on BlockTrades work on Hive software" /><published>2021-07-20T12:56:42-07:00</published><updated>2021-07-20T12:56:42-07:00</updated><id>https://hivedocs.info/news/core/development/2021/07/20/19th-update-of-2021-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/07/20/19th-update-of-2021-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)

Below is a list of Hive-related programming issues worked on by BlockTrades team during last week or so:

# Hived work (blockchain node software)

### Cli-wallet signing enhancement and bug fix
We’ve enhanced the command-line interface wallet to allow it to sign transactions using account authority. We’re currently testing this. 

We fixed an erroneous error message from cli wallet that could occur when calling `list_my_accounts`: https://gitlab.syncad.com/hive/hive/-/issues/173

### Performance metrics for continuous integration system
We’re also adding performance metrics to our automated build-and-test (CI) system: https://gitlab.syncad.com/hive/tests_api/-/tree/request-execution-time

These changes are also being made for hivemind tests:
https://gitlab.syncad.com/hive/tests_api/-/tree/request-execution-time 

This work is still in progress.

### Fixed final issues associated with account history and last irreversible block
We also completed a few fixes related to account history and the last irreversible block and added some new tests:
https://gitlab.syncad.com/hive/hive/-/merge_requests/275

### Continuing work on blockhain converter tool
We’re also continuing work on the blockchain converter that generates a testnet blockchain configuration form an existing blocklog. Most recently, we added multithreading support to speed it up. You can follow the work on this task here: https://gitlab.syncad.com/hive/hive/-/commits/tm-blockchain-converter/

# Hivemind (2nd layer applications + social media middleware)

### Dramatically reduced memory consumption
We’ve made some progress on hivemind’s memory consumption issue. While it appears the leak issue is gone (probably fixed when we pinned versions of library dependencies used by hivemind), we still saw some higher-than-desirable memory usage during massive sync. We made several changes (queues used by consumer/provider were too long, used prepared queries where possible, explicitly cleared some python containers)  to reduce this number and were able to **reduce hivemind’s peak memory usage from over 13GB to just over 4GB**. Changes are here: https://gitlab.syncad.com/hive/hivemind/-/commits/mt-memory-leak/

Here’s a graph of memory usage before and after the above changes:

![Clipboard - July 19, 2021 2_38 PM.png](https://images.hive.blog/DQmcRro9hkeXG7eRivaE6Jp4hbg2BHbHXbW3fDMMaoxEg11/Clipboard%20-%20July%2019,%202021%202_38%20PM.png)


### Optimized update_post_rshares down from 11.4hrs to ~15m
We’re also continuing work on optimizing the `update_post_rshares` function that is executed after massive sync. Originally this function took 11.4 hours and we’ve reduced it to around 15 minutes by adding an index. Initially this index was pretty large (around 25GB), but we’ve also made other optimizations to reduce database writes wrelated to posts that paid out during massigive sync, and this has not only reduced IO usage, it also reduced the size required by this index. It’s also worth noting that this index can be dropped after update_post_rshares has completed. 

### Made hivemind shutdown time configurable when it loses contact with hived
Hivemind had an annoying habit of shutting down completely if it lost contact with the hived node that it was using to get blockchain data (it retried 25 times, then shutdown). This was problematic because this meant that temporary network disruptions could leave hivemind dead in the water. We’ve added a new option, --max-retries (or –max-allowed-retries, to be determined) which will default to a value of -1 (infinitely retry).

We’re also moving hivemind tests out of the tests-api repo to hivemind repo as part of some general restructuring of the test system.

# Hive Application Framework (HAF)

Our primary developer for the forkresolver code in HAF is back as of yesterday and has resumed work on this project. Our next step is to begin developing some sample applications for HAF. I hope to be able to officially release HAF in about a month. Once we have HAF as a foundation, we can begin building our 2nd layer smart contract system on top of it.

# What’s next?

For the rest of this week, we’ll be focused on testing associated with above tasks.  In the week thereafter, we’ll begin planning what tasks will be scheduled for hardfork 26 as well as other tasks that we plan to complete which can be released sooner (as they don’t require protocol changes).  Of such non-hardfork tasks include development of common-use HAF applications (e.g a HAF application to generate tables about Hive accounts and custom_json).&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/19th-update-of-2021-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-19th-update-of-2021-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/19th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;19th update of 2021 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below is a list of Hive-related programming issues worked on by BlockTrades team during last week or so: # Hived work (blockchain node software) ### Cli-wallet signing enhancement and bug fix We’ve enhanced the command-line interface wallet to allow it to sign transactions using account authority. We’re currently testing this. We fixed an erroneous error message from cli wallet that could occur when calling `list_my_accounts`: https://gitlab.syncad.com/hive/hive/-/issues/173 ### Performance metrics for continuous integration system We’re also adding performance metrics to our automated build-and-test (CI) system: https://gitlab.syncad.com/hive/tests_api/-/tree/request-execution-time These changes are also being made for hivemind tests: https://gitlab.syncad.com/hive/tests_api/-/tree/request-execution-time This work is still in progress. ### Fixed final issues associated with account history and last irreversible block We also completed a few fixes related to account history and the last irreversible block and added some new tests: https://gitlab.syncad.com/hive/hive/-/merge_requests/275 ### Continuing work on blockhain converter tool We’re also continuing work on the blockchain converter that generates a testnet blockchain configuration form an existing blocklog. Most recently, we added multithreading support to speed it up. You can follow the work on this task here: https://gitlab.syncad.com/hive/hive/-/commits/tm-blockchain-converter/ # Hivemind (2nd layer applications + social media middleware) ### Dramatically reduced memory consumption We’ve made some progress on hivemind’s memory consumption issue. While it appears the leak issue is gone (probably fixed when we pinned versions of library dependencies used by hivemind), we still saw some higher-than-desirable memory usage during massive sync. We made several changes (queues used by consumer/provider were too long, used prepared queries where possible, explicitly cleared some python containers) to reduce this number and were able to **reduce hivemind’s peak memory usage from over 13GB to just over 4GB**. Changes are here: https://gitlab.syncad.com/hive/hivemind/-/commits/mt-memory-leak/ Here’s a graph of memory usage before and after the above changes: ![Clipboard - July 19, 2021 2_38 PM.png](https://images.hive.blog/DQmcRro9hkeXG7eRivaE6Jp4hbg2BHbHXbW3fDMMaoxEg11/Clipboard%20-%20July%2019,%202021%202_38%20PM.png) ### Optimized update_post_rshares down from 11.4hrs to ~15m We’re also continuing work on optimizing the `update_post_rshares` function that is executed after massive sync. Originally this function took 11.4 hours and we’ve reduced it to around 15 minutes by adding an index. Initially this index was pretty large (around 25GB), but we’ve also made other optimizations to reduce database writes wrelated to posts that paid out during massigive sync, and this has not only reduced IO usage, it also reduced the size required by this index. It’s also worth noting that this index can be dropped after update_post_rshares has completed. ### Made hivemind shutdown time configurable when it loses contact with hived Hivemind had an annoying habit of shutting down completely if it lost contact with the hived node that it was using to get blockchain data (it retried 25 times, then shutdown). This was problematic because this meant that temporary network disruptions could leave hivemind dead in the water. We’ve added a new option, --max-retries (or –max-allowed-retries, to be determined) which will default to a value of -1 (infinitely retry). We’re also moving hivemind tests out of the tests-api repo to hivemind repo as part of some general restructuring of the test system. # Hive Application Framework (HAF) Our primary developer for the forkresolver code in HAF is back as of yesterday and has resumed work on this project. Our next step is to begin developing some sample applications for HAF. I hope to be able to officially release HAF in about a month. Once we have HAF as a foundation, we can begin building our 2nd layer smart contract system on top of it. # What’s next? For the rest of this week, we’ll be focused on testing associated with above tasks. In the week thereafter, we’ll begin planning what tasks will be scheduled for hardfork 26 as well as other tasks that we plan to complete which can be released sooner (as they don’t require protocol changes). Of such non-hardfork tasks include development of common-use HAF applications (e.g a HAF application to generate tables about Hive accounts and custom_json). See: 19th update of 2021 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Hive core developer meeting #26</title><link href="https://hivedocs.info/news/core/development/2021/07/19/hive-core-developer-meeting-26.html" rel="alternate" type="text/html" title="Hive core developer meeting #26" /><published>2021-07-19T13:21:12-07:00</published><updated>2021-07-19T13:21:12-07:00</updated><id>https://hivedocs.info/news/core/development/2021/07/19/hive-core-developer-meeting-26</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/07/19/hive-core-developer-meeting-26.html">&lt;div id=&quot;content-howo-hive-core-developer-meeting-26&quot;&gt;https://www.youtube.com/watch?v=UF-t2ra9kQI

If you're listening to the whole thing, please post timestamps as a comment, the first one to do so (and correctly) will get a 100% upvote from me :)


meeting points:

# Dev sync:

Listen to that one

# direct rc delegations:

Work is well underway on my end, I basically talk about what I said last week there: https://peakd.com/core/@howo/core-development-report-12-good-progress-on-direct-rc-delegations I fixed the api problem and now am building automated test cases. It's going to take a little while

# VOP for hive -&amp;gt; DHF conversion:

 https://gitlab.syncad.com/hive/hive/-/issues/174

I'll do it. it's a change with little to no impact on performance. It's an issue today because if you send hive to the dhf then there is nothing that's stored in the blockchchain to tell future users how much hbd we got out. And with the rise of hbdstablizer by @smooth, for a lot 

# VOP for proposals creation containing the ID hive

https://gitlab.syncad.com/hive/hive/-/issues/177

It's an issue because if you parse the blockchain you can't get the id of a proposal after it was created unless you actively call an api call, which doesn't scale. Same it's something with little impact on performance that I'll tackle.

# cli_wallet command history persistence (commands are saved when quitting and restarting the program)

Something that I will tackle sooner or later, it's mostly for power users where it's annoying to lose your progress once you close cli_wallet.

# plans for hf26 ? 

I figured we would throw some ideas around and talk about them, like dynamic rc costs, automated actions, NAI etc. There are a bunch of talks in there. Keep in mind nothing is final it's mostly thoughts we had on things we'd like to see for hf26. But hf26 is very far away so everything might change, or be rejected by the community.

I heavily encourage your to listen to that one, but basically there a few ideas thrown around improving hive as a core like peer to peer connectivity (block sync) and the forking logic, dynamic rcs costs (a small post should cost less rc than a big post) etc
As for some of the more &quot;controversial&quot;/ things that people notice changes:

Discussions about accelerated power down time for a fee (@theycallmedan talked about it in length on how we could implement this) 
less than 10 votes a day (maybe 5), basically allow 200% votes. 
Use NAI for assets


Then there was a bunch of questions from @arcange that I encourage you to listen to because we jump from subject to subject. 

Cheers, @howo&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@howo&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/hive-core-developer-meeting-26&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-howo-hive-core-developer-meeting-26').html();
      const outputElem = $('#content-howo-hive-core-developer-meeting-26');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-howo-hive-core-developer-meeting-26 {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-howo-hive-core-developer-meeting-26 code {
    background: white;
  }
  #content-howo-hive-core-developer-meeting-26 a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-howo-hive-core-developer-meeting-26 a:hover {
    border-bottom: 0;
  }
  #content-howo-hive-core-developer-meeting-26 h1 {
    font-size: 2.2em;
  }
  #content-howo-hive-core-developer-meeting-26 h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-howo-hive-core-developer-meeting-26 header small {
    color: #999;
    font-size: 50%;
  }
  #content-howo-hive-core-developer-meeting-26 img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/core/@howo/hive-core-developer-meeting-26&quot;&gt;Hive core developer meeting #26&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@howo&quot;&gt;@howo&lt;/a&gt;
&lt;/p&gt;</content><author><name>howo</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">https://www.youtube.com/watch?v=UF-t2ra9kQI If you're listening to the whole thing, please post timestamps as a comment, the first one to do so (and correctly) will get a 100% upvote from me :) meeting points: # Dev sync: Listen to that one # direct rc delegations: Work is well underway on my end, I basically talk about what I said last week there: https://peakd.com/core/@howo/core-development-report-12-good-progress-on-direct-rc-delegations I fixed the api problem and now am building automated test cases. It's going to take a little while # VOP for hive -&amp;gt; DHF conversion: https://gitlab.syncad.com/hive/hive/-/issues/174 I'll do it. it's a change with little to no impact on performance. It's an issue today because if you send hive to the dhf then there is nothing that's stored in the blockchchain to tell future users how much hbd we got out. And with the rise of hbdstablizer by @smooth, for a lot # VOP for proposals creation containing the ID hive https://gitlab.syncad.com/hive/hive/-/issues/177 It's an issue because if you parse the blockchain you can't get the id of a proposal after it was created unless you actively call an api call, which doesn't scale. Same it's something with little impact on performance that I'll tackle. # cli_wallet command history persistence (commands are saved when quitting and restarting the program) Something that I will tackle sooner or later, it's mostly for power users where it's annoying to lose your progress once you close cli_wallet. # plans for hf26 ? I figured we would throw some ideas around and talk about them, like dynamic rc costs, automated actions, NAI etc. There are a bunch of talks in there. Keep in mind nothing is final it's mostly thoughts we had on things we'd like to see for hf26. But hf26 is very far away so everything might change, or be rejected by the community. I heavily encourage your to listen to that one, but basically there a few ideas thrown around improving hive as a core like peer to peer connectivity (block sync) and the forking logic, dynamic rcs costs (a small post should cost less rc than a big post) etc As for some of the more &quot;controversial&quot;/ things that people notice changes: Discussions about accelerated power down time for a fee (@theycallmedan talked about it in length on how we could implement this) less than 10 votes a day (maybe 5), basically allow 200% votes. Use NAI for assets Then there was a bunch of questions from @arcange that I encourage you to listen to because we jump from subject to subject. Cheers, @howo See: Hive core developer meeting #26 by @howo</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hivedocs.info/assets/images/favicon.png" /><media:content medium="image" url="https://hivedocs.info/assets/images/favicon.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Hive Pressure #3: Catching up with the head block.</title><link href="https://hivedocs.info/howto/devops/node/2021/07/14/hive-pressure-3-catching-up-with-the-head-block.html" rel="alternate" type="text/html" title="Hive Pressure #3: Catching up with the head block." /><published>2021-07-14T13:15:09-07:00</published><updated>2021-07-14T13:15:09-07:00</updated><id>https://hivedocs.info/howto/devops/node/2021/07/14/hive-pressure-3-catching-up-with-the-head-block</id><content type="html" xml:base="https://hivedocs.info/howto/devops/node/2021/07/14/hive-pressure-3-catching-up-with-the-head-block.html">&lt;div id=&quot;content-gtg-hive-pressure-3-catching-up-with-the-head-block&quot;&gt;Basic Hive node has a very simple configuration and with minor changes it can serve as a seed node, a witness node, a broadcaster node, a private node for your wallet (that’s what exchanges are using) or even a simple API node for your Hive microservices.

Regardless of its role, as long as a node has unrestricted network access, it will be part of the Hive p2p network, thus supporting Hive reliability and resilience.

Before your node becomes fully functional, it has to reach the head block of the blockchain.

# Get the Hive daemon
- Build it yourself from these sources:
https://github.com/openhive-network/hive (push mirror)
https://gitlab.syncad.com/hive/hive
- Use docker:
`docker pull hiveio/hive`
- Or get the binary from a trusted source:
https://gtg.openhive.network/get/bin/

# Get the blocks
The easy way or the fast way.

- Sync from the p2p network
By default, when a fresh Hive node starts, it connects to the Hive p2p network and retrieves blocks from it.
See: `--resync-blockchain`

- Get blocks yourself
Hive node can use an existing `block_log` either from another instance or from a public source such as https://gtg.openhive.network/get/blockchain
Our goal is to reach the head block as soon as possible so we chose that way.
`block_log` currently takes over 350GB, so depending on your connection and source, downloading it might take less than an hour or even half a day (for 1Gbps and 100Mbps respectively).
By default it’s expected to be located at `~/.hived/blockchain/block_log`.

# Configure your node
Configuration settings are by default in `~/.hived/config.ini`
This should be enough:
```
plugin = witness
plugin = rc

shared-file-dir = &quot;/run/hive&quot;
shared-file-size = 24G

flush-state-interval = 0
```
Please note that I’m using a custom location for `shared_memory.bin` file, keeping it on a `tmpfs` volume for maximum performance, make sure you have enough space there if you are going to use it.

# Process the blocks
Having all the blocks is not enough, your node needs to be aware of the current state of Hive.
Live nodes get blocks from the p2p network and process them updating state one block at a time (every three seconds), but when you start from scratch, you have to catch up.

- Snapshot
Snapshot is the fastest way because most of the job is already done.
That however will work only for compatible configurations.
We will play with snapshots another time.

- Replay
Once you have a `block_log` and `config.ini` files in place, you need to start `hived` with `--replay-blockchain`.
Replay uses the existing `block_log` to build up the shared memory file up to the highest block stored there, and then it continues with sync, up to the head block.
There's very little use of multi-threading here because every block depends on the previous one.
A lot of data is being processed, so your hardware specs really do matter here.
Not long ago Hive crossed the 55 millions block mark.
Let’s see how long does it take to replay that many blocks using different hardware specs.

`hived --force-replay --set-benchmark-interval 100000`
# Test Setups
### Alpha
&amp;gt; A popular workstation setup. Good enough but will run out of storage soon.
```
Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz
64GB RAM (DDR4 4x16GB 2133MHz)
2x256GB SSD in RAID0 (SAMSUNG MZ7LN256HMJP)
```
### Bravo
&amp;gt; Old but not obsolete. CPU released in 2014. New disks after the old ones failed.
```
Intel(R) Core(TM) i7-4790K CPU @ 4.00GHz
32GB RAM (DDR3 4x8GB 1600MHz)
2x480GB SSD in RAID0 (KINGSTON SEDC500M480G)
```
### Charlie
&amp;gt; The newest and the most expensive CPU in my list. Also the only AMD.
```
AMD Ryzen 5 3600
64GB RAM (DDR4 4x16GB 2666MHz)
2x512GB NVMe in RAID0 (SAMSUNG MZVLB512HBJQ)
```
### Delta
&amp;gt; My favorite, high quality components for serious tasks.
```
Intel(R) Xeon(R) E-2136 CPU @ 3.30GHz
64GB RAM (DDR4 4x16GB 2666MHz ECC)
2x512GB NVMe in RAID0 (WD CL SN720)
```
# Warning: spoilers ahead
What do you think? Which one will win the race?

&lt;center&gt;https://www.youtube.com/watch?v=vlW9lDE3DuI&lt;/center&gt;

# Results

|Server  | [s] |  H:M:S |
|--------|-----|--------|
|Alpha   |28120|7h48m40s|
|Bravo   |26280|7h18m00s|
|Charlie |25032|6h57m12s|
|Delta   |23314|6h28m34s|

What are your `--replay` times?&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@gtg&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/hive-pressure-3-catching-up-with-the-head-block&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-gtg-hive-pressure-3-catching-up-with-the-head-block').html();
      const outputElem = $('#content-gtg-hive-pressure-3-catching-up-with-the-head-block');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-gtg-hive-pressure-3-catching-up-with-the-head-block {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-gtg-hive-pressure-3-catching-up-with-the-head-block code {
    background: white;
  }
  #content-gtg-hive-pressure-3-catching-up-with-the-head-block a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-gtg-hive-pressure-3-catching-up-with-the-head-block a:hover {
    border-bottom: 0;
  }
  #content-gtg-hive-pressure-3-catching-up-with-the-head-block h1 {
    font-size: 2.2em;
  }
  #content-gtg-hive-pressure-3-catching-up-with-the-head-block h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-gtg-hive-pressure-3-catching-up-with-the-head-block header small {
    color: #999;
    font-size: 50%;
  }
  #content-gtg-hive-pressure-3-catching-up-with-the-head-block img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-160391/@gtg/hive-pressure-3-catching-up-with-the-head-block&quot;&gt;Hive Pressure #3: Catching up with the head block.&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@gtg&quot;&gt;@gtg&lt;/a&gt;
&lt;/p&gt;</content><author><name>gtg</name></author><category term="howto" /><category term="devops" /><category term="node" /><summary type="html">Basic Hive node has a very simple configuration and with minor changes it can serve as a seed node, a witness node, a broadcaster node, a private node for your wallet (that’s what exchanges are using) or even a simple API node for your Hive microservices. Regardless of its role, as long as a node has unrestricted network access, it will be part of the Hive p2p network, thus supporting Hive reliability and resilience. Before your node becomes fully functional, it has to reach the head block of the blockchain. # Get the Hive daemon - Build it yourself from these sources: https://github.com/openhive-network/hive (push mirror) https://gitlab.syncad.com/hive/hive - Use docker: `docker pull hiveio/hive` - Or get the binary from a trusted source: https://gtg.openhive.network/get/bin/ # Get the blocks The easy way or the fast way. - Sync from the p2p network By default, when a fresh Hive node starts, it connects to the Hive p2p network and retrieves blocks from it. See: `--resync-blockchain` - Get blocks yourself Hive node can use an existing `block_log` either from another instance or from a public source such as https://gtg.openhive.network/get/blockchain Our goal is to reach the head block as soon as possible so we chose that way. `block_log` currently takes over 350GB, so depending on your connection and source, downloading it might take less than an hour or even half a day (for 1Gbps and 100Mbps respectively). By default it’s expected to be located at `~/.hived/blockchain/block_log`. # Configure your node Configuration settings are by default in `~/.hived/config.ini` This should be enough: ``` plugin = witness plugin = rc shared-file-dir = &quot;/run/hive&quot; shared-file-size = 24G flush-state-interval = 0 ``` Please note that I’m using a custom location for `shared_memory.bin` file, keeping it on a `tmpfs` volume for maximum performance, make sure you have enough space there if you are going to use it. # Process the blocks Having all the blocks is not enough, your node needs to be aware of the current state of Hive. Live nodes get blocks from the p2p network and process them updating state one block at a time (every three seconds), but when you start from scratch, you have to catch up. - Snapshot Snapshot is the fastest way because most of the job is already done. That however will work only for compatible configurations. We will play with snapshots another time. - Replay Once you have a `block_log` and `config.ini` files in place, you need to start `hived` with `--replay-blockchain`. Replay uses the existing `block_log` to build up the shared memory file up to the highest block stored there, and then it continues with sync, up to the head block. There's very little use of multi-threading here because every block depends on the previous one. A lot of data is being processed, so your hardware specs really do matter here. Not long ago Hive crossed the 55 millions block mark. Let’s see how long does it take to replay that many blocks using different hardware specs. `hived --force-replay --set-benchmark-interval 100000` # Test Setups ### Alpha &amp;gt; A popular workstation setup. Good enough but will run out of storage soon. ``` Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz 64GB RAM (DDR4 4x16GB 2133MHz) 2x256GB SSD in RAID0 (SAMSUNG MZ7LN256HMJP) ``` ### Bravo &amp;gt; Old but not obsolete. CPU released in 2014. New disks after the old ones failed. ``` Intel(R) Core(TM) i7-4790K CPU @ 4.00GHz 32GB RAM (DDR3 4x8GB 1600MHz) 2x480GB SSD in RAID0 (KINGSTON SEDC500M480G) ``` ### Charlie &amp;gt; The newest and the most expensive CPU in my list. Also the only AMD. ``` AMD Ryzen 5 3600 64GB RAM (DDR4 4x16GB 2666MHz) 2x512GB NVMe in RAID0 (SAMSUNG MZVLB512HBJQ) ``` ### Delta &amp;gt; My favorite, high quality components for serious tasks. ``` Intel(R) Xeon(R) E-2136 CPU @ 3.30GHz 64GB RAM (DDR4 4x16GB 2666MHz ECC) 2x512GB NVMe in RAID0 (WD CL SN720) ``` # Warning: spoilers ahead What do you think? Which one will win the race? https://www.youtube.com/watch?v=vlW9lDE3DuI # Results |Server | [s] | H:M:S | |--------|-----|--------| |Alpha |28120|7h48m40s| |Bravo |26280|7h18m00s| |Charlie |25032|6h57m12s| |Delta |23314|6h28m34s| What are your `--replay` times? See: Hive Pressure #3: Catching up with the head block. by @gtg</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://img.youtube.com/vi/vlW9lDE3DuI/0.jpg" /><media:content medium="image" url="https://img.youtube.com/vi/vlW9lDE3DuI/0.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Core development report #12: Good progress on direct rc delegations</title><link href="https://hivedocs.info/news/core/development/2021/07/12/core-development-report-12-good-progress-on-direct-rc-delegations.html" rel="alternate" type="text/html" title="Core development report #12: Good progress on direct rc delegations" /><published>2021-07-12T13:30:15-07:00</published><updated>2021-07-12T13:30:15-07:00</updated><id>https://hivedocs.info/news/core/development/2021/07/12/core-development-report-12-good-progress-on-direct-rc-delegations</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/07/12/core-development-report-12-good-progress-on-direct-rc-delegations.html">&lt;div id=&quot;content-howo-core-development-report-12-good-progress-on-direct-rc-delegations&quot;&gt;
![image.png](https://files.peakd.com/file/peakd-hive/howo/AJeaeqp3EAX2QSK997FwxcZdJuACo817654t7riwiGf3aSNSLCLRwP5yGvpwrcb.png)

Hey,

I usually do my reports via voice during the hive developers meeting, but due to some vacations here and there and rush with hard fork 25, we decided to postpone it a few times so I figured I'd make an update about my progress via text.

Nowadays I am working on direct rc delegations, if you didn't follow, the initial implementation was judged too complex and there was a fear that this complexity could hide bug in the edge cases. Plus it makes it less user friendly. So I made a new specification here https://gitlab.syncad.com/hive/hive/-/issues/152.

And now I'm implementing it, work is in progress on that branch https://gitlab.syncad.com/hive/hive/-/tree/feature/direct_rc_delegations

The core core is done, you can delegate rc, update an existing delegation, delete a delegation and rc delegations are taken into account when calculating rc. I have also added some api endpoint so that when you call find_rc_account and list_rc_account the amount of RC received/delegated is reflected there.

I also added an endpoint to fetch direct rc delegations but I'm having some issues with the indexes (it induces a uniqueness constraint in the blockchain which is a problem because then if you want to delegate to one person and then another person you end up with a conflict because &quot;from&quot; is unique).

I've also included everything above in the cli_wallet for easy use. I'll update hive-js later on too so that most dapps will be able to play with it.

Apart from that I need to handle the edge cases where you lose RC (you delegated all your rc but then you power downs so you can no longer sustain those delegations) and write a mountain of automated tests.

Once that's done, I'll go through a bunch of reviews with @blocktrades' team (which I'm assuming won't happen until a few weeks as some of the core team is in vacations). Then we'll launch a testnet (or update the ongoing testnet, that part is unclear) so that some of the power users/ dapp users can test it out.

@howo&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@howo&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/core-development-report-12-good-progress-on-direct-rc-delegations&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-howo-core-development-report-12-good-progress-on-direct-rc-delegations').html();
      const outputElem = $('#content-howo-core-development-report-12-good-progress-on-direct-rc-delegations');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-howo-core-development-report-12-good-progress-on-direct-rc-delegations {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-howo-core-development-report-12-good-progress-on-direct-rc-delegations code {
    background: white;
  }
  #content-howo-core-development-report-12-good-progress-on-direct-rc-delegations a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-howo-core-development-report-12-good-progress-on-direct-rc-delegations a:hover {
    border-bottom: 0;
  }
  #content-howo-core-development-report-12-good-progress-on-direct-rc-delegations h1 {
    font-size: 2.2em;
  }
  #content-howo-core-development-report-12-good-progress-on-direct-rc-delegations h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-howo-core-development-report-12-good-progress-on-direct-rc-delegations header small {
    color: #999;
    font-size: 50%;
  }
  #content-howo-core-development-report-12-good-progress-on-direct-rc-delegations img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/core/@howo/core-development-report-12-good-progress-on-direct-rc-delegations&quot;&gt;Core development report #12: Good progress on direct rc delegations&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@howo&quot;&gt;@howo&lt;/a&gt;
&lt;/p&gt;</content><author><name>howo</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![image.png](https://files.peakd.com/file/peakd-hive/howo/AJeaeqp3EAX2QSK997FwxcZdJuACo817654t7riwiGf3aSNSLCLRwP5yGvpwrcb.png) Hey, I usually do my reports via voice during the hive developers meeting, but due to some vacations here and there and rush with hard fork 25, we decided to postpone it a few times so I figured I'd make an update about my progress via text. Nowadays I am working on direct rc delegations, if you didn't follow, the initial implementation was judged too complex and there was a fear that this complexity could hide bug in the edge cases. Plus it makes it less user friendly. So I made a new specification here https://gitlab.syncad.com/hive/hive/-/issues/152. And now I'm implementing it, work is in progress on that branch https://gitlab.syncad.com/hive/hive/-/tree/feature/direct_rc_delegations The core core is done, you can delegate rc, update an existing delegation, delete a delegation and rc delegations are taken into account when calculating rc. I have also added some api endpoint so that when you call find_rc_account and list_rc_account the amount of RC received/delegated is reflected there. I also added an endpoint to fetch direct rc delegations but I'm having some issues with the indexes (it induces a uniqueness constraint in the blockchain which is a problem because then if you want to delegate to one person and then another person you end up with a conflict because &quot;from&quot; is unique). I've also included everything above in the cli_wallet for easy use. I'll update hive-js later on too so that most dapps will be able to play with it. Apart from that I need to handle the edge cases where you lose RC (you delegated all your rc but then you power downs so you can no longer sustain those delegations) and write a mountain of automated tests. Once that's done, I'll go through a bunch of reviews with @blocktrades' team (which I'm assuming won't happen until a few weeks as some of the core team is in vacations). Then we'll launch a testnet (or update the ongoing testnet, that part is unclear) so that some of the power users/ dapp users can test it out. @howo See: Core development report #12: Good progress on direct rc delegations by @howo</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://files.peakd.com/file/peakd-hive/howo/AJeaeqp3EAX2QSK997FwxcZdJuACo817654t7riwiGf3aSNSLCLRwP5yGvpwrcb.png" /><media:content medium="image" url="https://files.peakd.com/file/peakd-hive/howo/AJeaeqp3EAX2QSK997FwxcZdJuACo817654t7riwiGf3aSNSLCLRwP5yGvpwrcb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">18th update of 2021 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2021/07/09/18th-update-of-2021-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="18th update of 2021 on BlockTrades work on Hive software" /><published>2021-07-09T14:26:30-07:00</published><updated>2021-07-09T14:26:30-07:00</updated><id>https://hivedocs.info/news/core/development/2021/07/09/18th-update-of-2021-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/07/09/18th-update-of-2021-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)

Below is a list of Hive-related programming issues worked on by BlockTrades team since my last report. 

# Hive network upgraded to Equilibrium
The most significant accomplishment was successful upgrade of Hive via hardfork 25 (the Equilibrium release), and a lot of time was spent monitoring the upgrade and supporting Hive apps as needed during the transition.

The Equilibrium upgrade did have one unanticipated side-effect: the change in curation rules resulted in votes cast after the hardfork being much stronger than votes cast before the hardfork with respect to curation weight, which meant that votes cast in the days before the hardfork generally didn’t receive much in the way of curation rewards. This was a temporary effect that has now been resolved since all posts being actively voted on now were created after the hardfork, but hopefully we’ll have enough traffic on our next iteration of the testnet that we’ll be able to detect such issues ahead of time.

# Hived work (blockchain node software)

Improvements to Testtools and tests used to verify hived functionality: https://gitlab.syncad.com/hive/hive/-/merge_requests/272

Added new –exit-before-sync flag to hived’s command-line interface (useful for dumping a snapshot without then syncing, see https://gitlab.syncad.com/hive/hive/-/issues/66 for more details on why this option was added):
https://gitlab.syncad.com/hive/hive/-/merge_requests/232
https://gitlab.syncad.com/hive/hive/-/merge_requests/273

We fixed the previously reported bug that requires a hived to be restarted after loading a snapshot:
https://gitlab.syncad.com/hive/hive/-/merge_requests/274

We have been analyzing the performance of our new “blockchain converter” tool for creating testnets quickly that mirror the mainnet and we’ve identified some code associated with nonces as the potential bottleneck.

# Hivemind (social media middleware)

We’ve added a new programmer to our hivemind team and he’ll initially be working on testing and minor bug fixes as a means of learning the code base. His first merge request is here:
https://gitlab.syncad.com/hive/hivemind/-/merge_requests/518

We added code for checking consistency of the hive_blocks table (this is part of previously mentioned plan to ensure robust operation in the case where hivemind’s postgres process shuts down suddenly or a rollback fails):
https://gitlab.syncad.com/hive/hivemind/-/merge_requests/516


We’re continuing work on improving performance of the update_rshares function immediately after massive sync of a hivemind instance. We’re trying two different alternatives to improve overall performance: 1) changing massive sync so that it updates rshares for paid posts on-the-fly, reducing the work of update_rshares to only updating rshares for unpaid posts (this approach requires introducing a new hived virtual_operation) and 2) adding an index (at least temporarily just after massive sync) to speed up update_rshares. Both approaches are currently undergoing testing.

We also resumed research into the reason why some hivemind nodes consume more memory than others. It has been suggested that it may be related to differences in python or python-library installations on the different systems, which I’m inclined to believe at this point, as we’re no longer seeing unexpected memory consumption on any of our production nodes running the latest official hivemind version. So if we’re unable to replicate this issue in our forthcoming tests, we’ll likely drop this issue soon, after merging in our diagnostic changes that identify sources of memory usage better.

# Hive Application Framework (HAF)

Our primary dev for this work is currently on vacation. 

I had hoped we would still be able to work on the psql_serializer plugin (which feeds data from hived to hivemind under the HAF system) in the meantime, but the dev tasked with that was tied up with other issues (e.g. fix of snapshot problem). A new dev has been assigned to work on psql_serializer this week (the previously tasked one is going on vacation for two weeks).

# Condenser and wallet (open-source code base for https://hive.blog and other similar frontends)

We reviewed and merged in a number of community-contributed upgrades to condenser and its wallet.

From @quochuy: https://gitlab.syncad.com/hive/wallet/-/merge_requests/102

From @guiltyparties: 
https://gitlab.syncad.com/hive/wallet/-/merge_requests/101
https://gitlab.syncad.com/hive/condenser/-/merge_requests/268

From @eonwarped: https://gitlab.syncad.com/hive/condenser/-/merge_requests/269


# What’s next?

Several of our Hive devs are either on vacation now or going on vacation this coming week (they had been delaying their vacations to be available for the hardfork and any potential problems that might arise afterwards). So we’ll only have 8 BlockTrades devs working on Hive for the next two weeks, and our progress will inevitably slow some during this time. 

After all our Hive devs return from vacation, we’ll take a couple of weeks to begin planning what work to schedule for the next hardfork (HF26). I have some preliminary ideas for improvements that our team will work on, but we’ll make a full list of proposed changes, then begin to prioritize what we want to fit on the roadmap for HF26. My plan at this time is to stick to our existing “upgrade Hive protocol every six months” schedule if possible.

Also, as during previous hardforks, our roadmaps aren’t fixed in stone, so we may consider making other proposed changes even after the initial roadmap is published, assuming the changes aren’t too big.

Note that the above process doesn’t mean we don’t have clear development goals prior to the completion of the HF26 roadmap. For one thing, we will be making performance upgrades to hived that don’t require an actual hardfork, and these changes will generally be released as they are completed.

Even more importantly, at this point our highest priority tasks revolve around the creation of the HAF framework and HAF-based applications, and this is all layer 2 work that doesn’t require any Hive protocol changes that would necessitate a hardfork. In other words, we can also release HAF and associated apps to the dev community as soon as they are ready, without the labor and scheduling issues involved in getting nodes to upgrade as part of a hardfork.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/18th-update-of-2021-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-18th-update-of-2021-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/18th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;18th update of 2021 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below is a list of Hive-related programming issues worked on by BlockTrades team since my last report. # Hive network upgraded to Equilibrium The most significant accomplishment was successful upgrade of Hive via hardfork 25 (the Equilibrium release), and a lot of time was spent monitoring the upgrade and supporting Hive apps as needed during the transition. The Equilibrium upgrade did have one unanticipated side-effect: the change in curation rules resulted in votes cast after the hardfork being much stronger than votes cast before the hardfork with respect to curation weight, which meant that votes cast in the days before the hardfork generally didn’t receive much in the way of curation rewards. This was a temporary effect that has now been resolved since all posts being actively voted on now were created after the hardfork, but hopefully we’ll have enough traffic on our next iteration of the testnet that we’ll be able to detect such issues ahead of time. # Hived work (blockchain node software) Improvements to Testtools and tests used to verify hived functionality: https://gitlab.syncad.com/hive/hive/-/merge_requests/272 Added new –exit-before-sync flag to hived’s command-line interface (useful for dumping a snapshot without then syncing, see https://gitlab.syncad.com/hive/hive/-/issues/66 for more details on why this option was added): https://gitlab.syncad.com/hive/hive/-/merge_requests/232 https://gitlab.syncad.com/hive/hive/-/merge_requests/273 We fixed the previously reported bug that requires a hived to be restarted after loading a snapshot: https://gitlab.syncad.com/hive/hive/-/merge_requests/274 We have been analyzing the performance of our new “blockchain converter” tool for creating testnets quickly that mirror the mainnet and we’ve identified some code associated with nonces as the potential bottleneck. # Hivemind (social media middleware) We’ve added a new programmer to our hivemind team and he’ll initially be working on testing and minor bug fixes as a means of learning the code base. His first merge request is here: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/518 We added code for checking consistency of the hive_blocks table (this is part of previously mentioned plan to ensure robust operation in the case where hivemind’s postgres process shuts down suddenly or a rollback fails): https://gitlab.syncad.com/hive/hivemind/-/merge_requests/516 We’re continuing work on improving performance of the update_rshares function immediately after massive sync of a hivemind instance. We’re trying two different alternatives to improve overall performance: 1) changing massive sync so that it updates rshares for paid posts on-the-fly, reducing the work of update_rshares to only updating rshares for unpaid posts (this approach requires introducing a new hived virtual_operation) and 2) adding an index (at least temporarily just after massive sync) to speed up update_rshares. Both approaches are currently undergoing testing. We also resumed research into the reason why some hivemind nodes consume more memory than others. It has been suggested that it may be related to differences in python or python-library installations on the different systems, which I’m inclined to believe at this point, as we’re no longer seeing unexpected memory consumption on any of our production nodes running the latest official hivemind version. So if we’re unable to replicate this issue in our forthcoming tests, we’ll likely drop this issue soon, after merging in our diagnostic changes that identify sources of memory usage better. # Hive Application Framework (HAF) Our primary dev for this work is currently on vacation. I had hoped we would still be able to work on the psql_serializer plugin (which feeds data from hived to hivemind under the HAF system) in the meantime, but the dev tasked with that was tied up with other issues (e.g. fix of snapshot problem). A new dev has been assigned to work on psql_serializer this week (the previously tasked one is going on vacation for two weeks). # Condenser and wallet (open-source code base for https://hive.blog and other similar frontends) We reviewed and merged in a number of community-contributed upgrades to condenser and its wallet. From @quochuy: https://gitlab.syncad.com/hive/wallet/-/merge_requests/102 From @guiltyparties: https://gitlab.syncad.com/hive/wallet/-/merge_requests/101 https://gitlab.syncad.com/hive/condenser/-/merge_requests/268 From @eonwarped: https://gitlab.syncad.com/hive/condenser/-/merge_requests/269 # What’s next? Several of our Hive devs are either on vacation now or going on vacation this coming week (they had been delaying their vacations to be available for the hardfork and any potential problems that might arise afterwards). So we’ll only have 8 BlockTrades devs working on Hive for the next two weeks, and our progress will inevitably slow some during this time. After all our Hive devs return from vacation, we’ll take a couple of weeks to begin planning what work to schedule for the next hardfork (HF26). I have some preliminary ideas for improvements that our team will work on, but we’ll make a full list of proposed changes, then begin to prioritize what we want to fit on the roadmap for HF26. My plan at this time is to stick to our existing “upgrade Hive protocol every six months” schedule if possible. Also, as during previous hardforks, our roadmaps aren’t fixed in stone, so we may consider making other proposed changes even after the initial roadmap is published, assuming the changes aren’t too big. Note that the above process doesn’t mean we don’t have clear development goals prior to the completion of the HF26 roadmap. For one thing, we will be making performance upgrades to hived that don’t require an actual hardfork, and these changes will generally be released as they are completed. Even more importantly, at this point our highest priority tasks revolve around the creation of the HAF framework and HAF-based applications, and this is all layer 2 work that doesn’t require any Hive protocol changes that would necessitate a hardfork. In other words, we can also release HAF and associated apps to the dev community as soon as they are ready, without the labor and scheduling issues involved in getting nodes to upgrade as part of a hardfork. See: 18th update of 2021 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Ledger App updated with HF25 operations</title><link href="https://hivedocs.info/news/nano/nodejs/wallet/2021/07/06/ledger-app-updated-with-hf25-operations.html" rel="alternate" type="text/html" title="Ledger App updated with HF25 operations" /><published>2021-07-06T12:38:18-07:00</published><updated>2021-07-06T12:38:18-07:00</updated><id>https://hivedocs.info/news/nano/nodejs/wallet/2021/07/06/ledger-app-updated-with-hf25-operations</id><content type="html" xml:base="https://hivedocs.info/news/nano/nodejs/wallet/2021/07/06/ledger-app-updated-with-hf25-operations.html">&lt;div id=&quot;content-engrave-ledger-app-updated-with-hf25-operations&quot;&gt;
![image.png](https://files.peakd.com/file/peakd-hive/engrave/Eo23B9HfFGSxgkzJckVtC8WSMeoEwfMcitDDJBZYJzaiVGqirgAwMZmEHpr2WcufBP6.png)
&lt;sup&gt;Image credit (@thepeakstudio)&lt;/sup&gt;

I'm slowly getting back on track to work on a Ledger Hive Application. My [previous post](https://peakd.com/hive-139531/@engrave/javascript-library-for-ledger-nano-s-hive-application) gathered a lot of traction so I feel committed to keeping you in touch.

# HF25

The latest hard fork (which happened a few days ago) introduced few new transactions, so I decided to modify both the app and the library to support it.

Javascript library is available on NPM with a newest version `2.1.0`:
 * `https://www.npmjs.com/package/@engrave/ledger-app-hive`


Because the original repository seems to be dead (my previous Merge Request hasn't been reviewed or merged), I decided to move it to the new home:

 * **Ledger Application**: `https://gitlab.com/engrave/ledger/hw-app-hive`
 * **Javascript Library**: `https://gitlab.com/engrave/ledger/ledger-app-hive`

Here is my first `recurrent_transfer` operation signed with my Ledger: `https://hiveblocks.com/tx/76293c907bf0038076f3507402a716c7d0b8ed57`


![image.png](https://files.peakd.com/file/peakd-hive/engrave/23z7Bh3LG2w23qb9SdLz1Vhmf2wRZKHztTw1H9RL58exJVe2XygcfAS4nqbPPUrTfKNqv.png)


# Code review

I'm slowly getting familiar with the codebase. What I can tell so far, is that it contains a lot of unused code which I'm removing. This seems like a copy-paste and I would really like to clean it before sending it to the official review. It also contains some code prepared for Ledger Nano X but I couldn't build it so far. This is probably some kind of a left-over from the boilerplate so there's a lot of work to be done yet.

I'm getting familiar with the ledger SDK as well to understand the code better.

# Development stack and SDK

SDK used to build the Hive app is already deprecated. There is also a new firmware version (2.0) that is not compatible with the previous one, hence we cannot run the app on new devices. I'm trying to port it to the new SDK version. The new SDK unifies the user flow for both Nano S and Nano X, so finally, we could run it on both devices.

I've even bought the Nano X to be able to test the app against this model.

Ledger prepared a docker image that contains all the necessary libraries to build the app. Unfortunately, the newest version does not work with the actual codebase so I need to make some changes to build it but at the end, this should make future development much easier.

# Ledger companion app

Simultaneously, I've started working on an electron-powered desktop app (as I promised in a previous post). I decided to go with Vue.js and for now, it contains basic CSS rules and views. Will push the code to the gitlab when it's a bit cleaner.


![image.png](https://files.peakd.com/file/peakd-hive/engrave/EowF9ezqJkPjHhLKMx9NjA82sqpuhKVddGJHDgbd5zyo9TsVhKtGc1714xjdsZxerab.png)

# What next

To get this app accepted by the Ledger, it needs to fulfill security, documentation, and functional requirements but I would also make it more user friendly, so the plan is as follow:
 * Make it compatible with a new SDK and 2.0 firmware version
 * Make it working on both Nano S and Nano X devices
 * Add Approve and Reject screens before signing the transaction
 * Prepare user guide and developer's documentation for Ledger Team
 * Develop a working version of the Hive companion app
 * Submit the app for the official review

***

Follow me to be up to date with incoming development updates!

***

&lt;center&gt;

**Click on the image to vote for @engrave witness:**

[![banner_engrave 100.png](https://images.hive.blog/DQmUghvic5TCPPvVkB4iB7eXmgQ3RQ4L8jkVizvuCBW8RMT/banner_engrave%20100.png)](https://hivesigner.com/sign/account-witness-vote?witness=engrave&amp;amp;approve=1)


&lt;/center&gt;
&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@engrave&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/ledger-app-updated-with-hf25-operations&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-engrave-ledger-app-updated-with-hf25-operations').html();
      const outputElem = $('#content-engrave-ledger-app-updated-with-hf25-operations');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-engrave-ledger-app-updated-with-hf25-operations {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-engrave-ledger-app-updated-with-hf25-operations code {
    background: white;
  }
  #content-engrave-ledger-app-updated-with-hf25-operations a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-engrave-ledger-app-updated-with-hf25-operations a:hover {
    border-bottom: 0;
  }
  #content-engrave-ledger-app-updated-with-hf25-operations h1 {
    font-size: 2.2em;
  }
  #content-engrave-ledger-app-updated-with-hf25-operations h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-engrave-ledger-app-updated-with-hf25-operations header small {
    color: #999;
    font-size: 50%;
  }
  #content-engrave-ledger-app-updated-with-hf25-operations img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/hive-139531/@engrave/ledger-app-updated-with-hf25-operations&quot;&gt;Ledger App updated with HF25 operations&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@engrave&quot;&gt;@engrave&lt;/a&gt;
&lt;/p&gt;</content><author><name>engrave</name></author><category term="news" /><category term="nano" /><category term="nodejs" /><category term="wallet" /><summary type="html">![image.png](https://files.peakd.com/file/peakd-hive/engrave/Eo23B9HfFGSxgkzJckVtC8WSMeoEwfMcitDDJBZYJzaiVGqirgAwMZmEHpr2WcufBP6.png) Image credit (@thepeakstudio) I'm slowly getting back on track to work on a Ledger Hive Application. My [previous post](https://peakd.com/hive-139531/@engrave/javascript-library-for-ledger-nano-s-hive-application) gathered a lot of traction so I feel committed to keeping you in touch. # HF25 The latest hard fork (which happened a few days ago) introduced few new transactions, so I decided to modify both the app and the library to support it. Javascript library is available on NPM with a newest version `2.1.0`: * `https://www.npmjs.com/package/@engrave/ledger-app-hive` Because the original repository seems to be dead (my previous Merge Request hasn't been reviewed or merged), I decided to move it to the new home: * **Ledger Application**: `https://gitlab.com/engrave/ledger/hw-app-hive` * **Javascript Library**: `https://gitlab.com/engrave/ledger/ledger-app-hive` Here is my first `recurrent_transfer` operation signed with my Ledger: `https://hiveblocks.com/tx/76293c907bf0038076f3507402a716c7d0b8ed57` ![image.png](https://files.peakd.com/file/peakd-hive/engrave/23z7Bh3LG2w23qb9SdLz1Vhmf2wRZKHztTw1H9RL58exJVe2XygcfAS4nqbPPUrTfKNqv.png) # Code review I'm slowly getting familiar with the codebase. What I can tell so far, is that it contains a lot of unused code which I'm removing. This seems like a copy-paste and I would really like to clean it before sending it to the official review. It also contains some code prepared for Ledger Nano X but I couldn't build it so far. This is probably some kind of a left-over from the boilerplate so there's a lot of work to be done yet. I'm getting familiar with the ledger SDK as well to understand the code better. # Development stack and SDK SDK used to build the Hive app is already deprecated. There is also a new firmware version (2.0) that is not compatible with the previous one, hence we cannot run the app on new devices. I'm trying to port it to the new SDK version. The new SDK unifies the user flow for both Nano S and Nano X, so finally, we could run it on both devices. I've even bought the Nano X to be able to test the app against this model. Ledger prepared a docker image that contains all the necessary libraries to build the app. Unfortunately, the newest version does not work with the actual codebase so I need to make some changes to build it but at the end, this should make future development much easier. # Ledger companion app Simultaneously, I've started working on an electron-powered desktop app (as I promised in a previous post). I decided to go with Vue.js and for now, it contains basic CSS rules and views. Will push the code to the gitlab when it's a bit cleaner. ![image.png](https://files.peakd.com/file/peakd-hive/engrave/EowF9ezqJkPjHhLKMx9NjA82sqpuhKVddGJHDgbd5zyo9TsVhKtGc1714xjdsZxerab.png) # What next To get this app accepted by the Ledger, it needs to fulfill security, documentation, and functional requirements but I would also make it more user friendly, so the plan is as follow: * Make it compatible with a new SDK and 2.0 firmware version * Make it working on both Nano S and Nano X devices * Add Approve and Reject screens before signing the transaction * Prepare user guide and developer's documentation for Ledger Team * Develop a working version of the Hive companion app * Submit the app for the official review *** Follow me to be up to date with incoming development updates! *** **Click on the image to vote for @engrave witness:** [![banner_engrave 100.png](https://images.hive.blog/DQmUghvic5TCPPvVkB4iB7eXmgQ3RQ4L8jkVizvuCBW8RMT/banner_engrave%20100.png)](https://hivesigner.com/sign/account-witness-vote?witness=engrave&amp;amp;approve=1) See: Ledger App updated with HF25 operations by @engrave</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://files.peakd.com/file/peakd-hive/engrave/Eo23B9HfFGSxgkzJckVtC8WSMeoEwfMcitDDJBZYJzaiVGqirgAwMZmEHpr2WcufBP6.png" /><media:content medium="image" url="https://files.peakd.com/file/peakd-hive/engrave/Eo23B9HfFGSxgkzJckVtC8WSMeoEwfMcitDDJBZYJzaiVGqirgAwMZmEHpr2WcufBP6.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">[Quickstart] How to upgrade Hive-in-a-box (HIAB / hive-docker) to HF25</title><link href="https://hivedocs.info/howto/witness/docker/node/2021/06/30/quickstart-how-to-upgrade-hive-in-a-box-hiab-hive-docker-to-hf25.html" rel="alternate" type="text/html" title="[Quickstart] How to upgrade Hive-in-a-box (HIAB / hive-docker) to HF25" /><published>2021-06-30T00:16:54-07:00</published><updated>2021-06-30T00:16:54-07:00</updated><id>https://hivedocs.info/howto/witness/docker/node/2021/06/30/quickstart-how-to-upgrade-hive-in-a-box-hiab-hive-docker-to-hf25</id><content type="html" xml:base="https://hivedocs.info/howto/witness/docker/node/2021/06/30/quickstart-how-to-upgrade-hive-in-a-box-hiab-hive-docker-to-hf25.html">&lt;div id=&quot;content-someguy123-quickstart-how-to-upgrade-hive-in-a-box-hiab-hive-docker-to-hf25&quot;&gt;
![Hive-in-a-box Logo](https://images.hive.blog/p/HuuaCwcKuiEjNgLb5Q7HbtUvB1HEHgck8hUkQMB4bFSJaxwFsYXGx7N9qtgEvLdGaoY?format=match&amp;amp;mode=fit)

I apologise for the late post, I've been overwhelmed trying to prepare @Privex for HF25, prepare my own servers, and provide one-on-one upgrade assistance to many close witness friends, and Privex customers.

This article covers how to upgrade from Hive HF24 to HF25 as quickly as possible for Privex Node-in-a-box VPS customers, as well as people who use normal Hive-in-a-box (`hive-docker`) on a Dedicated Server, or normal non-NIAB VPS.

Note that this post was written relatively quickly without fully testing the guide, nor checking for any typos / grammatical errors, to ensure any Hive witnesses who use HIAB and haven't yet upgraded - will have time to see this post and follow the instructions.

## HF25 is due at 14:00:00 (2:00 PM) UTC (15:00 / 3:00 PM United Kingdom BST Time) - or roughly 7-8 hrs from when this post was published. 

## [Click HERE to see the current UTC time, with realtime updates](https://time.is/UTC)


# Updating a Privex Node-in-a-box VPS

If you have a Privex Node-in-a-box (NIAB) VPS, e.g. a VHIVE8-SE or VHIVE16-FI which is still running HF24 - you need to [contact support for a reinstall](https://support.privex.io).

# Urgent Pre-installed HF25 Server

If you need a HF25 Hive Server urgently, and your existing servers aren't possible to repair in time for the HF, you can order one of our 8GB or 16GB Hive Node-in-a-box VPS's from @Privex - which are delivered within 20 minutes of payment (as long as you **leave the notes blank**, and **don't use a referral code unless you're 200% certain that it's a valid referral code**, since if it isn't, the order will be flagged for manual inspection to attempt to identify the intended referral).

- 🇸🇪 **SWEDEN** (SE)
    - [8GB Hive Node-in-a-box Sweden - VHIVE8-SE](https://pay.privex.io/order/package/vhive8-se)
    - [16GB Hive Node-in-a-box Sweden - VHIVE16-SE](https://pay.privex.io/order/package/vhive16-se)

- 🇫🇮 **FINLAND** (FI)
    - [8GB Hive Node-in-a-box Finland - VHIVE8-FI](https://pay.privex.io/order/package/vhive8-fi)
    - [16GB Hive Node-in-a-box Finland - VHIVE16-FI](https://pay.privex.io/order/package/vhive16-fi)


# Updating a normal Dedicated Server or non-Privex VPS from HF24 to HF25

If you're running Hive-in-a-box HF24 on either a Dedicated Server (whether Privex or not), or a normal VPS (one that is definitely not a Privex Node-in-a-box Pre-Installed Hive server), the upgrade to HF25 can be done within 30 mins to 1 hr, so long as your server has a high-speed network connection, has a recent `block_log` without any known corruption, and you're able to run the official `someguy123/hive` images which I release.

Before doing anything, enter the folder where you've installed `hive-docker`. Most people install it in their home directory.

```sh
cd ~/hive-docker
```

## Update Method 1 - Shared Memory Transplant - Fastest, but only works with my official binary docker images


First, you'll want to make sure that the `hive-docker` repository is up to date, so that you have the latest features and bug fixes required for the upgrade to go smoothly:

```sh
git fetch
git checkout master
git pull
```


Next, you'll want to install the HF25 Docker binary image. On most setups, a standard `./run.sh install` would work, but to avoid the risk of some people having configured a different default docker image in their `.env`, it's best to specify `hf25`:

```sh
./run.sh install hf25
```

Now, you'll need to stop your node - if it's actually running:

```sh
./run.sh stop
```

To help avoid corruption/contamination issues, and to potentially speed the downloads up by preventing `rsync` from having to calculate the differences between Privex's copy, and your copy of `shared_memory.bin` and `block_log.index` - it's best we remove them:

```sh
./run.sh clean shm

rm -fv data/witness_node_data_dir/blockchain/block_log.index
```

It's now time to truncate/update your block_log, replace your block_log.index, and obtain a fresh copy of shared_memory.bin for HF25:

```
DL_SERVER=&quot;se1.files.privex.io&quot; ./run.sh fix-blocks
```

![](https://images.hive.blog/p/7258xSVeJbKmECTaWChp4sJR8aZDJ7Y2HRvUHUnE6UnEtpj2Z6dqikt3ye5hs7B8Ef2bkjsjyQTre4JUb4x4j7Hxq281Uk2Fio53QLtzuovnfGtaRi2NEJC8GQ4MpvT915F6gKRVhZPwL?format=match&amp;amp;mode=fit)

- When asked whether you want to download the rest of the block_log, or truncate your block_log, press `y` and then hit enter.
- If it instead says that your `block_log` is the same size as the remote, and wants to do an &quot;integrity check&quot; or &quot;verify the block_log, press `n` and hit enter.
- When aksed if you want to replace/update/synchronise your `block_log.index` - press `y` and hit enter.
- When asked if you want to download/synchronise your snapshot files, press `n` and hit enter.
- When asked if you want to download/update/synchronise your `shared_memory.bin` - press `y` and hit enter.

Assuming everything went well, your server should now be ready to start.

Time to start your server:

```
./run.sh start
```

Now check the logs, and monitor it as it syncs. 

```
./run.sh logs
```

If it takes more than 5 minutes to start outputting &quot;Got X transactions from witness someguy123 on block 123456778&quot; - then you should run `./run.sh monitor` to see the sync progress.

Either way, it's a waiting game, it shouldn't take more than 20 mins to sync up (assuming you're following this guide within this week - this article was written on June 30th 2021), as `fix-blocks` would've gotten the recently synced chain files from [Privex's](https://www.privex.io/) file server.


+++ END OF METHOD 1 - Your node should now be up, or will be up very soon when it finishes syncing +++

----------------------

## Update Method 2 - Load a native snapshot - Slower than shared_memory.bin transplant - but this method will work with non-someguy123 built images

### ONLY FOLLOW THIS METHOD IF YOU COMPILE YOUR OWN DOCKER IMAGE, OR USE A THIRD-PARTY DOCKER IMAGE

If you use a custom image - whether one you built yourself using `./run.sh build`, `docker build`, - or one you downloaded from someone who isn't me - the shared memory transplant will likely be incompatible.

For this method, follow everything in method 1, but DO NOT follow the y/n suggestions for `fix-blocks`, instead, you'll want to answer them in the following way instead:

- When asked whether you want to download the rest of the block_log, or truncate your block_log, press `y` and then hit enter.
- If it instead says that your `block_log` is the same size as the remote, and wants to do an &quot;integrity check&quot; or &quot;verify the block_log, press `n` and hit enter.
- When aksed if you want to replace/update/synchronise your `block_log.index` - press `y` and hit enter.
- When asked if you want to download/synchronise your snapshot files, press `y` and hit enter.
- When asked if you want to download/update/synchronise your `shared_memory.bin` - press `n` and hit enter.

**ONLY FOLLOW THIS METHOD IF YOU COMPILE YOUR OWN DOCKER IMAGE, OR USE A THIRD-PARTY DOCKER IMAGE**

Now, use `./run.sh clean shm` to ensure you don't have a `shared_memory.bin`, since an existing shared_memory.bin file will cause problems.

```
./run.sh clean shm
```

You can now load the native snapshot using the command:

```
./run.sh loadsnap privexsnap
```

This will generate a `shared_memory.bin` using that snapshot, and once it's finished loading the snapshot, the node will automatically start normally, so for a witness node, that means it will be ready to start producing once it's finished loading the snapshot, and syncing up any blocks which it was behind.

All you have to do now, is monitor it (if you like), to track it's progress, and make sure nothing is going wrong:

```
./run.sh logs
```


+++ END OF METHOD 2 - Your node should now be up, or will be up very soon when it finishes syncing/loading the snapshot +++


---

Thanks for reading!
---

&lt;center&gt;

![](https://cdn.discordapp.com/attachments/379745956134125569/737091330143944806/SOMEX123.gif)

&lt;/center&gt;

GIF Avatar by @stellabelle

---

Do you like what I'm doing for Hive?
----


[Vote for me to be a Hive witness](https://peakd.com/witnesses) - every vote counts. 

----

Don't forget to follow me for more like this.
----

---

Have you ever thought about being a witness yourself? Join the [witness channel](https://openhive.chat/channel/witness). We're happy to guide you! Join in shaping the STEEM economy.

Are you looking for a new server provider? My company @privex offers highly-reliable and affordable dedicated and virtual servers for HIVE, HBD, EOS, LTC, BTC, and even DOGE! Check out our website at https://www.privex.io

You can join Privex's public Discord at https://discord.privex.io - we also have a Matrix server, sign up at https://riot.privex.io (or if you have an existing Matrix account, join our General chat at `#privex:privex.io`)

---
&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@someguy123&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/quickstart-how-to-upgrade-hive-in-a-box-hiab-hive-docker-to-hf25&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-someguy123-quickstart-how-to-upgrade-hive-in-a-box-hiab-hive-docker-to-hf25').html();
      const outputElem = $('#content-someguy123-quickstart-how-to-upgrade-hive-in-a-box-hiab-hive-docker-to-hf25');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-someguy123-quickstart-how-to-upgrade-hive-in-a-box-hiab-hive-docker-to-hf25 {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-someguy123-quickstart-how-to-upgrade-hive-in-a-box-hiab-hive-docker-to-hf25 code {
    background: white;
  }
  #content-someguy123-quickstart-how-to-upgrade-hive-in-a-box-hiab-hive-docker-to-hf25 a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-someguy123-quickstart-how-to-upgrade-hive-in-a-box-hiab-hive-docker-to-hf25 a:hover {
    border-bottom: 0;
  }
  #content-someguy123-quickstart-how-to-upgrade-hive-in-a-box-hiab-hive-docker-to-hf25 h1 {
    font-size: 2.2em;
  }
  #content-someguy123-quickstart-how-to-upgrade-hive-in-a-box-hiab-hive-docker-to-hf25 h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-someguy123-quickstart-how-to-upgrade-hive-in-a-box-hiab-hive-docker-to-hf25 header small {
    color: #999;
    font-size: 50%;
  }
  #content-someguy123-quickstart-how-to-upgrade-hive-in-a-box-hiab-hive-docker-to-hf25 img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://peakd.com/hive-139531/@someguy123/quickstart-how-to-upgrade-hive-in-a-box-hiab-hive-docker-to-hf25&quot;&gt;[Quickstart] How to upgrade Hive-in-a-box (HIAB / hive-docker) to HF25&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@someguy123&quot;&gt;@someguy123&lt;/a&gt;
&lt;/p&gt;</content><author><name>someguy123</name></author><category term="howto" /><category term="witness" /><category term="docker" /><category term="node" /><summary type="html">![Hive-in-a-box Logo](https://images.hive.blog/p/HuuaCwcKuiEjNgLb5Q7HbtUvB1HEHgck8hUkQMB4bFSJaxwFsYXGx7N9qtgEvLdGaoY?format=match&amp;amp;mode=fit) I apologise for the late post, I've been overwhelmed trying to prepare @Privex for HF25, prepare my own servers, and provide one-on-one upgrade assistance to many close witness friends, and Privex customers. This article covers how to upgrade from Hive HF24 to HF25 as quickly as possible for Privex Node-in-a-box VPS customers, as well as people who use normal Hive-in-a-box (`hive-docker`) on a Dedicated Server, or normal non-NIAB VPS. Note that this post was written relatively quickly without fully testing the guide, nor checking for any typos / grammatical errors, to ensure any Hive witnesses who use HIAB and haven't yet upgraded - will have time to see this post and follow the instructions. ## HF25 is due at 14:00:00 (2:00 PM) UTC (15:00 / 3:00 PM United Kingdom BST Time) - or roughly 7-8 hrs from when this post was published. ## [Click HERE to see the current UTC time, with realtime updates](https://time.is/UTC) # Updating a Privex Node-in-a-box VPS If you have a Privex Node-in-a-box (NIAB) VPS, e.g. a VHIVE8-SE or VHIVE16-FI which is still running HF24 - you need to [contact support for a reinstall](https://support.privex.io). # Urgent Pre-installed HF25 Server If you need a HF25 Hive Server urgently, and your existing servers aren't possible to repair in time for the HF, you can order one of our 8GB or 16GB Hive Node-in-a-box VPS's from @Privex - which are delivered within 20 minutes of payment (as long as you **leave the notes blank**, and **don't use a referral code unless you're 200% certain that it's a valid referral code**, since if it isn't, the order will be flagged for manual inspection to attempt to identify the intended referral). - 🇸🇪 **SWEDEN** (SE) - [8GB Hive Node-in-a-box Sweden - VHIVE8-SE](https://pay.privex.io/order/package/vhive8-se) - [16GB Hive Node-in-a-box Sweden - VHIVE16-SE](https://pay.privex.io/order/package/vhive16-se) - 🇫🇮 **FINLAND** (FI) - [8GB Hive Node-in-a-box Finland - VHIVE8-FI](https://pay.privex.io/order/package/vhive8-fi) - [16GB Hive Node-in-a-box Finland - VHIVE16-FI](https://pay.privex.io/order/package/vhive16-fi) # Updating a normal Dedicated Server or non-Privex VPS from HF24 to HF25 If you're running Hive-in-a-box HF24 on either a Dedicated Server (whether Privex or not), or a normal VPS (one that is definitely not a Privex Node-in-a-box Pre-Installed Hive server), the upgrade to HF25 can be done within 30 mins to 1 hr, so long as your server has a high-speed network connection, has a recent `block_log` without any known corruption, and you're able to run the official `someguy123/hive` images which I release. Before doing anything, enter the folder where you've installed `hive-docker`. Most people install it in their home directory. ```sh cd ~/hive-docker ``` ## Update Method 1 - Shared Memory Transplant - Fastest, but only works with my official binary docker images First, you'll want to make sure that the `hive-docker` repository is up to date, so that you have the latest features and bug fixes required for the upgrade to go smoothly: ```sh git fetch git checkout master git pull ``` Next, you'll want to install the HF25 Docker binary image. On most setups, a standard `./run.sh install` would work, but to avoid the risk of some people having configured a different default docker image in their `.env`, it's best to specify `hf25`: ```sh ./run.sh install hf25 ``` Now, you'll need to stop your node - if it's actually running: ```sh ./run.sh stop ``` To help avoid corruption/contamination issues, and to potentially speed the downloads up by preventing `rsync` from having to calculate the differences between Privex's copy, and your copy of `shared_memory.bin` and `block_log.index` - it's best we remove them: ```sh ./run.sh clean shm rm -fv data/witness_node_data_dir/blockchain/block_log.index ``` It's now time to truncate/update your block_log, replace your block_log.index, and obtain a fresh copy of shared_memory.bin for HF25: ``` DL_SERVER=&quot;se1.files.privex.io&quot; ./run.sh fix-blocks ``` ![](https://images.hive.blog/p/7258xSVeJbKmECTaWChp4sJR8aZDJ7Y2HRvUHUnE6UnEtpj2Z6dqikt3ye5hs7B8Ef2bkjsjyQTre4JUb4x4j7Hxq281Uk2Fio53QLtzuovnfGtaRi2NEJC8GQ4MpvT915F6gKRVhZPwL?format=match&amp;amp;mode=fit) - When asked whether you want to download the rest of the block_log, or truncate your block_log, press `y` and then hit enter. - If it instead says that your `block_log` is the same size as the remote, and wants to do an &quot;integrity check&quot; or &quot;verify the block_log, press `n` and hit enter. - When aksed if you want to replace/update/synchronise your `block_log.index` - press `y` and hit enter. - When asked if you want to download/synchronise your snapshot files, press `n` and hit enter. - When asked if you want to download/update/synchronise your `shared_memory.bin` - press `y` and hit enter. Assuming everything went well, your server should now be ready to start. Time to start your server: ``` ./run.sh start ``` Now check the logs, and monitor it as it syncs. ``` ./run.sh logs ``` If it takes more than 5 minutes to start outputting &quot;Got X transactions from witness someguy123 on block 123456778&quot; - then you should run `./run.sh monitor` to see the sync progress. Either way, it's a waiting game, it shouldn't take more than 20 mins to sync up (assuming you're following this guide within this week - this article was written on June 30th 2021), as `fix-blocks` would've gotten the recently synced chain files from [Privex's](https://www.privex.io/) file server. +++ END OF METHOD 1 - Your node should now be up, or will be up very soon when it finishes syncing +++ ---------------------- ## Update Method 2 - Load a native snapshot - Slower than shared_memory.bin transplant - but this method will work with non-someguy123 built images ### ONLY FOLLOW THIS METHOD IF YOU COMPILE YOUR OWN DOCKER IMAGE, OR USE A THIRD-PARTY DOCKER IMAGE If you use a custom image - whether one you built yourself using `./run.sh build`, `docker build`, - or one you downloaded from someone who isn't me - the shared memory transplant will likely be incompatible. For this method, follow everything in method 1, but DO NOT follow the y/n suggestions for `fix-blocks`, instead, you'll want to answer them in the following way instead: - When asked whether you want to download the rest of the block_log, or truncate your block_log, press `y` and then hit enter. - If it instead says that your `block_log` is the same size as the remote, and wants to do an &quot;integrity check&quot; or &quot;verify the block_log, press `n` and hit enter. - When aksed if you want to replace/update/synchronise your `block_log.index` - press `y` and hit enter. - When asked if you want to download/synchronise your snapshot files, press `y` and hit enter. - When asked if you want to download/update/synchronise your `shared_memory.bin` - press `n` and hit enter. **ONLY FOLLOW THIS METHOD IF YOU COMPILE YOUR OWN DOCKER IMAGE, OR USE A THIRD-PARTY DOCKER IMAGE** Now, use `./run.sh clean shm` to ensure you don't have a `shared_memory.bin`, since an existing shared_memory.bin file will cause problems. ``` ./run.sh clean shm ``` You can now load the native snapshot using the command: ``` ./run.sh loadsnap privexsnap ``` This will generate a `shared_memory.bin` using that snapshot, and once it's finished loading the snapshot, the node will automatically start normally, so for a witness node, that means it will be ready to start producing once it's finished loading the snapshot, and syncing up any blocks which it was behind. All you have to do now, is monitor it (if you like), to track it's progress, and make sure nothing is going wrong: ``` ./run.sh logs ``` +++ END OF METHOD 2 - Your node should now be up, or will be up very soon when it finishes syncing/loading the snapshot +++ --- Thanks for reading! --- ![](https://cdn.discordapp.com/attachments/379745956134125569/737091330143944806/SOMEX123.gif) GIF Avatar by @stellabelle --- Do you like what I'm doing for Hive? ---- [Vote for me to be a Hive witness](https://peakd.com/witnesses) - every vote counts. ---- Don't forget to follow me for more like this. ---- --- Have you ever thought about being a witness yourself? Join the [witness channel](https://openhive.chat/channel/witness). We're happy to guide you! Join in shaping the STEEM economy. Are you looking for a new server provider? My company @privex offers highly-reliable and affordable dedicated and virtual servers for HIVE, HBD, EOS, LTC, BTC, and even DOGE! Check out our website at https://www.privex.io You can join Privex's public Discord at https://discord.privex.io - we also have a Matrix server, sign up at https://riot.privex.io (or if you have an existing Matrix account, join our General chat at `#privex:privex.io`) --- See: [Quickstart] How to upgrade Hive-in-a-box (HIAB / hive-docker) to HF25 by @someguy123</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/p/HuuaCwcKuiEjNgLb5Q7HbtUvB1HEHgck8hUkQMB4bFSJaxwFsYXGx7N9qtgEvLdGaoY?format=match&amp;amp;mode=fit" /><media:content medium="image" url="https://images.hive.blog/p/HuuaCwcKuiEjNgLb5Q7HbtUvB1HEHgck8hUkQMB4bFSJaxwFsYXGx7N9qtgEvLdGaoY?format=match&amp;amp;mode=fit" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">17th update of 2021 on BlockTrades work on Hive software</title><link href="https://hivedocs.info/news/core/development/2021/06/29/17th-update-of-2021-on-blocktrades-work-on-hive-software.html" rel="alternate" type="text/html" title="17th update of 2021 on BlockTrades work on Hive software" /><published>2021-06-29T14:48:42-07:00</published><updated>2021-06-29T14:48:42-07:00</updated><id>https://hivedocs.info/news/core/development/2021/06/29/17th-update-of-2021-on-blocktrades-work-on-hive-software</id><content type="html" xml:base="https://hivedocs.info/news/core/development/2021/06/29/17th-update-of-2021-on-blocktrades-work-on-hive-software.html">&lt;div id=&quot;content-blocktrades-17th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png)

Below is a list of Hive-related programming issues worked on by BlockTrades team during last week or so:

# Hived work (blockchain node software)

Many users last week experienced problems getting their transactions accepted into the blockchain. It turns out there were two separate problems that could cause this issue, but to the outside observer they probably looked much the same:

### Beem had a problem detecting v1.25.0 Hive’s chainid, leading to invalidly signed transactions

The first problem, discovered a few days ago, was that the python-based Beem API library used by several Hive apps was misdetecting v1.25.0 Hive nodes as Steem nodes, and therefore trying to sign with the wrong chain id. This, in turn, led to Beem-based Hive Apps creating transactions that would be rejected by any Hive node, because the transactions weren’t properly signed for Hive. 

We assisted Beem developers with identifying the source of the problem and suggested a solution. Beem now checks for the configuration constant HIVE_CHAIN_ID, before it checks for the constant STEEM_CHAIN_ID, giving precedence to Hive nodes over Steem nodes in the detection process and thus using the proper chainid during transaction signing. 

In related work, we replaced the name STEEM_CHAIN_ID with OLD_CHAIN_ID and eliminated some duplicated constant values returned by get_config (but note these changes will not be included in the master branch until after the hardfork, like other changes completed in the last week):
https://gitlab.syncad.com/hive/hive/-/merge_requests/268
https://gitlab.syncad.com/hive/hive/-/merge_requests/267

### Snapshot-based initialization of Hived-nodes requires a restart to properly initialize chainid

The second problem appeared one day later, but a couple of days before the Beem library was updated, so the two problems overlapped in time, and this led to some confusion among app developers as to what was the source of the new problem. 

But by testing against various API nodes, we were able to determine that this second problem, although similar in appearance to the Beem problem, was occurring on non-Beem apps (e.g. hive.blog and peakd) and only occurring when those apps were using api.hive.blog as their API node. Since we hard recently updated the hived nodes on api.hive.blog, I suspected the problem has something to do with the update process.

After comparing update procedures used by other API node operators, I noticed that most of the API node operators had done a replay to update their nodes,  whereas we had updated our internal hived nodes from a snapshot. Ultimately this allowed us to identify a previously undetected error with nodes initialized from a snapshot: the variable that stores the chainid isn’t part of the database state that gets loaded from the snapshot data and the snapshot loading code didn’t trigger this variable to be updated. So a node updated in this way would reject validly signed transactions because it was operating with the wrong chainid (essentially the reverse of the Beem problem). 

### Snapshot  workaround

We also found that this improperly set chainid problem was resolved by stopping and restarting the node that was initialized from a snapshot, and that’s how we have worked around the issue until we release a full fix for snapshot-based node initialization. 

The code fix is currently being implemented and tested, and will be released after the hardfork (to keep things simple, we’re avoiding making too many changes to the development branch until after the hardfork has triggered).

## Completed hived work

Improvements to Testtools used to verify hived functionality: https://gitlab.syncad.com/hive/hive/-/merge_requests/266

Removed the copy/pasted version of secp256k1 library from hived and replaced with a submodule link:
https://gitlab.syncad.com/hive/hive/-/merge_requests/264

Fixed issue where enum_virtual_ops sometimes returned non-virtual ops: https://gitlab.syncad.com/hive/hive/-/merge_requests/219

We completed work to eliminate duplicated code in the command-line wallet for hived, but we’re delaying merging those changes into the develop branch until later because of the sheer amount of changes involved. The still-open merge request for that work is here: https://gitlab.syncad.com/hive/hive/-/merge_requests/170

## Continuing work on hived

We’re working on speeding up the blockchain converter that we’ve developed to initialize and control a testnet configured in basically the same way as a mainnet. This is to allow for more realistic testing inside of a testnet environment (especially for better performance testing). So far, we’ve identified that the performance bottleneck seems to be related to the function that generates nonces.

We’re continuing to work on the sql serializer plugin for hived that provides data to HAF-based applications. We expecting to complete that work this week, assuming no further distractions arise in the meantime (this work got delayed while we were troubleshooting the snapshot problem), then begin performance testing.

We’re also continuing to work on the CI system to automate deployment of hived for more strenuous testing scenarios (e.g. long replay testing).


# Hivemind (2nd layer applications + social media middleware)

Fix for previously report list_subscribers bug: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/513

We continue to investigate ways to improve performance of the update_rshares function immediately after massive sync of a hivemind instance. In addition to analyzing the cause of slowness of the current implementation, we’re also simultaneously looking at two different alternatives to improve overall performance: 1) eliminating the call completing and performing equivalent functionality during live sync  and 2) adding an index (at least temporarily just after massive sync) to speed up update_rshares.

The new index dramatically speeds up performance of update_rshares (time drops from 10 hours to 3 minutes and the index only takes about 10minutes to create), but it consumes 25GB of disk space (hence the idea to drop it after using it).

We’re still running functional and performance tests related to the broken reputation/ordering issue because of multiple transactions used per block. As part of this work we’re also adding “completion markers” to the database state to better detect any database consistency error that can arise from an abrupt shutdown of postgres (notably, this can happen when postgres does an auto-upgrade for a security fix).

We also resumed research into the reason why some hivemind nodes consume more memory than others. It has been suggested that it may be related to differences in python or python-library installations on the different systems.

# Hive Application Framework

We made good progress last week on both HAF code and the documentation for it, but our main dev for this work is now on vacation until June 19th, at which time the work will resume. 

But in the meantime, we should be able to do performance testing of this code using the new sql_serializer plugin, and I’m looking forward to seeing just how well we do.

# Reminder: hardfork 25 triggers tomorrow

Witnesses and devs alike will be “manning their keyboards” tomorrow (June 30th) as hardfork 25 triggers, ready for any potential problems that may arise. The hardfork is scheduled for approximately 14:00 UTC.&lt;/div&gt;
&lt;script crossorigin=&quot;anonymous&quot; integrity=&quot;sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=&quot; src=&quot;https://code.jquery.com/jquery-3.5.1.slim.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/steem-content-renderer&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;https://cdn.jsdelivr.net/npm/hive-content-renderer/dist/hive-content-renderer.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;script&gt;
  $(document).ready(function() {
    try {
      const renderer = new SteemContentRenderer.DefaultRenderer({
      // const renderer = new HiveContentRenderer({
        baseUrl: &quot;https://hive.blog/&quot;,
        breaks: true,
        skipSanitization: false,
        allowInsecureScriptTags: false,
        addNofollowToLinks: true,
        doNotShowImages: false,
        ipfsPrefix: &quot;&quot;,
        assetsWidth: 640,
        assetsHeight: 480,
        imageProxyFn: (url) =&gt; url,
        usertagUrlFn: (account) =&gt; &quot;/@blocktrades&quot;,
        hashtagUrlFn: (hashtag) =&gt; &quot;/17th-update-of-2021-on-blocktrades-work-on-hive-software&quot;,
        isLinkSafeFn: (url) =&gt; true,
      });
      
      const inputElem = $('#content-blocktrades-17th-update-of-2021-on-blocktrades-work-on-hive-software').html();
      const outputElem = $('#content-blocktrades-17th-update-of-2021-on-blocktrades-work-on-hive-software');
      const output = renderer.render(inputElem);
      
      outputElem.html(output);
    } catch(e) {
      console.log(e);
    }
  });
&lt;/script&gt;

&lt;style&gt;
  #content-blocktrades-17th-update-of-2021-on-blocktrades-work-on-hive-software {
    padding: 0 3rem;
    color: #444444;
    font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif;
    font-size: 16px;
    line-height: 1.8;
    text-shadow: 0 1px 0 #ffffff;
    padding: 0.5rem;
  }
  #content-blocktrades-17th-update-of-2021-on-blocktrades-work-on-hive-software code {
    background: white;
  }
  #content-blocktrades-17th-update-of-2021-on-blocktrades-work-on-hive-software a {
    border-bottom: 1px solid #444444; color: #444444; text-decoration: none;
  }
  #content-blocktrades-17th-update-of-2021-on-blocktrades-work-on-hive-software a:hover {
    border-bottom: 0;
  }
  #content-blocktrades-17th-update-of-2021-on-blocktrades-work-on-hive-software h1 {
    font-size: 2.2em;
  }
  #content-blocktrades-17th-update-of-2021-on-blocktrades-work-on-hive-software h2, h3, h4, h5 {
    margin-bottom: 0;
  }
  #content-blocktrades-17th-update-of-2021-on-blocktrades-work-on-hive-software header small {
    color: #999;
    font-size: 50%;
  }
  #content-blocktrades-17th-update-of-2021-on-blocktrades-work-on-hive-software img {
    max-width: 100%;
  }
&lt;/style&gt;

&lt;hr /&gt;

&lt;p&gt;
  See: &lt;a href=&quot;https://hive.blog/hive-139531/@blocktrades/17th-update-of-2021-on-blocktrades-work-on-hive-software&quot;&gt;17th update of 2021 on BlockTrades work on Hive software&lt;/a&gt;
  by
  &lt;a href=&quot;https://hive.blog/@blocktrades&quot;&gt;@blocktrades&lt;/a&gt;
&lt;/p&gt;</content><author><name>blocktrades</name></author><category term="news" /><category term="core" /><category term="development" /><summary type="html">![blocktrades update.png](https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png) Below is a list of Hive-related programming issues worked on by BlockTrades team during last week or so: # Hived work (blockchain node software) Many users last week experienced problems getting their transactions accepted into the blockchain. It turns out there were two separate problems that could cause this issue, but to the outside observer they probably looked much the same: ### Beem had a problem detecting v1.25.0 Hive’s chainid, leading to invalidly signed transactions The first problem, discovered a few days ago, was that the python-based Beem API library used by several Hive apps was misdetecting v1.25.0 Hive nodes as Steem nodes, and therefore trying to sign with the wrong chain id. This, in turn, led to Beem-based Hive Apps creating transactions that would be rejected by any Hive node, because the transactions weren’t properly signed for Hive. We assisted Beem developers with identifying the source of the problem and suggested a solution. Beem now checks for the configuration constant HIVE_CHAIN_ID, before it checks for the constant STEEM_CHAIN_ID, giving precedence to Hive nodes over Steem nodes in the detection process and thus using the proper chainid during transaction signing. In related work, we replaced the name STEEM_CHAIN_ID with OLD_CHAIN_ID and eliminated some duplicated constant values returned by get_config (but note these changes will not be included in the master branch until after the hardfork, like other changes completed in the last week): https://gitlab.syncad.com/hive/hive/-/merge_requests/268 https://gitlab.syncad.com/hive/hive/-/merge_requests/267 ### Snapshot-based initialization of Hived-nodes requires a restart to properly initialize chainid The second problem appeared one day later, but a couple of days before the Beem library was updated, so the two problems overlapped in time, and this led to some confusion among app developers as to what was the source of the new problem. But by testing against various API nodes, we were able to determine that this second problem, although similar in appearance to the Beem problem, was occurring on non-Beem apps (e.g. hive.blog and peakd) and only occurring when those apps were using api.hive.blog as their API node. Since we hard recently updated the hived nodes on api.hive.blog, I suspected the problem has something to do with the update process. After comparing update procedures used by other API node operators, I noticed that most of the API node operators had done a replay to update their nodes, whereas we had updated our internal hived nodes from a snapshot. Ultimately this allowed us to identify a previously undetected error with nodes initialized from a snapshot: the variable that stores the chainid isn’t part of the database state that gets loaded from the snapshot data and the snapshot loading code didn’t trigger this variable to be updated. So a node updated in this way would reject validly signed transactions because it was operating with the wrong chainid (essentially the reverse of the Beem problem). ### Snapshot workaround We also found that this improperly set chainid problem was resolved by stopping and restarting the node that was initialized from a snapshot, and that’s how we have worked around the issue until we release a full fix for snapshot-based node initialization. The code fix is currently being implemented and tested, and will be released after the hardfork (to keep things simple, we’re avoiding making too many changes to the development branch until after the hardfork has triggered). ## Completed hived work Improvements to Testtools used to verify hived functionality: https://gitlab.syncad.com/hive/hive/-/merge_requests/266 Removed the copy/pasted version of secp256k1 library from hived and replaced with a submodule link: https://gitlab.syncad.com/hive/hive/-/merge_requests/264 Fixed issue where enum_virtual_ops sometimes returned non-virtual ops: https://gitlab.syncad.com/hive/hive/-/merge_requests/219 We completed work to eliminate duplicated code in the command-line wallet for hived, but we’re delaying merging those changes into the develop branch until later because of the sheer amount of changes involved. The still-open merge request for that work is here: https://gitlab.syncad.com/hive/hive/-/merge_requests/170 ## Continuing work on hived We’re working on speeding up the blockchain converter that we’ve developed to initialize and control a testnet configured in basically the same way as a mainnet. This is to allow for more realistic testing inside of a testnet environment (especially for better performance testing). So far, we’ve identified that the performance bottleneck seems to be related to the function that generates nonces. We’re continuing to work on the sql serializer plugin for hived that provides data to HAF-based applications. We expecting to complete that work this week, assuming no further distractions arise in the meantime (this work got delayed while we were troubleshooting the snapshot problem), then begin performance testing. We’re also continuing to work on the CI system to automate deployment of hived for more strenuous testing scenarios (e.g. long replay testing). # Hivemind (2nd layer applications + social media middleware) Fix for previously report list_subscribers bug: https://gitlab.syncad.com/hive/hivemind/-/merge_requests/513 We continue to investigate ways to improve performance of the update_rshares function immediately after massive sync of a hivemind instance. In addition to analyzing the cause of slowness of the current implementation, we’re also simultaneously looking at two different alternatives to improve overall performance: 1) eliminating the call completing and performing equivalent functionality during live sync and 2) adding an index (at least temporarily just after massive sync) to speed up update_rshares. The new index dramatically speeds up performance of update_rshares (time drops from 10 hours to 3 minutes and the index only takes about 10minutes to create), but it consumes 25GB of disk space (hence the idea to drop it after using it). We’re still running functional and performance tests related to the broken reputation/ordering issue because of multiple transactions used per block. As part of this work we’re also adding “completion markers” to the database state to better detect any database consistency error that can arise from an abrupt shutdown of postgres (notably, this can happen when postgres does an auto-upgrade for a security fix). We also resumed research into the reason why some hivemind nodes consume more memory than others. It has been suggested that it may be related to differences in python or python-library installations on the different systems. # Hive Application Framework We made good progress last week on both HAF code and the documentation for it, but our main dev for this work is now on vacation until June 19th, at which time the work will resume. But in the meantime, we should be able to do performance testing of this code using the new sql_serializer plugin, and I’m looking forward to seeing just how well we do. # Reminder: hardfork 25 triggers tomorrow Witnesses and devs alike will be “manning their keyboards” tomorrow (June 30th) as hardfork 25 triggers, ready for any potential problems that may arise. The hardfork is scheduled for approximately 14:00 UTC. See: 17th update of 2021 on BlockTrades work on Hive software by @blocktrades</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" /><media:content medium="image" url="https://images.hive.blog/DQmSihw8Kz4U7TuCQa98DDdCzqbqPFRumuVWAbareiYZW1Z/blocktrades%20update.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>