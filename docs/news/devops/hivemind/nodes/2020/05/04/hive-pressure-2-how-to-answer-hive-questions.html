<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Hive Pressure 2: How to Answer Hive Questions? | Hive Chain Documentation</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Hive Pressure 2: How to Answer Hive Questions?" />
<meta name="author" content="gtg" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="First, you need to learn how to ask good questions, and here are some of the resources that will help you to do so: https://developers.hive.io/apidefinitions/ https://hive.hivesigner.com/ (Kudos to @inertia and @good-karma) This set of API calls is far from perfect, but for now it has to be enough for general purpose API nodes. https://www.youtube.com/watch ?v=vlW9lDE3DuI The big, the slow, and the ugly. Back in the days, we used to run a so-called “full node”, that is a single steemd (yeah, we haven’t renamed the binary yet) node that was built with LOW_MEMORY_NODE=OFF and CLEAR_VOTES=OFF and configured with all the plugins you can get. It required a lot of RAM, it replayed for ages, and it was a huge pain to keep it running. Our code is great for running blockchain. It’s not equally efficient when it has to answer complex questions. Current architecture The idea is to move the workload requiring complex queries out of our blockchain nodes. +----------+ | &lt;-----------------+ @ @@@@@@ ,@@@@@% | Hivemind | | @@@@ (@@@@@* @@@@@@ +-------+ | &lt;-------+ | %@@@@@@ @@@@@@ %@@@@@, | | +-----^----+ | | @@@@@@@@@@ @@@@@@ @@@@@@ | redis | | | | ,@@@@@@@@@@@@ @@@@@@ @@@@@@ | &lt;--+ | +----v-----+ | @@@@@@@@@@@@@@@&amp; @@@@@@ @@@@@@ +-------+ | +-v-+ | | | @@@@@@@@@@@@@@@@@@ .@@@@@% @@@@@@ | | &lt;-----&gt; AH node | | @@@@@@@@@@@@@@@@@@@@@( .@@@@@% +-------+ +--&gt; j | | | | @@@@@@@@@@@@@@@@@@@@ @@@@@@ &lt;-------&gt; | | u | +----------+ | *@@@@@@@@@@@@@@@@ @@@@@@ @@@@@@. &lt;-------&gt; nginx &lt;-----&gt; s | | @@@@@@@@@@@@@@ &amp;@@@@@. @@@@@@ &lt;-------&gt; | | s | +----------+ | #@@@@@@@@@@ @@@@@@ #@@@@@/ +-------+ | i | | | | @@@@@@@@ /@@@@@/ @@@@@@ | &lt;-----&gt; FAT node &lt;---+ @@@@@( @@@@@@ .@@@@@&amp; +---+ | | @@ @@@@@&amp; @@@@@@ +----------+ Sorry, lack of GIMP skills Hivemind For this purpose I use Hivemind (hats off to @roadscape) backed by PostgreSQL. Hive is a “consensus interpretation” layer for the Hive blockchain, maintaining the state of social features such as post feeds, follows, and communities. Written in Python, it synchronizes an SQL database with chain state, providing developers with a more flexible/extensible alternative to the raw hived API. FAT node Also, instead of a single hived node with all the plugins, I chose to run two nodes, one of them is a “fat node” (LOW_MEMORY_NODE=OFF and CLEAR_VOTES=OFF) on a MIRA-enabled instance to feed the Hivemind. Please note that I have NOT included market_history in my configuration, simply because it doesn’t require a “fat node”, but Hivemind requires it, so make sure that you have it somewhere. AH node Account history node is the other node I use in my setup. It serves not only account history, but it’s definitely the heaviest plugin here, hence the name. I’m not using MIRA here, because I prefer the pre-MIRA implementation of the account history plugin and MIRA had some issues with it. Also, it’s way too slow for replay. Jussi Instead of one service, I now have three specialized ones, I need to route incoming calls to them. So the get_account_history goes to the AH node, while the get_followers goes to Hivemind. That’s what jussi does, but it also caches things. Redis Jussi uses Redis as in-memory data cache. This can very effectively take load off the nodes. Even though most of the entries quickly expire, it’s enough to effectively answer common questions such as “what’s in the head block?” 8 dApps asking for the latest block will result in 1 call to the node and 7 cache hits from Redis. Nginx That’s the world facing component - here you can have your SSL termination, rate limiting, load balancing, and all other fancy stuff related to serving your clients. Resources Now when you know all the components, let’s take a look at what is required to run them all and (in the darkness) bind them. Small stuff There are no specific needs here. The more traffic you expect, the more resources you will need, but they can run on any reasonable server on instance: Nginx needs what nginx usually needs - a bunch of cores and some RAM. Jussi is no different than nginx when it comes to resources. Redis needs what redis usually needs - a few GB of RAM to hold the data. Big stuff the AH node, which is non-MIRA in my setup, requires plenty of RAM for the shared_memory.bin file to either hold it on tmpfs or buffer/cache it, especially during replay. A machine with 32GB RAM will work, but I would rather suggest using a 64GB RAM machine these days. Of course, low latency storage such as SSD or NVMe is a must. You need 600GB of it. the FAT node in my setup is running MIRA, so it’s not that memory hungry, but the more RAM you have, the more effective it can be. A machine with 16GB RAM might work, but I would go with 32GB or 64GB for it. Don’t even try without a very fast SSD or NVMe. You need 400GB of it. Hivemind itself is a simple script, but it needs PostgreSQL as a database backend, and for that you need all the things that PostgreSQL usually needs. It can run on pretty much everything, as long as you have enough space to fit the data, currently 300GB. Of course, faster storage and more RAM will result in much better performance. From zero to hero Reference hardware configuration: Intel(R) Xeon(R) E-2274G CPU @ 4.00GHz 64GB RAM, ECC, 2666MT/s 2x NVMe 960GB (SAMSUNG PM983) When you are starting from scratch, it’s best to get a recent block_log I’m providing one at https://gtg.openhive.network/get/blockchain/ How fast you can get it depends on your network and load on my server. The average downloading speed is around 30MB/s, so you should be able to get it in less than 3 hours. Node type Replay Time AH Node 15 hours 42 minutes Fat Node 48 hours 53 minutes Hivemind 85 hours 50 minutes Roughly you need 4 days and 9 hours to have it synced to the latest head block. A lot can be improved at the backend, and we are working on that. To be developer-friendly, we also need to improve the set of available API calls, finally get rid of the deprecated ones such as get_state, and move away from the all in one wrapper condenser_api. But that’s a different story for a different occasion. See: Hive Pressure 2: How to Answer Hive Questions? by @gtg" />
<meta property="og:description" content="First, you need to learn how to ask good questions, and here are some of the resources that will help you to do so: https://developers.hive.io/apidefinitions/ https://hive.hivesigner.com/ (Kudos to @inertia and @good-karma) This set of API calls is far from perfect, but for now it has to be enough for general purpose API nodes. https://www.youtube.com/watch ?v=vlW9lDE3DuI The big, the slow, and the ugly. Back in the days, we used to run a so-called “full node”, that is a single steemd (yeah, we haven’t renamed the binary yet) node that was built with LOW_MEMORY_NODE=OFF and CLEAR_VOTES=OFF and configured with all the plugins you can get. It required a lot of RAM, it replayed for ages, and it was a huge pain to keep it running. Our code is great for running blockchain. It’s not equally efficient when it has to answer complex questions. Current architecture The idea is to move the workload requiring complex queries out of our blockchain nodes. +----------+ | &lt;-----------------+ @ @@@@@@ ,@@@@@% | Hivemind | | @@@@ (@@@@@* @@@@@@ +-------+ | &lt;-------+ | %@@@@@@ @@@@@@ %@@@@@, | | +-----^----+ | | @@@@@@@@@@ @@@@@@ @@@@@@ | redis | | | | ,@@@@@@@@@@@@ @@@@@@ @@@@@@ | &lt;--+ | +----v-----+ | @@@@@@@@@@@@@@@&amp; @@@@@@ @@@@@@ +-------+ | +-v-+ | | | @@@@@@@@@@@@@@@@@@ .@@@@@% @@@@@@ | | &lt;-----&gt; AH node | | @@@@@@@@@@@@@@@@@@@@@( .@@@@@% +-------+ +--&gt; j | | | | @@@@@@@@@@@@@@@@@@@@ @@@@@@ &lt;-------&gt; | | u | +----------+ | *@@@@@@@@@@@@@@@@ @@@@@@ @@@@@@. &lt;-------&gt; nginx &lt;-----&gt; s | | @@@@@@@@@@@@@@ &amp;@@@@@. @@@@@@ &lt;-------&gt; | | s | +----------+ | #@@@@@@@@@@ @@@@@@ #@@@@@/ +-------+ | i | | | | @@@@@@@@ /@@@@@/ @@@@@@ | &lt;-----&gt; FAT node &lt;---+ @@@@@( @@@@@@ .@@@@@&amp; +---+ | | @@ @@@@@&amp; @@@@@@ +----------+ Sorry, lack of GIMP skills Hivemind For this purpose I use Hivemind (hats off to @roadscape) backed by PostgreSQL. Hive is a “consensus interpretation” layer for the Hive blockchain, maintaining the state of social features such as post feeds, follows, and communities. Written in Python, it synchronizes an SQL database with chain state, providing developers with a more flexible/extensible alternative to the raw hived API. FAT node Also, instead of a single hived node with all the plugins, I chose to run two nodes, one of them is a “fat node” (LOW_MEMORY_NODE=OFF and CLEAR_VOTES=OFF) on a MIRA-enabled instance to feed the Hivemind. Please note that I have NOT included market_history in my configuration, simply because it doesn’t require a “fat node”, but Hivemind requires it, so make sure that you have it somewhere. AH node Account history node is the other node I use in my setup. It serves not only account history, but it’s definitely the heaviest plugin here, hence the name. I’m not using MIRA here, because I prefer the pre-MIRA implementation of the account history plugin and MIRA had some issues with it. Also, it’s way too slow for replay. Jussi Instead of one service, I now have three specialized ones, I need to route incoming calls to them. So the get_account_history goes to the AH node, while the get_followers goes to Hivemind. That’s what jussi does, but it also caches things. Redis Jussi uses Redis as in-memory data cache. This can very effectively take load off the nodes. Even though most of the entries quickly expire, it’s enough to effectively answer common questions such as “what’s in the head block?” 8 dApps asking for the latest block will result in 1 call to the node and 7 cache hits from Redis. Nginx That’s the world facing component - here you can have your SSL termination, rate limiting, load balancing, and all other fancy stuff related to serving your clients. Resources Now when you know all the components, let’s take a look at what is required to run them all and (in the darkness) bind them. Small stuff There are no specific needs here. The more traffic you expect, the more resources you will need, but they can run on any reasonable server on instance: Nginx needs what nginx usually needs - a bunch of cores and some RAM. Jussi is no different than nginx when it comes to resources. Redis needs what redis usually needs - a few GB of RAM to hold the data. Big stuff the AH node, which is non-MIRA in my setup, requires plenty of RAM for the shared_memory.bin file to either hold it on tmpfs or buffer/cache it, especially during replay. A machine with 32GB RAM will work, but I would rather suggest using a 64GB RAM machine these days. Of course, low latency storage such as SSD or NVMe is a must. You need 600GB of it. the FAT node in my setup is running MIRA, so it’s not that memory hungry, but the more RAM you have, the more effective it can be. A machine with 16GB RAM might work, but I would go with 32GB or 64GB for it. Don’t even try without a very fast SSD or NVMe. You need 400GB of it. Hivemind itself is a simple script, but it needs PostgreSQL as a database backend, and for that you need all the things that PostgreSQL usually needs. It can run on pretty much everything, as long as you have enough space to fit the data, currently 300GB. Of course, faster storage and more RAM will result in much better performance. From zero to hero Reference hardware configuration: Intel(R) Xeon(R) E-2274G CPU @ 4.00GHz 64GB RAM, ECC, 2666MT/s 2x NVMe 960GB (SAMSUNG PM983) When you are starting from scratch, it’s best to get a recent block_log I’m providing one at https://gtg.openhive.network/get/blockchain/ How fast you can get it depends on your network and load on my server. The average downloading speed is around 30MB/s, so you should be able to get it in less than 3 hours. Node type Replay Time AH Node 15 hours 42 minutes Fat Node 48 hours 53 minutes Hivemind 85 hours 50 minutes Roughly you need 4 days and 9 hours to have it synced to the latest head block. A lot can be improved at the backend, and we are working on that. To be developer-friendly, we also need to improve the set of available API calls, finally get rid of the deprecated ones such as get_state, and move away from the all in one wrapper condenser_api. But that’s a different story for a different occasion. See: Hive Pressure 2: How to Answer Hive Questions? by @gtg" />
<link rel="canonical" href="https://hive.blog/hive-160391/@gtg/hive-pressure-2-how-to-answer-hive-questions" />
<meta property="og:url" content="https://hive.blog/hive-160391/@gtg/hive-pressure-2-how-to-answer-hive-questions" />
<meta property="og:site_name" content="Hive Chain Documentation" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-04T11:32:57-07:00" />
<script type="application/ld+json">
{"description":"First, you need to learn how to ask good questions, and here are some of the resources that will help you to do so: https://developers.hive.io/apidefinitions/ https://hive.hivesigner.com/ (Kudos to @inertia and @good-karma) This set of API calls is far from perfect, but for now it has to be enough for general purpose API nodes. https://www.youtube.com/watch ?v=vlW9lDE3DuI The big, the slow, and the ugly. Back in the days, we used to run a so-called “full node”, that is a single steemd (yeah, we haven’t renamed the binary yet) node that was built with LOW_MEMORY_NODE=OFF and CLEAR_VOTES=OFF and configured with all the plugins you can get. It required a lot of RAM, it replayed for ages, and it was a huge pain to keep it running. Our code is great for running blockchain. It’s not equally efficient when it has to answer complex questions. Current architecture The idea is to move the workload requiring complex queries out of our blockchain nodes. +----------+ | &lt;-----------------+ @ @@@@@@ ,@@@@@% | Hivemind | | @@@@ (@@@@@* @@@@@@ +-------+ | &lt;-------+ | %@@@@@@ @@@@@@ %@@@@@, | | +-----^----+ | | @@@@@@@@@@ @@@@@@ @@@@@@ | redis | | | | ,@@@@@@@@@@@@ @@@@@@ @@@@@@ | &lt;--+ | +----v-----+ | @@@@@@@@@@@@@@@&amp; @@@@@@ @@@@@@ +-------+ | +-v-+ | | | @@@@@@@@@@@@@@@@@@ .@@@@@% @@@@@@ | | &lt;-----&gt; AH node | | @@@@@@@@@@@@@@@@@@@@@( .@@@@@% +-------+ +--&gt; j | | | | @@@@@@@@@@@@@@@@@@@@ @@@@@@ &lt;-------&gt; | | u | +----------+ | *@@@@@@@@@@@@@@@@ @@@@@@ @@@@@@. &lt;-------&gt; nginx &lt;-----&gt; s | | @@@@@@@@@@@@@@ &amp;@@@@@. @@@@@@ &lt;-------&gt; | | s | +----------+ | #@@@@@@@@@@ @@@@@@ #@@@@@/ +-------+ | i | | | | @@@@@@@@ /@@@@@/ @@@@@@ | &lt;-----&gt; FAT node &lt;---+ @@@@@( @@@@@@ .@@@@@&amp; +---+ | | @@ @@@@@&amp; @@@@@@ +----------+ Sorry, lack of GIMP skills Hivemind For this purpose I use Hivemind (hats off to @roadscape) backed by PostgreSQL. Hive is a “consensus interpretation” layer for the Hive blockchain, maintaining the state of social features such as post feeds, follows, and communities. Written in Python, it synchronizes an SQL database with chain state, providing developers with a more flexible/extensible alternative to the raw hived API. FAT node Also, instead of a single hived node with all the plugins, I chose to run two nodes, one of them is a “fat node” (LOW_MEMORY_NODE=OFF and CLEAR_VOTES=OFF) on a MIRA-enabled instance to feed the Hivemind. Please note that I have NOT included market_history in my configuration, simply because it doesn’t require a “fat node”, but Hivemind requires it, so make sure that you have it somewhere. AH node Account history node is the other node I use in my setup. It serves not only account history, but it’s definitely the heaviest plugin here, hence the name. I’m not using MIRA here, because I prefer the pre-MIRA implementation of the account history plugin and MIRA had some issues with it. Also, it’s way too slow for replay. Jussi Instead of one service, I now have three specialized ones, I need to route incoming calls to them. So the get_account_history goes to the AH node, while the get_followers goes to Hivemind. That’s what jussi does, but it also caches things. Redis Jussi uses Redis as in-memory data cache. This can very effectively take load off the nodes. Even though most of the entries quickly expire, it’s enough to effectively answer common questions such as “what’s in the head block?” 8 dApps asking for the latest block will result in 1 call to the node and 7 cache hits from Redis. Nginx That’s the world facing component - here you can have your SSL termination, rate limiting, load balancing, and all other fancy stuff related to serving your clients. Resources Now when you know all the components, let’s take a look at what is required to run them all and (in the darkness) bind them. Small stuff There are no specific needs here. The more traffic you expect, the more resources you will need, but they can run on any reasonable server on instance: Nginx needs what nginx usually needs - a bunch of cores and some RAM. Jussi is no different than nginx when it comes to resources. Redis needs what redis usually needs - a few GB of RAM to hold the data. Big stuff the AH node, which is non-MIRA in my setup, requires plenty of RAM for the shared_memory.bin file to either hold it on tmpfs or buffer/cache it, especially during replay. A machine with 32GB RAM will work, but I would rather suggest using a 64GB RAM machine these days. Of course, low latency storage such as SSD or NVMe is a must. You need 600GB of it. the FAT node in my setup is running MIRA, so it’s not that memory hungry, but the more RAM you have, the more effective it can be. A machine with 16GB RAM might work, but I would go with 32GB or 64GB for it. Don’t even try without a very fast SSD or NVMe. You need 400GB of it. Hivemind itself is a simple script, but it needs PostgreSQL as a database backend, and for that you need all the things that PostgreSQL usually needs. It can run on pretty much everything, as long as you have enough space to fit the data, currently 300GB. Of course, faster storage and more RAM will result in much better performance. From zero to hero Reference hardware configuration: Intel(R) Xeon(R) E-2274G CPU @ 4.00GHz 64GB RAM, ECC, 2666MT/s 2x NVMe 960GB (SAMSUNG PM983) When you are starting from scratch, it’s best to get a recent block_log I’m providing one at https://gtg.openhive.network/get/blockchain/ How fast you can get it depends on your network and load on my server. The average downloading speed is around 30MB/s, so you should be able to get it in less than 3 hours. Node type Replay Time AH Node 15 hours 42 minutes Fat Node 48 hours 53 minutes Hivemind 85 hours 50 minutes Roughly you need 4 days and 9 hours to have it synced to the latest head block. A lot can be improved at the backend, and we are working on that. To be developer-friendly, we also need to improve the set of available API calls, finally get rid of the deprecated ones such as get_state, and move away from the all in one wrapper condenser_api. But that’s a different story for a different occasion. See: Hive Pressure 2: How to Answer Hive Questions? by @gtg","headline":"Hive Pressure 2: How to Answer Hive Questions?","dateModified":"2020-05-04T11:32:57-07:00","datePublished":"2020-05-04T11:32:57-07:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hive.blog/hive-160391/@gtg/hive-pressure-2-how-to-answer-hive-questions"},"url":"https://hive.blog/hive-160391/@gtg/hive-pressure-2-how-to-answer-hive-questions","author":{"@type":"Person","name":"gtg"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://hivedocs.info/feed.xml" title="Hive Chain Documentation" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-161692811-1', 'auto');
  ga('send', 'pageview');
}
</script>
  
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/turbolinks/5.2.0/turbolinks.js" integrity="sha256-iM4Yzi/zLj/IshPWMC1IluRxTtRjMqjPGd97TZ9yYpU=" crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Cousine|Inconsolata" rel="stylesheet">
  <link rel="shortcut icon" type="image/png" href="/assets/images/favicon.png">
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Hive Chain Documentation</a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/news/">News</a><a class="page-link" href="/howto/">How To</a><a class="page-link" href="/links/">Useful Links</a><!-- HTML elements for search -->
          <input type="text" id="search-input" placeholder="Search" />
          
          <ul id="results-container"></ul>

          <script src="/assets/javascript/search.js"></script>
          <script>
            var sjs = SimpleJekyllSearch({
              searchInput: document.getElementById('search-input'),
              resultsContainer: document.getElementById('results-container'),
              json: '/search.json'
            })
          </script>
          <!-- <form style="float: right;" class="form-inline" method="get" action="http://www.google.com/search">
            <input type="hidden" name="sitesearch" value="hivedocs.info" />
            <input type="text" name="q" maxlength="255" placeholder="Search" />
          </form> -->
        </div>
      </nav>
  </div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Hive Pressure 2: How to Answer Hive Questions?</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-05-04T11:32:57-07:00" itemprop="datePublished">May 4, 2020
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">
          <a href="https://hive.blog/@gtg">gtg</a>
        </span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>First, you need to learn how to ask good questions, and here are some of the resources that will help you to do so:</p>

<p><a href="https://developers.hive.io/apidefinitions/">https://developers.hive.io/apidefinitions/</a></p>

<p><a href="https://hive.hivesigner.com/">https://hive.hivesigner.com/</a></p>

<p>(Kudos to @inertia and @good-karma)</p>

<p>This set of API calls is far from perfect, but for now it has to be enough for general purpose API nodes.</p>

<center>
<a href="https://www.youtube.com/watch">https://www.youtube.com/watch</a>
?v=vlW9lDE3DuI</center>

<h1 id="the-big-the-slow-and-the-ugly">The big, the slow, and the ugly.</h1>
<p>Back in the days, we used to run a so-called “full node”, that is a single <del><code class="highlighter-rouge">steemd</code></del> (yeah, we haven’t renamed the binary yet) node that was built with <code class="highlighter-rouge">LOW_MEMORY_NODE=OFF</code> and <code class="highlighter-rouge">CLEAR_VOTES=OFF</code> and configured with all the plugins you can get.
It required a lot of RAM, it replayed for ages, and it was a huge pain to keep it running.
Our code is great for running blockchain. It’s not equally efficient when it has to answer complex questions.</p>

<h1 id="current-architecture">Current architecture</h1>
<p>The idea is to move the workload requiring complex queries out of our blockchain nodes.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                  +----------+                                                                
                  |          &lt;-----------------+             @     @@@@@@    ,@@@@@%          
                  | Hivemind |                 |            @@@@    (@@@@@*    @@@@@@         
        +-------+ |          &lt;-------+         |          %@@@@@@     @@@@@@    %@@@@@,       
        |       | +-----^----+       |         |         @@@@@@@@@@    @@@@@@     @@@@@@      
        | redis |       |            |         |       ,@@@@@@@@@@@@     @@@@@@    @@@@@@     
        |       &lt;--+    |       +----v-----+   |      @@@@@@@@@@@@@@@&amp;    @@@@@@     @@@@@@   
        +-------+  |  +-v-+     |          |   |     @@@@@@@@@@@@@@@@@@    .@@@@@%    @@@@@@  
                   |  |   &lt;-----&gt; AH node  |   |   @@@@@@@@@@@@@@@@@@@@@(              .@@@@@%
        +-------+  +--&gt; j |     |          |   |    @@@@@@@@@@@@@@@@@@@@               @@@@@@ 
&lt;-------&gt;       |     | u |     +----------+   |     *@@@@@@@@@@@@@@@@     @@@@@@    @@@@@@.  
&lt;-------&gt; nginx &lt;-----&gt; s |                    |       @@@@@@@@@@@@@@    &amp;@@@@@.    @@@@@@    
&lt;-------&gt;       |     | s |     +----------+   |        #@@@@@@@@@@     @@@@@@    #@@@@@/     
        +-------+     | i |     |          |   |          @@@@@@@@    /@@@@@/    @@@@@@       
                      |   &lt;-----&gt; FAT node &lt;---+           @@@@@(    @@@@@@    .@@@@@&amp;        
                      +---+     |          |                 @@     @@@@@&amp;    @@@@@@          
                                +----------+                                                  
</code></pre></div></div>
<p><sup>
Sorry, lack of GIMP skills
</sup></p>

<h2 id="hivemind">Hivemind</h2>
<p>For this purpose I use Hivemind (hats off to @roadscape) backed by PostgreSQL.</p>
<blockquote>
  <p>Hive is a “consensus interpretation” layer for the Hive blockchain, maintaining the state of social features such as post feeds, follows, and communities. Written in Python, it synchronizes an SQL database with chain state, providing developers with a more flexible/extensible alternative to the raw hived API.</p>
</blockquote>

<h2 id="fat-node">FAT node</h2>
<p>Also, instead of a single <code class="highlighter-rouge">hived</code> node with all the plugins, I chose to run two nodes, one of them is a “fat node” (<code class="highlighter-rouge">LOW_MEMORY_NODE=OFF</code> and <code class="highlighter-rouge">CLEAR_VOTES=OFF</code>) on a MIRA-enabled instance to feed the Hivemind.</p>

<p>Please note that I have NOT included <code class="highlighter-rouge">market_history</code> in my configuration, simply because it doesn’t require a “fat node”, but Hivemind requires it, so make sure that you have it somewhere.</p>

<h2 id="ah-node">AH node</h2>
<p>Account history node is the other node I use in my setup. It serves not only account history, but it’s definitely the heaviest plugin here, hence the name.
I’m not using MIRA here, because I prefer the pre-MIRA implementation of the account history plugin and MIRA had some issues with it. Also, it’s way too slow for replay.</p>

<h2 id="jussi">Jussi</h2>
<p>Instead of one service, I now have three specialized ones, I need to route incoming calls to them.
So the <code class="highlighter-rouge">get_account_history</code> goes to the AH node, while the <code class="highlighter-rouge">get_followers</code> goes to Hivemind.
That’s what jussi does, but it also caches things.</p>

<h2 id="redis">Redis</h2>
<p>Jussi uses Redis as in-memory data cache. This can very effectively take load off the nodes. Even though most of the entries quickly expire, it’s enough to effectively answer common questions such as “what’s in the head block?”</p>

<div style="display: block; margin: 0 auto;">
  <p><img src="https://images.hive.blog/DQmdb8PSfpBkD2dm8cJtLZJNvCYAe2aYLy5Rr9MCLQxHLoN/headblock.jpg" alt="headblock.jpg" /></p>
</div>

<p>8 dApps asking for the latest block will result in 1 call to the node and 7 cache hits from Redis.</p>

<h2 id="nginx">Nginx</h2>
<p>That’s the world facing component - here you can have your SSL termination, rate limiting, load balancing, and all other fancy stuff related to serving your clients.</p>

<h1 id="resources">Resources</h1>
<p>Now when you know all the components, let’s take a look at what is required to run them all and (in the darkness) bind them.</p>

<h2 id="small-stuff">Small stuff</h2>
<p>There are no specific needs here. The more traffic you expect, the more resources you will need, but they can run on any reasonable server on instance:</p>

<ul>
  <li>Nginx needs what nginx usually needs - a bunch of cores and some RAM.</li>
  <li>Jussi is no different than nginx when it comes to resources.</li>
  <li>Redis needs what redis usually needs - a few GB of RAM to hold the data.</li>
</ul>

<h2 id="big-stuff">Big stuff</h2>

<ul>
  <li>
    <p>the AH node, which is non-MIRA in my setup, requires plenty of RAM for the <code class="highlighter-rouge">shared_memory.bin</code> file to either hold it on tmpfs or buffer/cache it, especially during replay. A machine with 32GB RAM will work, but I would rather suggest using a 64GB RAM machine these days. Of course, low latency storage such as SSD or NVMe is a must. You need 600GB of it.</p>
  </li>
  <li>
    <p>the FAT node in my setup is running MIRA, so it’s not that memory hungry, but the more RAM you have, the more effective it can be. A machine with 16GB RAM might work, but I would go with 32GB or 64GB for it. Don’t even try without a very fast SSD or NVMe. You need 400GB of it.</p>
  </li>
  <li>
    <p>Hivemind itself is a simple script, but it needs PostgreSQL as a database backend, and for that you need all the things that PostgreSQL usually needs. It can run on pretty much everything, as long as you have enough space to fit the data, currently 300GB. Of course, faster storage and more RAM will result in much better performance.</p>
  </li>
</ul>

<h1 id="from-zero-to-hero">From zero to hero</h1>
<p>Reference hardware configuration:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Intel(R) Xeon(R) E-2274G CPU @ 4.00GHz
64GB RAM, ECC, 2666MT/s
2x NVMe 960GB (SAMSUNG PM983)
</code></pre></div></div>
<p>When you are starting from scratch, it’s best to get a recent <code class="highlighter-rouge">block_log</code>
I’m providing one at 
<a href="https://gtg.openhive.network/get/blockchain/">https://gtg.openhive.network/get/blockchain/</a></p>

<p>How fast you can get it depends on your network and load on my server. The average downloading speed is around 30MB/s, so you should be able to get it in less than 3 hours.</p>

<div style="display: block; margin: 0 auto;">
  <p><img src="https://images.hive.blog/DQmRTxHALWD8Nszhg7cYkqu4tQHCLotEYMK7USDF2xthbAH/replay-speed.png" alt="replay-speed.png" /></p>
</div>

<div style="display: block; margin: 0 auto;">
  <p><img src="https://images.hive.blog/DQmeYsSaz9PTyt6wamQU76Z6Mz7gJGce8aHxmj78Q58DLuo/replay-times.png" alt="replay-times.png" /></p>
</div>

<table>
  <thead>
    <tr>
      <th>Node type</th>
      <th>Replay Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>AH Node</td>
      <td>15 hours 42 minutes</td>
    </tr>
    <tr>
      <td>Fat Node</td>
      <td>48 hours 53 minutes</td>
    </tr>
    <tr>
      <td>Hivemind</td>
      <td>85 hours 50 minutes</td>
    </tr>
  </tbody>
</table>

<p>Roughly you need 4 days and 9 hours to have it synced to the latest head block.</p>

<div style="display: block; margin: 0 auto;">
  <p><img src="https://images.hive.blog/DQmTQXoQYYFenMLrvLRZzuHZQdZ6wStc394jKTnGF8bya1s/4days9hours.jpg" alt="4days9hours.jpg" /></p>
</div>

<p>A lot can be improved at the backend, and we are working on that. To be developer-friendly, we also need to improve the set of available API calls, finally get rid of the deprecated ones such as <code class="highlighter-rouge">get_state</code>, and move away from the all in one wrapper <code class="highlighter-rouge">condenser_api</code>. But that’s a different story for a different occasion.</p>

<div style="display: block; margin: 0 auto;">
  <p><img src="https://images.hive.blog/DQmSJUo4g9AmoVFoAbs6gzMw6coVURUQKg7URtBSfkEj5oJ/Hive_free-file.png" alt="Hive_Queen" /></p>
</div>
<hr />

<p>
  See: <a href="https://hive.blog/hive-160391/@gtg/hive-pressure-2-how-to-answer-hive-questions">Hive Pressure 2: How to Answer Hive Questions?</a>
  by
  <a href="https://hive.blog/@gtg">@gtg</a>
</p>


  </div><a class="u-url" href="/news/devops/hivemind/nodes/2020/05/04/hive-pressure-2-how-to-answer-hive-questions.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading"><a rel="author" href="/">Hive Chain Documentation</a></h2>
    
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">site curated by: @inertia</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Your resource for various levels of Hive Documentation.</p>
      </div>
    </div>
  </div>

</footer>
</body>

</html>
